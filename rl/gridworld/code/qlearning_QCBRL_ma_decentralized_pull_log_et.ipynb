{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- starting point of Episode 0 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f4070b80>, <__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40920b0>, <__main__.Case object at 0x7f35f4099a20>, <__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f409ae00>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f409b2e0>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409b640>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f409b7f0>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409ac50>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f409bdf0>, <__main__.Case object at 0x7f35f409bca0>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409ace0>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f40ac2b0>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ac160>, <__main__.Case object at 0x7f35f40ac6a0>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409b8e0>, <__main__.Case object at 0x7f35f40ac3a0>, <__main__.Case object at 0x7f35f409ba30>, <__main__.Case object at 0x7f35f40ac9a0>, <__main__.Case object at 0x7f35f40accd0>, <__main__.Case object at 0x7f35f40ac7c0>, <__main__.Case object at 0x7f35f409be50>, <__main__.Case object at 0x7f35f409b6d0>, <__main__.Case object at 0x7f35f40ace20>, <__main__.Case object at 0x7f35f40acf10>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40ad120>, <__main__.Case object at 0x7f35f409af20>, <__main__.Case object at 0x7f35f40acca0>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40ad5d0>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40ac550>, <__main__.Case object at 0x7f35f40ad9f0>, <__main__.Case object at 0x7f35f40adae0>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40adc60>, <__main__.Case object at 0x7f35f40ade40>, <__main__.Case object at 0x7f35f40adf90>, <__main__.Case object at 0x7f35f40ae0e0>, <__main__.Case object at 0x7f35f40ae1d0>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40651b0>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f409ad10>, <__main__.Case object at 0x7f35f409ae90>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409b0a0>, <__main__.Case object at 0x7f35f409af50>, <__main__.Case object at 0x7f35f409b2b0>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409b400>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409b040>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409b820>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409b9d0>, <__main__.Case object at 0x7f35f409bb20>, <__main__.Case object at 0x7f35f409bc40>, <__main__.Case object at 0x7f35f409bd00>, <__main__.Case object at 0x7f35f409bdc0>, <__main__.Case object at 0x7f35f409beb0>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40ac190>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40ac340>, <__main__.Case object at 0x7f35f40ac400>, <__main__.Case object at 0x7f35f40ac460>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40ac640>, <__main__.Case object at 0x7f35f40ac700>, <__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40ac8b0>, <__main__.Case object at 0x7f35f40ac940>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f40aca60>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40acd00>, <__main__.Case object at 0x7f35f40acdc0>, <__main__.Case object at 0x7f35f40acd60>, <__main__.Case object at 0x7f35f40acac0>, <__main__.Case object at 0x7f35f40ad000>, <__main__.Case object at 0x7f35f40ad0c0>, <__main__.Case object at 0x7f35f40ad060>, <__main__.Case object at 0x7f35f40ace80>, <__main__.Case object at 0x7f35f40ad300>, <__main__.Case object at 0x7f35f40ad3c0>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f40ad600>, <__main__.Case object at 0x7f35f40ad6c0>, <__main__.Case object at 0x7f35f40ad7e0>, <__main__.Case object at 0x7f35f40ad8d0>, <__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40ada80>, <__main__.Case object at 0x7f35f40adb40>, <__main__.Case object at 0x7f35f40adc90>, <__main__.Case object at 0x7f35f40ac850>, <__main__.Case object at 0x7f35f40adea0>, <__main__.Case object at 0x7f35f40adfc0>, <__main__.Case object at 0x7f35f40ae080>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 0, Total Steps: 66, Total Rewards: [-111, -165], Status Episode: False\n",
      "------------------------------------------End of episode 0 loop--------------------\n",
      "----- starting point of Episode 1 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 1 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f409ad40>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f409b2e0>, <__main__.Case object at 0x7f35f409b5b0>, <__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f409bdf0>, <__main__.Case object at 0x7f35f409be80>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409b130>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409bb20>, <__main__.Case object at 0x7f35f40920b0>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f40ae110>, <__main__.Case object at 0x7f35f40ad510>, <__main__.Case object at 0x7f35f409b430>, <__main__.Case object at 0x7f35f40ac2b0>, <__main__.Case object at 0x7f35f40ac0d0>, <__main__.Case object at 0x7f35f40ad8a0>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f40add50>, <__main__.Case object at 0x7f35f40ad0f0>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f40acd30>, <__main__.Case object at 0x7f35f40adbd0>, <__main__.Case object at 0x7f35f40adae0>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f4073160>, <__main__.Case object at 0x7f35f4073340>, <__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f409afe0>, <__main__.Case object at 0x7f35f409b070>, <__main__.Case object at 0x7f35f409b6a0>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409bc70>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f409be50>, <__main__.Case object at 0x7f35f409ba30>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f409b310>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409b490>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409b610>, <__main__.Case object at 0x7f35f409b9d0>, <__main__.Case object at 0x7f35f409bc40>, <__main__.Case object at 0x7f35f409bdc0>, <__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40ac4c0>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40ac490>, <__main__.Case object at 0x7f35f40acfa0>, <__main__.Case object at 0x7f35f40ac910>, <__main__.Case object at 0x7f35f40acb80>, <__main__.Case object at 0x7f35f40ad7b0>, <__main__.Case object at 0x7f35f40ac580>, <__main__.Case object at 0x7f35f40ad1e0>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40acca0>, <__main__.Case object at 0x7f35f40ac550>, <__main__.Case object at 0x7f35f40adcc0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 1, Total Steps: 40, Total Rewards: [-117, -139], Status Episode: False\n",
      "------------------------------------------End of episode 1 loop--------------------\n",
      "----- starting point of Episode 2 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 2 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f4070b80>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f409b2e0>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409bdf0>, <__main__.Case object at 0x7f35f409b700>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409bbb0>, <__main__.Case object at 0x7f35f4099a20>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409b3a0>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f4073160>, <__main__.Case object at 0x7f35f409ae90>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f409b8b0>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f409bcd0>, <__main__.Case object at 0x7f35f40adff0>, <__main__.Case object at 0x7f35f409af50>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f40ace20>, <__main__.Case object at 0x7f35f409b820>, <__main__.Case object at 0x7f35f40ae200>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40ac490>, <__main__.Case object at 0x7f35f409b6a0>, <__main__.Case object at 0x7f35f409b850>, <__main__.Case object at 0x7f35f40acb80>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b310>, <__main__.Case object at 0x7f35f40ad660>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f40acbe0>, <__main__.Case object at 0x7f35f40ac400>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40ac070>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40ad690>, <__main__.Case object at 0x7f35f40ad7e0>, <__main__.Case object at 0x7f35f40ad8d0>, <__main__.Case object at 0x7f35f40adc60>, <__main__.Case object at 0x7f35f40ada80>, <__main__.Case object at 0x7f35f40ad5d0>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f40ac8e0>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f40ad9c0>, <__main__.Case object at 0x7f35f40ae8c0>, <__main__.Case object at 0x7f35f40ae7d0>, <__main__.Case object at 0x7f35f40aeb30>, <__main__.Case object at 0x7f35f40ae290>, <__main__.Case object at 0x7f35f40ae530>, <__main__.Case object at 0x7f35f40ac5e0>, <__main__.Case object at 0x7f35f40ae3e0>, <__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40ae320>, <__main__.Case object at 0x7f35f40aeef0>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f40920b0>, <__main__.Case object at 0x7f35f40651b0>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f40880a0>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f409ae00>, <__main__.Case object at 0x7f35f409be80>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409b130>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f409b520>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f409af20>, <__main__.Case object at 0x7f35f409bb20>, <__main__.Case object at 0x7f35f409b040>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ad930>, <__main__.Case object at 0x7f35f40ac1f0>, <__main__.Case object at 0x7f35f407eaa0>, <__main__.Case object at 0x7f35f40ad540>, <__main__.Case object at 0x7f35f40aded0>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40adae0>, <__main__.Case object at 0x7f35f40ac910>, <__main__.Case object at 0x7f35f40accd0>, <__main__.Case object at 0x7f35f40ac580>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40ac550>, <__main__.Case object at 0x7f35f40ad390>, <__main__.Case object at 0x7f35f40ad2d0>, <__main__.Case object at 0x7f35f40ad000>, <__main__.Case object at 0x7f35f40acdc0>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40aca30>, <__main__.Case object at 0x7f35f40ac7f0>, <__main__.Case object at 0x7f35f40ac670>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40ad6c0>, <__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40add80>, <__main__.Case object at 0x7f35f40ac850>, <__main__.Case object at 0x7f35f40adfc0>, <__main__.Case object at 0x7f35f40aece0>, <__main__.Case object at 0x7f35f40aebf0>, <__main__.Case object at 0x7f35f40aeb90>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40ae9b0>, <__main__.Case object at 0x7f35f40ae830>, <__main__.Case object at 0x7f35f40ae710>, <__main__.Case object at 0x7f35f40ae680>, <__main__.Case object at 0x7f35f40ae5c0>, <__main__.Case object at 0x7f35f40ae4d0>, <__main__.Case object at 0x7f35f40ae440>, <__main__.Case object at 0x7f35f40aceb0>, <__main__.Case object at 0x7f35f40aeda0>, <__main__.Case object at 0x7f35f40ae2c0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode succeeded, case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 3, 0.5)\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 1, 0.5)\n",
      "Episode succeeded, case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.5)\n",
      "Episode succeeded, case (4, 5) is empty. Temporary case base stored to the case base: ((4, 5), 4, 0.5)\n",
      "Episode succeeded, case (3, 5) is empty. Temporary case base stored to the case base: ((3, 5), 4, 0.5)\n",
      "Episode succeeded, case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 1, 0.5)\n",
      "Episode succeeded, case (4, 6) is empty. Temporary case base stored to the case base: ((4, 6), 3, 0.5)\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) is empty. Temporary case base stored to the case base: ((5, 6), 1, 0.5)\n",
      "Episode succeeded, case (6, 6) is empty. Temporary case base stored to the case base: ((6, 6), 3, 0.5)\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 7) is empty. Temporary case base stored to the case base: ((5, 7), 1, 0.5)\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 3, 0.5)\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.5)\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 1, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 64\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 63\n",
      "cases content after RETAIN, problem: (4, 5), solution: 4, tv: 0.5, time steps: 62\n",
      "cases content after RETAIN, problem: (3, 5), solution: 4, tv: 0.5, time steps: 61\n",
      "cases content after RETAIN, problem: (3, 6), solution: 1, tv: 0.5, time steps: 60\n",
      "cases content after RETAIN, problem: (4, 6), solution: 3, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (5, 6), solution: 1, tv: 0.5, time steps: 56\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 55\n",
      "cases content after RETAIN, problem: (5, 7), solution: 1, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "Episode: 2, Total Steps: 70, Total Rewards: [-134, 31], Status Episode: False\n",
      "------------------------------------------End of episode 2 loop--------------------\n",
      "----- starting point of Episode 3 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.5, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.5, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 1 to next state (8, 0): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.5, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 40)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5, 41)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 45)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.5, 66)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.5, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.5, 69)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 3 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f409b7f0>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409b3a0>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f409b130>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f409af20>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f40ac9d0>, <__main__.Case object at 0x7f35f40add20>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40ad660>, <__main__.Case object at 0x7f35f409b3d0>, <__main__.Case object at 0x7f35f40ac340>, <__main__.Case object at 0x7f35f40aec20>, <__main__.Case object at 0x7f35f40ad750>, <__main__.Case object at 0x7f35f409b8e0>, <__main__.Case object at 0x7f35f40ae050>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40aea10>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f40ae6e0>, <__main__.Case object at 0x7f35f40ac640>, <__main__.Case object at 0x7f35f40ae380>, <__main__.Case object at 0x7f35f40ae860>, <__main__.Case object at 0x7f35f40ad2a0>, <__main__.Case object at 0x7f35f40adb40>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4070b80>, <__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f4073340>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f409bdf0>, <__main__.Case object at 0x7f35f409bbb0>, <__main__.Case object at 0x7f35f409ba30>, <__main__.Case object at 0x7f35f409b490>, <__main__.Case object at 0x7f35f409b8b0>, <__main__.Case object at 0x7f35f409b6a0>, <__main__.Case object at 0x7f35f409ae00>, <__main__.Case object at 0x7f35f409b0a0>]\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.5)\n",
      "Integrated case process. comm case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 3, 0.5)\n",
      "Integrated case process. comm case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 1, 0.5)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 3, 0.5)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 1, 0.5)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f40920b0>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f409b2e0>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f4099a20>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409bcd0>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b640>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f409be50>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f40ae560>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40adff0>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f40ac490>, <__main__.Case object at 0x7f35f40ae200>, <__main__.Case object at 0x7f35f40ad090>, <__main__.Case object at 0x7f35f409b700>, <__main__.Case object at 0x7f35f40ac730>, <__main__.Case object at 0x7f35f40ae0e0>, <__main__.Case object at 0x7f35f40ae140>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f40ae9e0>, <__main__.Case object at 0x7f35f40ae770>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f40ae4a0>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40aef50>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40adae0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 69\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.6, time steps: 66\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 65\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.4, time steps: 64\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.4, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 4, tv: 0.4, time steps: 62\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 4, tv: 0.4, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 1, tv: 0.4, time steps: 60\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 3, tv: 0.4, time steps: 59\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 1, tv: 0.4, time steps: 56\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.4, time steps: 55\n",
      "case content after REVISE for agent 1, problem: (5, 7), solution: 1, tv: 0.4, time steps: 53\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.6, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 40\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.6, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 3), 2, 0.5, 43)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 3), 3, 0.5, 43)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 1, 0.5, 43)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 43)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 43)\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 43\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.6, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.6, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "Episode: 3, Total Steps: 43, Total Rewards: [-142, 87], Status Episode: False\n",
      "------------------------------------------End of episode 3 loop--------------------\n",
      "----- starting point of Episode 4 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.6, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.6, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.6, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 0) with action 0 to next state (7, 0): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.6, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 40)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6, 41)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.5, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 3) with action 3 to next state (4, 3): pull reward: 0.08822640645959776\n",
      "----- starting point of Episode 4 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.5, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 4 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.059452028477472135\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 2 to next state (1, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f409b310>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40ae530>, <__main__.Case object at 0x7f35f40ac4c0>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40adc60>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40ac5e0>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f40ad060>, <__main__.Case object at 0x7f35f40ae0e0>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40ae9e0>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f40aded0>, <__main__.Case object at 0x7f35f40aea40>, <__main__.Case object at 0x7f35f40aebf0>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f40ad5a0>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40ae470>, <__main__.Case object at 0x7f35f40ad6c0>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40aee30>, <__main__.Case object at 0x7f35f40af970>, <__main__.Case object at 0x7f35f40af880>, <__main__.Case object at 0x7f35f40af6d0>, <__main__.Case object at 0x7f35f40af610>, <__main__.Case object at 0x7f35f40af550>, <__main__.Case object at 0x7f35f40af3d0>, <__main__.Case object at 0x7f35f40af280>, <__main__.Case object at 0x7f35f40af190>, <__main__.Case object at 0x7f35f40afaf0>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40afc70>, <__main__.Case object at 0x7f35f40afd90>, <__main__.Case object at 0x7f35f40afe50>, <__main__.Case object at 0x7f35f40affd0>, <__main__.Case object at 0x7f35f40bc130>, <__main__.Case object at 0x7f35f40bc1f0>, <__main__.Case object at 0x7f35f40aebc0>, <__main__.Case object at 0x7f35f40bc370>, <__main__.Case object at 0x7f35f40bc4f0>, <__main__.Case object at 0x7f35f40bc610>, <__main__.Case object at 0x7f35f40bc6d0>, <__main__.Case object at 0x7f35f40bc850>, <__main__.Case object at 0x7f35f40bc970>, <__main__.Case object at 0x7f35f40bca90>, <__main__.Case object at 0x7f35f40bcb50>, <__main__.Case object at 0x7f35f40bccd0>, <__main__.Case object at 0x7f35f40bcdf0>, <__main__.Case object at 0x7f35f40bcf10>, <__main__.Case object at 0x7f35f40bcfd0>, <__main__.Case object at 0x7f35f40bcbb0>, <__main__.Case object at 0x7f35f40bc3d0>, <__main__.Case object at 0x7f35f40bd270>, <__main__.Case object at 0x7f35f40bd390>, <__main__.Case object at 0x7f35f40bd510>, <__main__.Case object at 0x7f35f40bd630>, <__main__.Case object at 0x7f35f40bd750>, <__main__.Case object at 0x7f35f40bd870>, <__main__.Case object at 0x7f35f40bd810>, <__main__.Case object at 0x7f35f40bda50>, <__main__.Case object at 0x7f35f40bdbd0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409b130>, <__main__.Case object at 0x7f35f409af20>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f409afe0>, <__main__.Case object at 0x7f35f409af50>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f409bca0>, <__main__.Case object at 0x7f35f40add50>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40ad300>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40aea10>, <__main__.Case object at 0x7f35f40adff0>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f409ace0>, <__main__.Case object at 0x7f35f40acd00>, <__main__.Case object at 0x7f35f40afa00>, <__main__.Case object at 0x7f35f40aeb00>, <__main__.Case object at 0x7f35f40ae560>, <__main__.Case object at 0x7f35f40ac430>, <__main__.Case object at 0x7f35f40ad3c0>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40ae4a0>, <__main__.Case object at 0x7f35f40ac880>, <__main__.Case object at 0x7f35f40afa60>, <__main__.Case object at 0x7f35f40acd90>, <__main__.Case object at 0x7f35f40af7c0>, <__main__.Case object at 0x7f35f40af670>, <__main__.Case object at 0x7f35f40af4f0>, <__main__.Case object at 0x7f35f40ae380>, <__main__.Case object at 0x7f35f40af310>, <__main__.Case object at 0x7f35f40af1c0>, <__main__.Case object at 0x7f35f40ae230>, <__main__.Case object at 0x7f35f40ad090>, <__main__.Case object at 0x7f35f40afc10>, <__main__.Case object at 0x7f35f40afd30>, <__main__.Case object at 0x7f35f40afeb0>, <__main__.Case object at 0x7f35f40ae320>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f40bc490>, <__main__.Case object at 0x7f35f40bc5b0>, <__main__.Case object at 0x7f35f40af940>, <__main__.Case object at 0x7f35f40bc310>, <__main__.Case object at 0x7f35f40bc910>, <__main__.Case object at 0x7f35f40bca30>, <__main__.Case object at 0x7f35f40af4c0>, <__main__.Case object at 0x7f35f40bc730>, <__main__.Case object at 0x7f35f40bcd90>, <__main__.Case object at 0x7f35f40bceb0>, <__main__.Case object at 0x7f35f40af070>, <__main__.Case object at 0x7f35f40bd210>, <__main__.Case object at 0x7f35f40afee0>, <__main__.Case object at 0x7f35f40bd030>, <__main__.Case object at 0x7f35f40bd5d0>, <__main__.Case object at 0x7f35f40bd6f0>, <__main__.Case object at 0x7f35f40bd2d0>, <__main__.Case object at 0x7f35f40bd9f0>, <__main__.Case object at 0x7f35f40bdb70>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.6)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.6)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.6)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f40880a0>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f409b610>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f409ac50>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f40ae290>, <__main__.Case object at 0x7f35f40ad7b0>, <__main__.Case object at 0x7f35f40aca90>, <__main__.Case object at 0x7f35f40aed40>, <__main__.Case object at 0x7f35f40ad7e0>, <__main__.Case object at 0x7f35f40ae050>, <__main__.Case object at 0x7f35f40ae5f0>, <__main__.Case object at 0x7f35f40ae860>, <__main__.Case object at 0x7f35f40ad540>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40acbe0>, <__main__.Case object at 0x7f35f40ac400>, <__main__.Case object at 0x7f35f40ae140>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f40ae770>, <__main__.Case object at 0x7f35f40aef50>, <__main__.Case object at 0x7f35f40adb10>, <__main__.Case object at 0x7f35f40afa30>, <__main__.Case object at 0x7f35f40ac850>, <__main__.Case object at 0x7f35f40adb70>, <__main__.Case object at 0x7f35f40ac250>, <__main__.Case object at 0x7f35f40ac8b0>, <__main__.Case object at 0x7f35f40accd0>, <__main__.Case object at 0x7f35f40ae710>, <__main__.Case object at 0x7f35f40ae800>, <__main__.Case object at 0x7f35f40aed70>, <__main__.Case object at 0x7f35f40ac280>, <__main__.Case object at 0x7f35f40af8b0>, <__main__.Case object at 0x7f35f40af7f0>, <__main__.Case object at 0x7f35f40af700>, <__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40af490>, <__main__.Case object at 0x7f35f40af370>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40af130>, <__main__.Case object at 0x7f35f40af040>, <__main__.Case object at 0x7f35f40afbb0>, <__main__.Case object at 0x7f35f40afcd0>, <__main__.Case object at 0x7f35f40afdf0>, <__main__.Case object at 0x7f35f40aff40>, <__main__.Case object at 0x7f35f40bc070>, <__main__.Case object at 0x7f35f40bc190>, <__main__.Case object at 0x7f35f40bc250>, <__main__.Case object at 0x7f35f40bc0d0>, <__main__.Case object at 0x7f35f40bc430>, <__main__.Case object at 0x7f35f40bc550>, <__main__.Case object at 0x7f35f40bc670>, <__main__.Case object at 0x7f35f40bc7c0>, <__main__.Case object at 0x7f35f40bc8b0>, <__main__.Case object at 0x7f35f40bc9d0>, <__main__.Case object at 0x7f35f40bcaf0>, <__main__.Case object at 0x7f35f40bcc70>, <__main__.Case object at 0x7f35f40bcd30>, <__main__.Case object at 0x7f35f40bce50>, <__main__.Case object at 0x7f35f40bcf70>, <__main__.Case object at 0x7f35f40bd0f0>, <__main__.Case object at 0x7f35f40acf10>, <__main__.Case object at 0x7f35f40bd1b0>, <__main__.Case object at 0x7f35f40bd330>, <__main__.Case object at 0x7f35f40bd480>, <__main__.Case object at 0x7f35f40bd570>, <__main__.Case object at 0x7f35f40bd690>, <__main__.Case object at 0x7f35f40bd7b0>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f40bdb10>, <__main__.Case object at 0x7f35f40aef80>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.6, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.6, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.7, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 40\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.7, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.7, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 42\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 43\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.7, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.7, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.7, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6, time steps: 42\n",
      "Episode: 4, Total Steps: 80, Total Rewards: [-179, 87], Status Episode: False\n",
      "------------------------------------------End of episode 4 loop--------------------\n",
      "----- starting point of Episode 5 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.7, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.7, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.7, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.7, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7, 40)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7, 41)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.6, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.6, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.6, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.6, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.6, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 5 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 5 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 5 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 5 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: 0.06085543664774573\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: -0.0402672460770966\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 2 to next state (0, 5): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 1 to next state (0, 4): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 1 to next state (0, 3): pull reward: 0.0402672460770966\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: -0.06085543664774573\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 5 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 5 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 4 to next state (2, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f40880a0>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f409b130>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f409b310>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f409bf70>, <__main__.Case object at 0x7f35f4099a20>, <__main__.Case object at 0x7f35f409be50>, <__main__.Case object at 0x7f35f40acdf0>, <__main__.Case object at 0x7f35f40ad300>, <__main__.Case object at 0x7f35f40ad210>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40ae4a0>, <__main__.Case object at 0x7f35f40af820>, <__main__.Case object at 0x7f35f40af730>, <__main__.Case object at 0x7f35f40af160>, <__main__.Case object at 0x7f35f40afca0>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40af4c0>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40ac5e0>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f40ac7c0>, <__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40ae470>, <__main__.Case object at 0x7f35f40af880>, <__main__.Case object at 0x7f35f40af550>, <__main__.Case object at 0x7f35f40af190>, <__main__.Case object at 0x7f35f40afe50>, <__main__.Case object at 0x7f35f40affd0>, <__main__.Case object at 0x7f35f40ad240>, <__main__.Case object at 0x7f35f40ad9c0>, <__main__.Case object at 0x7f35f40ac640>, <__main__.Case object at 0x7f35f40ac070>, <__main__.Case object at 0x7f35f40ae3e0>, <__main__.Case object at 0x7f35f40ae1a0>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40ae740>, <__main__.Case object at 0x7f35f40af9d0>, <__main__.Case object at 0x7f35f40af640>, <__main__.Case object at 0x7f35f40af520>, <__main__.Case object at 0x7f35f40afb20>, <__main__.Case object at 0x7f35f40afe80>, <__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f40bd990>, <__main__.Case object at 0x7f35f40bdcc0>, <__main__.Case object at 0x7f35f40bc5b0>, <__main__.Case object at 0x7f35f40bc9a0>, <__main__.Case object at 0x7f35f40bcac0>, <__main__.Case object at 0x7f35f40bd300>, <__main__.Case object at 0x7f35f40bd6f0>, <__main__.Case object at 0x7f35f40bd2d0>, <__main__.Case object at 0x7f35f40bcd90>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40bc610>, <__main__.Case object at 0x7f35f40bcb50>, <__main__.Case object at 0x7f35f40bccd0>, <__main__.Case object at 0x7f35f40bc3d0>, <__main__.Case object at 0x7f35f40bd510>, <__main__.Case object at 0x7f35f40bd870>, <__main__.Case object at 0x7f35f40bd810>, <__main__.Case object at 0x7f35f40bc2e0>, <__main__.Case object at 0x7f35f40bc5e0>, <__main__.Case object at 0x7f35f40bc940>, <__main__.Case object at 0x7f35f40bca60>, <__main__.Case object at 0x7f35f40bd000>, <__main__.Case object at 0x7f35f40bd1b0>, <__main__.Case object at 0x7f35f40bd330>, <__main__.Case object at 0x7f35f40bdba0>, <__main__.Case object at 0x7f35f40be680>, <__main__.Case object at 0x7f35f40be590>, <__main__.Case object at 0x7f35f40be470>, <__main__.Case object at 0x7f35f40be3b0>, <__main__.Case object at 0x7f35f40be230>, <__main__.Case object at 0x7f35f40be110>, <__main__.Case object at 0x7f35f40bdff0>, <__main__.Case object at 0x7f35f40bdf90>, <__main__.Case object at 0x7f35f40bdc30>, <__main__.Case object at 0x7f35f40bdd50>, <__main__.Case object at 0x7f35f40be7d0>, <__main__.Case object at 0x7f35f40bea10>, <__main__.Case object at 0x7f35f40bead0>, <__main__.Case object at 0x7f35f40bebf0>, <__main__.Case object at 0x7f35f40bed10>, <__main__.Case object at 0x7f35f40bed70>, <__main__.Case object at 0x7f35f40bef50>, <__main__.Case object at 0x7f35f40bf070>, <__main__.Case object at 0x7f35f40bf190>, <__main__.Case object at 0x7f35f40bf340>, <__main__.Case object at 0x7f35f40bf3d0>, <__main__.Case object at 0x7f35f40bf430>, <__main__.Case object at 0x7f35f40bd7b0>, <__main__.Case object at 0x7f35f40bf730>, <__main__.Case object at 0x7f35f40bf790>, <__main__.Case object at 0x7f35f40bf8b0>, <__main__.Case object at 0x7f35f40bf9d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409bb20>, <__main__.Case object at 0x7f35f409bcd0>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40ac6d0>, <__main__.Case object at 0x7f35f409abc0>, <__main__.Case object at 0x7f35f40aeb00>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40af910>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40af310>, <__main__.Case object at 0x7f35f40aefe0>, <__main__.Case object at 0x7f35f40afdc0>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40adc60>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ad060>, <__main__.Case object at 0x7f35f40adc30>, <__main__.Case object at 0x7f35f40ad5a0>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40afac0>, <__main__.Case object at 0x7f35f40af6d0>, <__main__.Case object at 0x7f35f40af3d0>, <__main__.Case object at 0x7f35f40afc70>, <__main__.Case object at 0x7f35f40afaf0>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40ac340>, <__main__.Case object at 0x7f35f40ae890>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40ada80>, <__main__.Case object at 0x7f35f40ac1f0>, <__main__.Case object at 0x7f35f40ac670>, <__main__.Case object at 0x7f35f40ac8b0>, <__main__.Case object at 0x7f35f40ae950>, <__main__.Case object at 0x7f35f40af850>, <__main__.Case object at 0x7f35f40af2e0>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40afc40>, <__main__.Case object at 0x7f35f40affa0>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f40bc160>, <__main__.Case object at 0x7f35f40bc790>, <__main__.Case object at 0x7f35f40afd00>, <__main__.Case object at 0x7f35f40bcf40>, <__main__.Case object at 0x7f35f40bd030>, <__main__.Case object at 0x7f35f40ae530>, <__main__.Case object at 0x7f35f40bc280>, <__main__.Case object at 0x7f35f40bc970>, <__main__.Case object at 0x7f35f40ac250>, <__main__.Case object at 0x7f35f40bcbb0>, <__main__.Case object at 0x7f35f40bd270>, <__main__.Case object at 0x7f35f40bd630>, <__main__.Case object at 0x7f35f40af370>, <__main__.Case object at 0x7f35f40bc070>, <__main__.Case object at 0x7f35f40bc3a0>, <__main__.Case object at 0x7f35f40bc700>, <__main__.Case object at 0x7f35f40adfc0>, <__main__.Case object at 0x7f35f40bcee0>, <__main__.Case object at 0x7f35f40bd120>, <__main__.Case object at 0x7f35f40bd570>, <__main__.Case object at 0x7f35f40bc850>, <__main__.Case object at 0x7f35f40be650>, <__main__.Case object at 0x7f35f40be500>, <__main__.Case object at 0x7f35f40be380>, <__main__.Case object at 0x7f35f40be2c0>, <__main__.Case object at 0x7f35f40be1d0>, <__main__.Case object at 0x7f35f40be0e0>, <__main__.Case object at 0x7f35f40bdf00>, <__main__.Case object at 0x7f35f40bde10>, <__main__.Case object at 0x7f35f40bddb0>, <__main__.Case object at 0x7f35f40be6e0>, <__main__.Case object at 0x7f35f40be950>, <__main__.Case object at 0x7f35f40bea70>, <__main__.Case object at 0x7f35f40beb30>, <__main__.Case object at 0x7f35f40bec50>, <__main__.Case object at 0x7f35f40bee30>, <__main__.Case object at 0x7f35f40beec0>, <__main__.Case object at 0x7f35f40befb0>, <__main__.Case object at 0x7f35f40bf0d0>, <__main__.Case object at 0x7f35f40bf2b0>, <__main__.Case object at 0x7f35f40bf1f0>, <__main__.Case object at 0x7f35f40bf4f0>, <__main__.Case object at 0x7f35f40bf5b0>, <__main__.Case object at 0x7f35f40bf7f0>, <__main__.Case object at 0x7f35f40bf910>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.6)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.6)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.6)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.6)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.7)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.7)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.7)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.7)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f409af20>, <__main__.Case object at 0x7f35f409afe0>, <__main__.Case object at 0x7f35f409ace0>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409af50>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409b7f0>, <__main__.Case object at 0x7f35f409b3d0>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f409b8e0>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40aeef0>, <__main__.Case object at 0x7f35f40acd30>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40aca30>, <__main__.Case object at 0x7f35f40ac880>, <__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40af5e0>, <__main__.Case object at 0x7f35f40aceb0>, <__main__.Case object at 0x7f35f40afb80>, <__main__.Case object at 0x7f35f40aff10>, <__main__.Case object at 0x7f35f40afee0>, <__main__.Case object at 0x7f35f40ae680>, <__main__.Case object at 0x7f35f40af460>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40ae500>, <__main__.Case object at 0x7f35f40aeb90>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40ad6c0>, <__main__.Case object at 0x7f35f40ae0e0>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40af280>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40ac700>, <__main__.Case object at 0x7f35f40ac9d0>, <__main__.Case object at 0x7f35f40af610>, <__main__.Case object at 0x7f35f40ad2a0>, <__main__.Case object at 0x7f35f40afa00>, <__main__.Case object at 0x7f35f40ae8c0>, <__main__.Case object at 0x7f35f40af9a0>, <__main__.Case object at 0x7f35f40ad1b0>, <__main__.Case object at 0x7f35f40ae380>, <__main__.Case object at 0x7f35f40adf30>, <__main__.Case object at 0x7f35f40af790>, <__main__.Case object at 0x7f35f40af430>, <__main__.Case object at 0x7f35f40ae110>, <__main__.Case object at 0x7f35f40afd60>, <__main__.Case object at 0x7f35f40afa60>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f40bd8a0>, <__main__.Case object at 0x7f35f40bd930>, <__main__.Case object at 0x7f35f40bc880>, <__main__.Case object at 0x7f35f40bc730>, <__main__.Case object at 0x7f35f40bd420>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f40bd9f0>, <__main__.Case object at 0x7f35f40bc1c0>, <__main__.Case object at 0x7f35f40acf10>, <__main__.Case object at 0x7f35f40bc6d0>, <__main__.Case object at 0x7f35f40bca90>, <__main__.Case object at 0x7f35f40bcdf0>, <__main__.Case object at 0x7f35f40bc370>, <__main__.Case object at 0x7f35f40bc4f0>, <__main__.Case object at 0x7f35f40bd750>, <__main__.Case object at 0x7f35f40bda50>, <__main__.Case object at 0x7f35f40bcf10>, <__main__.Case object at 0x7f35f40bc4c0>, <__main__.Case object at 0x7f35f40bc820>, <__main__.Case object at 0x7f35f40bcb80>, <__main__.Case object at 0x7f35f40bd390>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40bd480>, <__main__.Case object at 0x7f35f40bd690>, <__main__.Case object at 0x7f35f40bcca0>, <__main__.Case object at 0x7f35f40be620>, <__main__.Case object at 0x7f35f40be4a0>, <__main__.Case object at 0x7f35f40be3e0>, <__main__.Case object at 0x7f35f40bce20>, <__main__.Case object at 0x7f35f40addb0>, <__main__.Case object at 0x7f35f40be170>, <__main__.Case object at 0x7f35f40bdf30>, <__main__.Case object at 0x7f35f40ac2b0>, <__main__.Case object at 0x7f35f40bdd80>, <__main__.Case object at 0x7f35f40be8c0>, <__main__.Case object at 0x7f35f40bcfa0>, <__main__.Case object at 0x7f35f40be890>, <__main__.Case object at 0x7f35f40beb90>, <__main__.Case object at 0x7f35f40be770>, <__main__.Case object at 0x7f35f40bedd0>, <__main__.Case object at 0x7f35f40bdc60>, <__main__.Case object at 0x7f35f40bf010>, <__main__.Case object at 0x7f35f40bc0a0>, <__main__.Case object at 0x7f35f40bf250>, <__main__.Case object at 0x7f35f40bd1e0>, <__main__.Case object at 0x7f35f40bf490>, <__main__.Case object at 0x7f35f40bf550>, <__main__.Case object at 0x7f35f40bcc70>, <__main__.Case object at 0x7f35f40bd900>, <__main__.Case object at 0x7f35f40be080>, <__main__.Case object at 0x7f35f40ae080>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.7, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.7, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.7, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.7999999999999999, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 40\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.7999999999999999, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.7999999999999999, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.7, time steps: 42\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.7, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.7, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7, time steps: 43\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.7999999999999999, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.7999999999999999, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.7999999999999999, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.7, time steps: 42\n",
      "Episode: 5, Total Steps: 104, Total Rewards: [-203, 88], Status Episode: False\n",
      "------------------------------------------End of episode 5 loop--------------------\n",
      "----- starting point of Episode 6 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.7999999999999999, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.7999999999999999, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.7999999999999999, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999999, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.7999999999999999, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7999999999999999, 40)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7999999999999999, 41)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 4 to next state (7, 3): pull reward: -0.1\n",
      "----- starting point of Episode 6 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.7, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.7, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.7, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 186 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 187 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 188 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 189 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 190 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 191 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 192 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 193 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 194 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 195 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 196 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 197 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 198 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 199 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 200 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 201 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 202 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 203 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 204 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 205 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 206 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 207 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 208 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 209 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 210 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 211 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 212 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 213 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 214 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 215 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 216 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 217 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 218 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 219 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 220 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 221 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 222 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 223 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 224 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 225 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 226 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 227 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 228 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 229 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 230 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 231 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 232 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 233 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 234 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 235 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 236 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 237 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 238 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 239 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 240 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 241 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 242 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 243 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 244 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 245 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 246 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 247 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 248 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 249 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 250 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 251 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 252 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 253 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 254 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 255 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 256 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 257 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 258 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 259 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 260 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 261 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 262 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 263 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 264 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 265 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 266 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 267 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 268 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 269 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 270 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 271 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 272 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (3, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 273 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 274 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 275 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 3 to next state (2, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 276 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 277 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 278 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 279 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 280 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 281 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 282 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 283 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 284 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 285 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 286 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 287 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 288 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 289 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 290 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 291 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 292 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 293 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 294 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 295 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 296 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 297 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 298 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 299 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 300 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 301 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 302 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 303 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 304 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 305 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 306 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 307 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 308 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 309 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 310 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 311 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 312 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 313 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 314 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 315 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 316 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 317 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 318 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 319 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 320 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 321 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 322 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 323 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 324 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 325 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 326 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 327 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 328 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 329 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 330 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 331 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 332 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 333 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 334 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 335 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 336 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 337 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 338 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 6 in steps 339 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 340 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 341 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 342 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 343 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 344 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 2 to next state (3, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 345 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 2 to next state (3, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 346 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 1 to next state (3, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 347 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 4 to next state (4, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 348 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 4 to next state (5, 2): pull reward: -0.011571775657104919\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 349 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 3 to next state (4, 2): pull reward: 0.011571775657104919\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 350 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 3 to next state (3, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 351 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 3 to next state (2, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40880a0>, <__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f409b070>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40ac6d0>, <__main__.Case object at 0x7f35f40aece0>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40adc60>, <__main__.Case object at 0x7f35f40adc30>, <__main__.Case object at 0x7f35f40afac0>, <__main__.Case object at 0x7f35f40af6d0>, <__main__.Case object at 0x7f35f40ac340>, <__main__.Case object at 0x7f35f40ae770>, <__main__.Case object at 0x7f35f40ae710>, <__main__.Case object at 0x7f35f40ac280>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f40acdf0>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f40af160>, <__main__.Case object at 0x7f35f40af4c0>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f40ae9e0>, <__main__.Case object at 0x7f35f40af6a0>, <__main__.Case object at 0x7f35f40afe20>, <__main__.Case object at 0x7f35f40ae5f0>, <__main__.Case object at 0x7f35f40ae200>, <__main__.Case object at 0x7f35f40ae800>, <__main__.Case object at 0x7f35f40af0a0>, <__main__.Case object at 0x7f35f40ad660>, <__main__.Case object at 0x7f35f40aea10>, <__main__.Case object at 0x7f35f40acd90>, <__main__.Case object at 0x7f35f40afc10>, <__main__.Case object at 0x7f35f40ac190>, <__main__.Case object at 0x7f35f40ae6e0>, <__main__.Case object at 0x7f35f40ae500>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40ac700>, <__main__.Case object at 0x7f35f40ac9d0>, <__main__.Case object at 0x7f35f40af9a0>, <__main__.Case object at 0x7f35f40ad1b0>, <__main__.Case object at 0x7f35f40adf30>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f40ae110>, <__main__.Case object at 0x7f35f40bcbe0>, <__main__.Case object at 0x7f35f40bfa00>, <__main__.Case object at 0x7f35f40bc040>, <__main__.Case object at 0x7f35f40bc340>, <__main__.Case object at 0x7f35f40bcbb0>, <__main__.Case object at 0x7f35f40bc100>, <__main__.Case object at 0x7f35f40bc250>, <__main__.Case object at 0x7f35f40bd570>, <__main__.Case object at 0x7f35f40be500>, <__main__.Case object at 0x7f35f40be380>, <__main__.Case object at 0x7f35f40bc8b0>, <__main__.Case object at 0x7f35f40be6e0>, <__main__.Case object at 0x7f35f40beb30>, <__main__.Case object at 0x7f35f40beec0>, <__main__.Case object at 0x7f35f40befb0>, <__main__.Case object at 0x7f35f40bf5b0>, <__main__.Case object at 0x7f35f40bdcc0>, <__main__.Case object at 0x7f35f40bcac0>, <__main__.Case object at 0x7f35f40bd300>, <__main__.Case object at 0x7f35f40bca00>, <__main__.Case object at 0x7f35f40bd360>, <__main__.Case object at 0x7f35f40bc220>, <__main__.Case object at 0x7f35f40bc0d0>, <__main__.Case object at 0x7f35f40bd4e0>, <__main__.Case object at 0x7f35f40be5c0>, <__main__.Case object at 0x7f35f40be290>, <__main__.Case object at 0x7f35f40bdd20>, <__main__.Case object at 0x7f35f40be740>, <__main__.Case object at 0x7f35f40beb60>, <__main__.Case object at 0x7f35f40beef0>, <__main__.Case object at 0x7f35f40bf460>, <__main__.Case object at 0x7f35f40bf520>, <__main__.Case object at 0x7f35f40bf820>, <__main__.Case object at 0x7f35f40bd150>, <__main__.Case object at 0x7f35f40bc520>, <__main__.Case object at 0x7f35f40bd9f0>, <__main__.Case object at 0x7f35f40bcb20>, <__main__.Case object at 0x7f35f40bd4b0>, <__main__.Case object at 0x7f35f40bd7e0>, <__main__.Case object at 0x7f35f40bcaf0>, <__main__.Case object at 0x7f35f40bd480>, <__main__.Case object at 0x7f35f40be620>, <__main__.Case object at 0x7f35f40be4a0>, <__main__.Case object at 0x7f35f40bdde0>, <__main__.Case object at 0x7f35f40bcfa0>, <__main__.Case object at 0x7f35f40be770>, <__main__.Case object at 0x7f35f40bedd0>, <__main__.Case object at 0x7f35f40bd1e0>, <__main__.Case object at 0x7f35f40bcc70>, <__main__.Case object at 0x7f35f40bffa0>, <__main__.Case object at 0x7f35f40bff70>, <__main__.Case object at 0x7f35f40bfdf0>, <__main__.Case object at 0x7f35f40bfd00>, <__main__.Case object at 0x7f35f40be350>, <__main__.Case object at 0x7f35f40bc730>, <__main__.Case object at 0x7f35f40d0100>, <__main__.Case object at 0x7f35f40d0220>, <__main__.Case object at 0x7f35f40d0340>, <__main__.Case object at 0x7f35f40d03a0>, <__main__.Case object at 0x7f35f40d0580>, <__main__.Case object at 0x7f35f40d06a0>, <__main__.Case object at 0x7f35f40d07c0>, <__main__.Case object at 0x7f35f40d0820>, <__main__.Case object at 0x7f35f40d0a00>, <__main__.Case object at 0x7f35f40d0b20>, <__main__.Case object at 0x7f35f40d0c40>, <__main__.Case object at 0x7f35f40d0ca0>, <__main__.Case object at 0x7f35f40d0e80>, <__main__.Case object at 0x7f35f40d0fa0>, <__main__.Case object at 0x7f35f40d10c0>, <__main__.Case object at 0x7f35f40d1120>, <__main__.Case object at 0x7f35f40d1300>, <__main__.Case object at 0x7f35f40d1420>, <__main__.Case object at 0x7f35f40d1540>, <__main__.Case object at 0x7f35f40d15a0>, <__main__.Case object at 0x7f35f40d1780>, <__main__.Case object at 0x7f35f40d18a0>, <__main__.Case object at 0x7f35f40d19c0>, <__main__.Case object at 0x7f35f40d1a20>, <__main__.Case object at 0x7f35f40d1c00>, <__main__.Case object at 0x7f35f40d1d20>, <__main__.Case object at 0x7f35f40d1e40>, <__main__.Case object at 0x7f35f40d1ea0>, <__main__.Case object at 0x7f35f40d2080>, <__main__.Case object at 0x7f35f40d21a0>, <__main__.Case object at 0x7f35f40d22c0>, <__main__.Case object at 0x7f35f40d2320>, <__main__.Case object at 0x7f35f40d2500>, <__main__.Case object at 0x7f35f40d2620>, <__main__.Case object at 0x7f35f40d2740>, <__main__.Case object at 0x7f35f40d27a0>, <__main__.Case object at 0x7f35f40d2980>, <__main__.Case object at 0x7f35f40d2aa0>, <__main__.Case object at 0x7f35f40d2bc0>, <__main__.Case object at 0x7f35f40d2c20>, <__main__.Case object at 0x7f35f40d2e00>, <__main__.Case object at 0x7f35f40d2f20>, <__main__.Case object at 0x7f35f40d3040>, <__main__.Case object at 0x7f35f40d30a0>, <__main__.Case object at 0x7f35f40d3280>, <__main__.Case object at 0x7f35f40d33a0>, <__main__.Case object at 0x7f35f40d34c0>, <__main__.Case object at 0x7f35f40d3520>, <__main__.Case object at 0x7f35f40d3700>, <__main__.Case object at 0x7f35f40d3760>, <__main__.Case object at 0x7f35f40d3820>, <__main__.Case object at 0x7f35f40d3940>, <__main__.Case object at 0x7f35f40d3b20>, <__main__.Case object at 0x7f35f40d3c40>, <__main__.Case object at 0x7f35f40d3d60>, <__main__.Case object at 0x7f35f40d3e80>, <__main__.Case object at 0x7f35f40d3dc0>, <__main__.Case object at 0x7f35f40d40a0>, <__main__.Case object at 0x7f35f40d41c0>, <__main__.Case object at 0x7f35f40d4220>, <__main__.Case object at 0x7f35f40d4400>, <__main__.Case object at 0x7f35f40d4520>, <__main__.Case object at 0x7f35f40d4640>, <__main__.Case object at 0x7f35f40d46a0>, <__main__.Case object at 0x7f35f40d4880>, <__main__.Case object at 0x7f35f40d49a0>, <__main__.Case object at 0x7f35f40d4ac0>, <__main__.Case object at 0x7f35f40d4b20>, <__main__.Case object at 0x7f35f40d4d00>, <__main__.Case object at 0x7f35f40d4e20>, <__main__.Case object at 0x7f35f40d4f40>, <__main__.Case object at 0x7f35f40d4fa0>, <__main__.Case object at 0x7f35f40d5180>, <__main__.Case object at 0x7f35f40d52a0>, <__main__.Case object at 0x7f35f40d53c0>, <__main__.Case object at 0x7f35f40d5420>, <__main__.Case object at 0x7f35f40d5600>, <__main__.Case object at 0x7f35f40d5720>, <__main__.Case object at 0x7f35f40d5840>, <__main__.Case object at 0x7f35f40d58a0>, <__main__.Case object at 0x7f35f40d5a80>, <__main__.Case object at 0x7f35f40d5ba0>, <__main__.Case object at 0x7f35f40d5cc0>, <__main__.Case object at 0x7f35f40d5d20>, <__main__.Case object at 0x7f35f40d5f00>, <__main__.Case object at 0x7f35f40d6020>, <__main__.Case object at 0x7f35f40d6140>, <__main__.Case object at 0x7f35f40d61a0>, <__main__.Case object at 0x7f35f40d6380>, <__main__.Case object at 0x7f35f40d64a0>, <__main__.Case object at 0x7f35f40d65c0>, <__main__.Case object at 0x7f35f40d6620>, <__main__.Case object at 0x7f35f40d6800>, <__main__.Case object at 0x7f35f40d6920>, <__main__.Case object at 0x7f35f40d6a40>, <__main__.Case object at 0x7f35f40d6c20>, <__main__.Case object at 0x7f35f40d6c80>, <__main__.Case object at 0x7f35f40d6da0>, <__main__.Case object at 0x7f35f40d6ec0>, <__main__.Case object at 0x7f35f40d6f20>, <__main__.Case object at 0x7f35f40d7100>, <__main__.Case object at 0x7f35f40d7220>, <__main__.Case object at 0x7f35f40d7340>, <__main__.Case object at 0x7f35f40d73a0>, <__main__.Case object at 0x7f35f40d7580>, <__main__.Case object at 0x7f35f40d76a0>, <__main__.Case object at 0x7f35f40d77c0>, <__main__.Case object at 0x7f35f40d7820>, <__main__.Case object at 0x7f35f40d7970>, <__main__.Case object at 0x7f35f40d7a00>, <__main__.Case object at 0x7f35f40d7be0>, <__main__.Case object at 0x7f35f40d7c40>, <__main__.Case object at 0x7f35f40d7e20>, <__main__.Case object at 0x7f35f40d7f40>, <__main__.Case object at 0x7f35f40d80a0>, <__main__.Case object at 0x7f35f40d8100>, <__main__.Case object at 0x7f35f40d82e0>, <__main__.Case object at 0x7f35f40d8400>, <__main__.Case object at 0x7f35f40d8520>, <__main__.Case object at 0x7f35f40d8580>, <__main__.Case object at 0x7f35f40d8760>, <__main__.Case object at 0x7f35f40d8880>, <__main__.Case object at 0x7f35f40d89a0>, <__main__.Case object at 0x7f35f40d8a00>, <__main__.Case object at 0x7f35f40d8be0>, <__main__.Case object at 0x7f35f40d8d00>, <__main__.Case object at 0x7f35f40d8e20>, <__main__.Case object at 0x7f35f40d8e80>, <__main__.Case object at 0x7f35f40d9000>, <__main__.Case object at 0x7f35f40d9060>, <__main__.Case object at 0x7f35f40d9240>, <__main__.Case object at 0x7f35f40d92a0>, <__main__.Case object at 0x7f35f40d9480>, <__main__.Case object at 0x7f35f40d95a0>, <__main__.Case object at 0x7f35f40d96c0>, <__main__.Case object at 0x7f35f40d9720>, <__main__.Case object at 0x7f35f40d9870>, <__main__.Case object at 0x7f35f40d9900>, <__main__.Case object at 0x7f35f40d9ae0>, <__main__.Case object at 0x7f35f40d9cc0>, <__main__.Case object at 0x7f35f40d9d20>, <__main__.Case object at 0x7f35f40d9e40>, <__main__.Case object at 0x7f35f40d9f60>, <__main__.Case object at 0x7f35f40d9fc0>, <__main__.Case object at 0x7f35f40da1a0>, <__main__.Case object at 0x7f35f40da2c0>, <__main__.Case object at 0x7f35f40da3e0>, <__main__.Case object at 0x7f35f40da440>, <__main__.Case object at 0x7f35f40da620>, <__main__.Case object at 0x7f35f40da740>, <__main__.Case object at 0x7f35f40da860>, <__main__.Case object at 0x7f35f40da8c0>, <__main__.Case object at 0x7f35f40daa10>, <__main__.Case object at 0x7f35f40daaa0>, <__main__.Case object at 0x7f35f40dac80>, <__main__.Case object at 0x7f35f40dace0>, <__main__.Case object at 0x7f35f40daec0>, <__main__.Case object at 0x7f35f40dafe0>, <__main__.Case object at 0x7f35f40db100>, <__main__.Case object at 0x7f35f40db160>, <__main__.Case object at 0x7f35f40db340>, <__main__.Case object at 0x7f35f40db460>, <__main__.Case object at 0x7f35f40db580>, <__main__.Case object at 0x7f35f40db5e0>, <__main__.Case object at 0x7f35f40db7c0>, <__main__.Case object at 0x7f35f40db8e0>, <__main__.Case object at 0x7f35f40dba00>, <__main__.Case object at 0x7f35f40dbbe0>, <__main__.Case object at 0x7f35f40dbc40>, <__main__.Case object at 0x7f35f40dbd60>, <__main__.Case object at 0x7f35f40dbe80>, <__main__.Case object at 0x7f35f40da9b0>, <__main__.Case object at 0x7f35f40ec100>, <__main__.Case object at 0x7f35f40ec220>, <__main__.Case object at 0x7f35f40ec340>, <__main__.Case object at 0x7f35f40ec3a0>, <__main__.Case object at 0x7f35f40ec580>, <__main__.Case object at 0x7f35f40ec6a0>, <__main__.Case object at 0x7f35f40ec7c0>, <__main__.Case object at 0x7f35f40ec820>, <__main__.Case object at 0x7f35f40eca00>, <__main__.Case object at 0x7f35f40ecb20>, <__main__.Case object at 0x7f35f40ecc40>, <__main__.Case object at 0x7f35f40ecca0>, <__main__.Case object at 0x7f35f40ece80>, <__main__.Case object at 0x7f35f40ecfa0>, <__main__.Case object at 0x7f35f40ed0c0>, <__main__.Case object at 0x7f35f40dbb50>, <__main__.Case object at 0x7f35f40ed120>, <__main__.Case object at 0x7f35f40ed3c0>, <__main__.Case object at 0x7f35f40ed4e0>, <__main__.Case object at 0x7f35f40ed540>, <__main__.Case object at 0x7f35f40ed690>, <__main__.Case object at 0x7f35f40ed720>, <__main__.Case object at 0x7f35f40ed900>, <__main__.Case object at 0x7f35f40ed960>, <__main__.Case object at 0x7f35f40edb40>, <__main__.Case object at 0x7f35f40edc60>, <__main__.Case object at 0x7f35f40edcc0>, <__main__.Case object at 0x7f35f40edf00>, <__main__.Case object at 0x7f35f40edf60>, <__main__.Case object at 0x7f35f40ee080>, <__main__.Case object at 0x7f35f40ee1a0>, <__main__.Case object at 0x7f35f40ee200>, <__main__.Case object at 0x7f35f40ee3e0>, <__main__.Case object at 0x7f35f40ee500>, <__main__.Case object at 0x7f35f40ee620>, <__main__.Case object at 0x7f35f40ee680>, <__main__.Case object at 0x7f35f40ee860>, <__main__.Case object at 0x7f35f40ee980>, <__main__.Case object at 0x7f35f40ee9e0>, <__main__.Case object at 0x7f35f40eec20>, <__main__.Case object at 0x7f35f40eec80>, <__main__.Case object at 0x7f35f40eeda0>, <__main__.Case object at 0x7f35f40eeec0>, <__main__.Case object at 0x7f35f40eef20>, <__main__.Case object at 0x7f35f40ef100>, <__main__.Case object at 0x7f35f40ef220>, <__main__.Case object at 0x7f35f40ef340>, <__main__.Case object at 0x7f35f40ef3a0>, <__main__.Case object at 0x7f35f40ef580>, <__main__.Case object at 0x7f35f40ef6a0>, <__main__.Case object at 0x7f35f40ef700>, <__main__.Case object at 0x7f35f40ef910>, <__main__.Case object at 0x7f35f40ef9a0>, <__main__.Case object at 0x7f35f40efac0>, <__main__.Case object at 0x7f35f40efbe0>, <__main__.Case object at 0x7f35f40efdc0>, <__main__.Case object at 0x7f35f40efe20>, <__main__.Case object at 0x7f35f40efe80>, <__main__.Case object at 0x7f35f40f4040>, <__main__.Case object at 0x7f35f40f40a0>, <__main__.Case object at 0x7f35f40f4280>, <__main__.Case object at 0x7f35f40f43a0>, <__main__.Case object at 0x7f35f40f44c0>, <__main__.Case object at 0x7f35f40f4520>, <__main__.Case object at 0x7f35f40f4700>, <__main__.Case object at 0x7f35f40f4880>, <__main__.Case object at 0x7f35f40f49a0>, <__main__.Case object at 0x7f35f40f4ac0>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f40f4dc0>, <__main__.Case object at 0x7f35f40f4ee0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409b3a0>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f409b430>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40aedd0>, <__main__.Case object at 0x7f35f40afd30>, <__main__.Case object at 0x7f35f40af7c0>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ad5a0>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f40aebc0>, <__main__.Case object at 0x7f35f40ac400>, <__main__.Case object at 0x7f35f40afa30>, <__main__.Case object at 0x7f35f409bcd0>, <__main__.Case object at 0x7f35f40afdf0>, <__main__.Case object at 0x7f35f40ae530>, <__main__.Case object at 0x7f35f40ad300>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40af670>, <__main__.Case object at 0x7f35f40afca0>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f40af970>, <__main__.Case object at 0x7f35f40af340>, <__main__.Case object at 0x7f35f40ae290>, <__main__.Case object at 0x7f35f40aef50>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40af8b0>, <__main__.Case object at 0x7f35f40afbb0>, <__main__.Case object at 0x7f35f40acd00>, <__main__.Case object at 0x7f35f40aca30>, <__main__.Case object at 0x7f35f40af4f0>, <__main__.Case object at 0x7f35f40ae320>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40ae9b0>, <__main__.Case object at 0x7f35f40af280>, <__main__.Case object at 0x7f35f40ad2a0>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f40af580>, <__main__.Case object at 0x7f35f40af430>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40bc490>, <__main__.Case object at 0x7f35f40ae1a0>, <__main__.Case object at 0x7f35f40bd450>, <__main__.Case object at 0x7f35f40bc8e0>, <__main__.Case object at 0x7f35f40bd270>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40bcf70>, <__main__.Case object at 0x7f35f40bc850>, <__main__.Case object at 0x7f35f40add80>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f40be950>, <__main__.Case object at 0x7f35f40bec50>, <__main__.Case object at 0x7f35f40acbe0>, <__main__.Case object at 0x7f35f40bf400>, <__main__.Case object at 0x7f35f40bf7f0>, <__main__.Case object at 0x7f35f40bc5b0>, <__main__.Case object at 0x7f35f40acf10>, <__main__.Case object at 0x7f35f40bc460>, <__main__.Case object at 0x7f35f40bcd60>, <__main__.Case object at 0x7f35f40bd6c0>, <__main__.Case object at 0x7f35f40bc9d0>, <__main__.Case object at 0x7f35f40bd000>, <__main__.Case object at 0x7f35f40bd720>, <__main__.Case object at 0x7f35f40be530>, <__main__.Case object at 0x7f35f40bdf60>, <__main__.Case object at 0x7f35f40be1a0>, <__main__.Case object at 0x7f35f40be860>, <__main__.Case object at 0x7f35f40bec80>, <__main__.Case object at 0x7f35f40bf220>, <__main__.Case object at 0x7f35f40befe0>, <__main__.Case object at 0x7f35f40bf5e0>, <__main__.Case object at 0x7f35f40bf940>, <__main__.Case object at 0x7f35f40bcd00>, <__main__.Case object at 0x7f35f40bd420>, <__main__.Case object at 0x7f35f40bc1c0>, <__main__.Case object at 0x7f35f40bce80>, <__main__.Case object at 0x7f35f40bc190>, <__main__.Case object at 0x7f35f40bc4c0>, <__main__.Case object at 0x7f35f40bce50>, <__main__.Case object at 0x7f35f40bd690>, <__main__.Case object at 0x7f35f40bce20>, <__main__.Case object at 0x7f35f40be170>, <__main__.Case object at 0x7f35f40bdd80>, <__main__.Case object at 0x7f35f40be890>, <__main__.Case object at 0x7f35f40bf010>, <__main__.Case object at 0x7f35f40bf280>, <__main__.Case object at 0x7f35f40bf490>, <__main__.Case object at 0x7f35f40bd900>, <__main__.Case object at 0x7f35f40bdf00>, <__main__.Case object at 0x7f35f40bfd90>, <__main__.Case object at 0x7f35f40bfc70>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f40bfb50>, <__main__.Case object at 0x7f35f40d0160>, <__main__.Case object at 0x7f35f40d0280>, <__main__.Case object at 0x7f35f40bcf10>, <__main__.Case object at 0x7f35f40d04f0>, <__main__.Case object at 0x7f35f40d05e0>, <__main__.Case object at 0x7f35f40d0700>, <__main__.Case object at 0x7f35f40be140>, <__main__.Case object at 0x7f35f40d0970>, <__main__.Case object at 0x7f35f40d0a60>, <__main__.Case object at 0x7f35f40d0b80>, <__main__.Case object at 0x7f35f40bf160>, <__main__.Case object at 0x7f35f40d0e20>, <__main__.Case object at 0x7f35f40d0ee0>, <__main__.Case object at 0x7f35f40d1000>, <__main__.Case object at 0x7f35f40bfe50>, <__main__.Case object at 0x7f35f40d1270>, <__main__.Case object at 0x7f35f40d1360>, <__main__.Case object at 0x7f35f40d1480>, <__main__.Case object at 0x7f35f40bdea0>, <__main__.Case object at 0x7f35f40d16f0>, <__main__.Case object at 0x7f35f40d17e0>, <__main__.Case object at 0x7f35f40d1900>, <__main__.Case object at 0x7f35f40d1ae0>, <__main__.Case object at 0x7f35f40d1b70>, <__main__.Case object at 0x7f35f40d1c60>, <__main__.Case object at 0x7f35f40d1d80>, <__main__.Case object at 0x7f35f40d1f60>, <__main__.Case object at 0x7f35f40d1ff0>, <__main__.Case object at 0x7f35f40d20e0>, <__main__.Case object at 0x7f35f40d2200>, <__main__.Case object at 0x7f35f40d23e0>, <__main__.Case object at 0x7f35f40d2470>, <__main__.Case object at 0x7f35f40d2560>, <__main__.Case object at 0x7f35f40d2680>, <__main__.Case object at 0x7f35f40d2860>, <__main__.Case object at 0x7f35f40d28f0>, <__main__.Case object at 0x7f35f40d29e0>, <__main__.Case object at 0x7f35f40d2b00>, <__main__.Case object at 0x7f35f40d2ce0>, <__main__.Case object at 0x7f35f40d2d70>, <__main__.Case object at 0x7f35f40d2e60>, <__main__.Case object at 0x7f35f40d2f80>, <__main__.Case object at 0x7f35f40d3160>, <__main__.Case object at 0x7f35f40d31f0>, <__main__.Case object at 0x7f35f40d32e0>, <__main__.Case object at 0x7f35f40d3400>, <__main__.Case object at 0x7f35f40d35e0>, <__main__.Case object at 0x7f35f40d3670>, <__main__.Case object at 0x7f35f40bdf90>, <__main__.Case object at 0x7f35f40d3a00>, <__main__.Case object at 0x7f35f40d3a90>, <__main__.Case object at 0x7f35f40d3b80>, <__main__.Case object at 0x7f35f40d3ca0>, <__main__.Case object at 0x7f35f40d38e0>, <__main__.Case object at 0x7f35f40d2890>, <__main__.Case object at 0x7f35f40d4100>, <__main__.Case object at 0x7f35f40d2d10>, <__main__.Case object at 0x7f35f40d4370>, <__main__.Case object at 0x7f35f40d4460>, <__main__.Case object at 0x7f35f40d4580>, <__main__.Case object at 0x7f35f40d3190>, <__main__.Case object at 0x7f35f40d47f0>, <__main__.Case object at 0x7f35f40d48e0>, <__main__.Case object at 0x7f35f40d4a00>, <__main__.Case object at 0x7f35f40d3610>, <__main__.Case object at 0x7f35f40d4c70>, <__main__.Case object at 0x7f35f40d4d60>, <__main__.Case object at 0x7f35f40d4e80>, <__main__.Case object at 0x7f35f40d3a30>, <__main__.Case object at 0x7f35f40d50f0>, <__main__.Case object at 0x7f35f40d51e0>, <__main__.Case object at 0x7f35f40d5300>, <__main__.Case object at 0x7f35f40d3eb0>, <__main__.Case object at 0x7f35f40d5570>, <__main__.Case object at 0x7f35f40d5660>, <__main__.Case object at 0x7f35f40d5780>, <__main__.Case object at 0x7f35f40d5960>, <__main__.Case object at 0x7f35f40d59f0>, <__main__.Case object at 0x7f35f40d5ae0>, <__main__.Case object at 0x7f35f40d5c00>, <__main__.Case object at 0x7f35f40d5de0>, <__main__.Case object at 0x7f35f40d5e70>, <__main__.Case object at 0x7f35f40d5f60>, <__main__.Case object at 0x7f35f40d6080>, <__main__.Case object at 0x7f35f40d6260>, <__main__.Case object at 0x7f35f40d62f0>, <__main__.Case object at 0x7f35f40d63e0>, <__main__.Case object at 0x7f35f40d6500>, <__main__.Case object at 0x7f35f40d66e0>, <__main__.Case object at 0x7f35f40d6770>, <__main__.Case object at 0x7f35f40d6860>, <__main__.Case object at 0x7f35f40d6980>, <__main__.Case object at 0x7f35f40d6b60>, <__main__.Case object at 0x7f35f40d6aa0>, <__main__.Case object at 0x7f35f40d6ce0>, <__main__.Case object at 0x7f35f40d6e00>, <__main__.Case object at 0x7f35f40d6fe0>, <__main__.Case object at 0x7f35f40d7070>, <__main__.Case object at 0x7f35f40d7160>, <__main__.Case object at 0x7f35f40d7280>, <__main__.Case object at 0x7f35f40d7460>, <__main__.Case object at 0x7f35f40d74f0>, <__main__.Case object at 0x7f35f40d75e0>, <__main__.Case object at 0x7f35f40d7700>, <__main__.Case object at 0x7f35f40d78e0>, <__main__.Case object at 0x7f35f40d2410>, <__main__.Case object at 0x7f35f40d7b20>, <__main__.Case object at 0x7f35f40d7d00>, <__main__.Case object at 0x7f35f40d7d90>, <__main__.Case object at 0x7f35f40d7e80>, <__main__.Case object at 0x7f35f40d6710>, <__main__.Case object at 0x7f35f40d6b90>, <__main__.Case object at 0x7f35f40d8250>, <__main__.Case object at 0x7f35f40d8340>, <__main__.Case object at 0x7f35f40d8460>, <__main__.Case object at 0x7f35f40d7010>, <__main__.Case object at 0x7f35f40d86d0>, <__main__.Case object at 0x7f35f40d87c0>, <__main__.Case object at 0x7f35f40d88e0>, <__main__.Case object at 0x7f35f40d7490>, <__main__.Case object at 0x7f35f40d8b80>, <__main__.Case object at 0x7f35f40d8c40>, <__main__.Case object at 0x7f35f40d8d60>, <__main__.Case object at 0x7f35f40d7910>, <__main__.Case object at 0x7f35f40d7ac0>, <__main__.Case object at 0x7f35f40d9180>, <__main__.Case object at 0x7f35f40d7d30>, <__main__.Case object at 0x7f35f40d9420>, <__main__.Case object at 0x7f35f40d94e0>, <__main__.Case object at 0x7f35f40d9600>, <__main__.Case object at 0x7f35f40d97e0>, <__main__.Case object at 0x7f35f40d9120>, <__main__.Case object at 0x7f35f40d9a20>, <__main__.Case object at 0x7f35f40d9c00>, <__main__.Case object at 0x7f35f40d9b40>, <__main__.Case object at 0x7f35f40d9d80>, <__main__.Case object at 0x7f35f40d9ea0>, <__main__.Case object at 0x7f35f40da080>, <__main__.Case object at 0x7f35f40da110>, <__main__.Case object at 0x7f35f40da200>, <__main__.Case object at 0x7f35f40da320>, <__main__.Case object at 0x7f35f40da500>, <__main__.Case object at 0x7f35f40da590>, <__main__.Case object at 0x7f35f40da680>, <__main__.Case object at 0x7f35f40da7a0>, <__main__.Case object at 0x7f35f40da980>, <__main__.Case object at 0x7f35f40d99c0>, <__main__.Case object at 0x7f35f40dabc0>, <__main__.Case object at 0x7f35f40dada0>, <__main__.Case object at 0x7f35f40dae30>, <__main__.Case object at 0x7f35f40daf20>, <__main__.Case object at 0x7f35f40db040>, <__main__.Case object at 0x7f35f40db220>, <__main__.Case object at 0x7f35f40db2b0>, <__main__.Case object at 0x7f35f40db3a0>, <__main__.Case object at 0x7f35f40db4c0>, <__main__.Case object at 0x7f35f40db6a0>, <__main__.Case object at 0x7f35f40db730>, <__main__.Case object at 0x7f35f40db820>, <__main__.Case object at 0x7f35f40db940>, <__main__.Case object at 0x7f35f40dbb20>, <__main__.Case object at 0x7f35f40dba60>, <__main__.Case object at 0x7f35f40dbca0>, <__main__.Case object at 0x7f35f40dbdc0>, <__main__.Case object at 0x7f35f40bfbb0>, <__main__.Case object at 0x7f35f40dbfa0>, <__main__.Case object at 0x7f35f40ec160>, <__main__.Case object at 0x7f35f40ec280>, <__main__.Case object at 0x7f35f40dadd0>, <__main__.Case object at 0x7f35f40ec4f0>, <__main__.Case object at 0x7f35f40ec5e0>, <__main__.Case object at 0x7f35f40ec700>, <__main__.Case object at 0x7f35f40db250>, <__main__.Case object at 0x7f35f40ec9a0>, <__main__.Case object at 0x7f35f40eca60>, <__main__.Case object at 0x7f35f40ecb80>, <__main__.Case object at 0x7f35f40db6d0>, <__main__.Case object at 0x7f35f40ecdf0>, <__main__.Case object at 0x7f35f40ecee0>, <__main__.Case object at 0x7f35f40ed000>, <__main__.Case object at 0x7f35f40dab60>, <__main__.Case object at 0x7f35f40ed300>, <__main__.Case object at 0x7f35f40ed420>, <__main__.Case object at 0x7f35f40dbfd0>, <__main__.Case object at 0x7f35f40ecd60>, <__main__.Case object at 0x7f35f40ed840>, <__main__.Case object at 0x7f35f40eda20>, <__main__.Case object at 0x7f35f40edab0>, <__main__.Case object at 0x7f35f40edba0>, <__main__.Case object at 0x7f35f40edd80>, <__main__.Case object at 0x7f35f40ed7e0>, <__main__.Case object at 0x7f35f40edfc0>, <__main__.Case object at 0x7f35f40ee0e0>, <__main__.Case object at 0x7f35f40ee2c0>, <__main__.Case object at 0x7f35f40ee350>, <__main__.Case object at 0x7f35f40ee440>, <__main__.Case object at 0x7f35f40ee560>, <__main__.Case object at 0x7f35f40ee740>, <__main__.Case object at 0x7f35f40ee7d0>, <__main__.Case object at 0x7f35f40ee8c0>, <__main__.Case object at 0x7f35f40eeaa0>, <__main__.Case object at 0x7f35f40ede40>, <__main__.Case object at 0x7f35f40eece0>, <__main__.Case object at 0x7f35f40eee00>, <__main__.Case object at 0x7f35f40eefe0>, <__main__.Case object at 0x7f35f40ef070>, <__main__.Case object at 0x7f35f40ef160>, <__main__.Case object at 0x7f35f40ef280>, <__main__.Case object at 0x7f35f40ef460>, <__main__.Case object at 0x7f35f40ef4f0>, <__main__.Case object at 0x7f35f40ef5e0>, <__main__.Case object at 0x7f35f40ef7c0>, <__main__.Case object at 0x7f35f40eeb60>, <__main__.Case object at 0x7f35f40efa00>, <__main__.Case object at 0x7f35f40efb20>, <__main__.Case object at 0x7f35f40efd00>, <__main__.Case object at 0x7f35f40efc40>, <__main__.Case object at 0x7f35f40ef880>, <__main__.Case object at 0x7f35f40ee770>, <__main__.Case object at 0x7f35f40dbee0>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40f4400>, <__main__.Case object at 0x7f35f40ef010>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40f4820>, <__main__.Case object at 0x7f35f40f4940>, <__main__.Case object at 0x7f35f40ef490>, <__main__.Case object at 0x7f35f40f4c40>, <__main__.Case object at 0x7f35f40f4d60>, <__main__.Case object at 0x7f35f40f4e80>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.7)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.7)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.7)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.7)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f409b610>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409b520>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409b8e0>, <__main__.Case object at 0x7f35f40add20>, <__main__.Case object at 0x7f35f40aec20>, <__main__.Case object at 0x7f35f40ac430>, <__main__.Case object at 0x7f35f40af310>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40ad060>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40af3d0>, <__main__.Case object at 0x7f35f40ae890>, <__main__.Case object at 0x7f35f40aefe0>, <__main__.Case object at 0x7f35f40ac550>, <__main__.Case object at 0x7f35f40af700>, <__main__.Case object at 0x7f35f40afc70>, <__main__.Case object at 0x7f35f40af370>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f409bdc0>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40af1f0>, <__main__.Case object at 0x7f35f40ac5e0>, <__main__.Case object at 0x7f35f40aea40>, <__main__.Case object at 0x7f35f40ae4a0>, <__main__.Case object at 0x7f35f40afa90>, <__main__.Case object at 0x7f35f40aed40>, <__main__.Case object at 0x7f35f40ae140>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40ad690>, <__main__.Case object at 0x7f35f40afdc0>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40ad5d0>, <__main__.Case object at 0x7f35f40ac610>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40af8e0>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40af610>, <__main__.Case object at 0x7f35f40af1c0>, <__main__.Case object at 0x7f35f40aff40>, <__main__.Case object at 0x7f35f40af790>, <__main__.Case object at 0x7f35f40afd60>, <__main__.Case object at 0x7f35f40addb0>, <__main__.Case object at 0x7f35f40bd5d0>, <__main__.Case object at 0x7f35f40ad210>, <__main__.Case object at 0x7f35f40bc640>, <__main__.Case object at 0x7f35f40bdbd0>, <__main__.Case object at 0x7f35f40bcc40>, <__main__.Case object at 0x7f35f40bd630>, <__main__.Case object at 0x7f35f40bc550>, <__main__.Case object at 0x7f35f40bca30>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40be2c0>, <__main__.Case object at 0x7f35f40be0e0>, <__main__.Case object at 0x7f35f40bddb0>, <__main__.Case object at 0x7f35f40bea70>, <__main__.Case object at 0x7f35f40ae380>, <__main__.Case object at 0x7f35f40bf0d0>, <__main__.Case object at 0x7f35f40bde10>, <__main__.Case object at 0x7f35f40bd990>, <__main__.Case object at 0x7f35f40bc9a0>, <__main__.Case object at 0x7f35f40bd6f0>, <__main__.Case object at 0x7f35f40bf2b0>, <__main__.Case object at 0x7f35f40bfac0>, <__main__.Case object at 0x7f35f40bd9c0>, <__main__.Case object at 0x7f35f40bd0c0>, <__main__.Case object at 0x7f35f40bd2d0>, <__main__.Case object at 0x7f35f40bdb10>, <__main__.Case object at 0x7f35f40bee30>, <__main__.Case object at 0x7f35f40be050>, <__main__.Case object at 0x7f35f40bceb0>, <__main__.Case object at 0x7f35f40be9e0>, <__main__.Case object at 0x7f35f40beda0>, <__main__.Case object at 0x7f35f40bf100>, <__main__.Case object at 0x7f35f40be410>, <__main__.Case object at 0x7f35f40bf700>, <__main__.Case object at 0x7f35f40bfa60>, <__main__.Case object at 0x7f35f40bc670>, <__main__.Case object at 0x7f35f40be1d0>, <__main__.Case object at 0x7f35f40bc7f0>, <__main__.Case object at 0x7f35f40bd180>, <__main__.Case object at 0x7f35f40be650>, <__main__.Case object at 0x7f35f40bf310>, <__main__.Case object at 0x7f35f40bd2a0>, <__main__.Case object at 0x7f35f40bcca0>, <__main__.Case object at 0x7f35f40be3e0>, <__main__.Case object at 0x7f35f40bc130>, <__main__.Case object at 0x7f35f40be8c0>, <__main__.Case object at 0x7f35f40beb90>, <__main__.Case object at 0x7f35f40bdc60>, <__main__.Case object at 0x7f35f40bca60>, <__main__.Case object at 0x7f35f40bf550>, <__main__.Case object at 0x7f35f40bcdc0>, <__main__.Case object at 0x7f35f40bfee0>, <__main__.Case object at 0x7f35f40bff10>, <__main__.Case object at 0x7f35f40bfd30>, <__main__.Case object at 0x7f35f40bc910>, <__main__.Case object at 0x7f35f40bfb80>, <__main__.Case object at 0x7f35f40d00a0>, <__main__.Case object at 0x7f35f40d01c0>, <__main__.Case object at 0x7f35f40d02e0>, <__main__.Case object at 0x7f35f40d0400>, <__main__.Case object at 0x7f35f40d0040>, <__main__.Case object at 0x7f35f40bdae0>, <__main__.Case object at 0x7f35f40d0760>, <__main__.Case object at 0x7f35f40d0640>, <__main__.Case object at 0x7f35f40d0460>, <__main__.Case object at 0x7f35f40bfc40>, <__main__.Case object at 0x7f35f40d0be0>, <__main__.Case object at 0x7f35f40d0d00>, <__main__.Case object at 0x7f35f40d08e0>, <__main__.Case object at 0x7f35f40d0f40>, <__main__.Case object at 0x7f35f40d1060>, <__main__.Case object at 0x7f35f40d1180>, <__main__.Case object at 0x7f35f40d0ac0>, <__main__.Case object at 0x7f35f40d13c0>, <__main__.Case object at 0x7f35f40d14e0>, <__main__.Case object at 0x7f35f40d0880>, <__main__.Case object at 0x7f35f40d11e0>, <__main__.Case object at 0x7f35f40d1840>, <__main__.Case object at 0x7f35f40d1960>, <__main__.Case object at 0x7f35f40d1a80>, <__main__.Case object at 0x7f35f40be080>, <__main__.Case object at 0x7f35f40d0d60>, <__main__.Case object at 0x7f35f40d1de0>, <__main__.Case object at 0x7f35f40d1f00>, <__main__.Case object at 0x7f35f40d0490>, <__main__.Case object at 0x7f35f40d2140>, <__main__.Case object at 0x7f35f40d2260>, <__main__.Case object at 0x7f35f40d2380>, <__main__.Case object at 0x7f35f40d1600>, <__main__.Case object at 0x7f35f40d25c0>, <__main__.Case object at 0x7f35f40d26e0>, <__main__.Case object at 0x7f35f40d2800>, <__main__.Case object at 0x7f35f40d0d90>, <__main__.Case object at 0x7f35f40d2a40>, <__main__.Case object at 0x7f35f40d1660>, <__main__.Case object at 0x7f35f40d2c80>, <__main__.Case object at 0x7f35f40d2b60>, <__main__.Case object at 0x7f35f40d2ec0>, <__main__.Case object at 0x7f35f40d2fe0>, <__main__.Case object at 0x7f35f40d0910>, <__main__.Case object at 0x7f35f40d1690>, <__main__.Case object at 0x7f35f40d3340>, <__main__.Case object at 0x7f35f40d3460>, <__main__.Case object at 0x7f35f40d3100>, <__main__.Case object at 0x7f35f40d1b10>, <__main__.Case object at 0x7f35f40d37c0>, <__main__.Case object at 0x7f35f40d1210>, <__main__.Case object at 0x7f35f40d3880>, <__main__.Case object at 0x7f35f40d1f90>, <__main__.Case object at 0x7f35f40d3be0>, <__main__.Case object at 0x7f35f40d39a0>, <__main__.Case object at 0x7f35f40d3e20>, <__main__.Case object at 0x7f35f40d3f10>, <__main__.Case object at 0x7f35f40d3fa0>, <__main__.Case object at 0x7f35f40d4160>, <__main__.Case object at 0x7f35f40ada50>, <__main__.Case object at 0x7f35f40d4040>, <__main__.Case object at 0x7f35f40d4280>, <__main__.Case object at 0x7f35f40d1cc0>, <__main__.Case object at 0x7f35f40d4700>, <__main__.Case object at 0x7f35f40d42e0>, <__main__.Case object at 0x7f35f40d4940>, <__main__.Case object at 0x7f35f40d4a60>, <__main__.Case object at 0x7f35f40d4b80>, <__main__.Case object at 0x7f35f40d4760>, <__main__.Case object at 0x7f35f40d4dc0>, <__main__.Case object at 0x7f35f40d4ee0>, <__main__.Case object at 0x7f35f40d5000>, <__main__.Case object at 0x7f35f40d4be0>, <__main__.Case object at 0x7f35f40d5240>, <__main__.Case object at 0x7f35f40d5360>, <__main__.Case object at 0x7f35f40d5480>, <__main__.Case object at 0x7f35f40d5060>, <__main__.Case object at 0x7f35f40d56c0>, <__main__.Case object at 0x7f35f40d57e0>, <__main__.Case object at 0x7f35f40d5900>, <__main__.Case object at 0x7f35f40d3580>, <__main__.Case object at 0x7f35f40d5b40>, <__main__.Case object at 0x7f35f40d5c60>, <__main__.Case object at 0x7f35f40d5d80>, <__main__.Case object at 0x7f35f40d4310>, <__main__.Case object at 0x7f35f40d5fc0>, <__main__.Case object at 0x7f35f40d60e0>, <__main__.Case object at 0x7f35f40d54e0>, <__main__.Case object at 0x7f35f40d4790>, <__main__.Case object at 0x7f35f40d6440>, <__main__.Case object at 0x7f35f40d6560>, <__main__.Case object at 0x7f35f40d6680>, <__main__.Case object at 0x7f35f40d4c10>, <__main__.Case object at 0x7f35f40d68c0>, <__main__.Case object at 0x7f35f40d69e0>, <__main__.Case object at 0x7f35f40d6200>, <__main__.Case object at 0x7f35f40d3d00>, <__main__.Case object at 0x7f35f40d6d40>, <__main__.Case object at 0x7f35f40d6e60>, <__main__.Case object at 0x7f35f40d6f80>, <__main__.Case object at 0x7f35f40d5510>, <__main__.Case object at 0x7f35f40d5090>, <__main__.Case object at 0x7f35f40d72e0>, <__main__.Case object at 0x7f35f40d6b00>, <__main__.Case object at 0x7f35f40d5990>, <__main__.Case object at 0x7f35f40d7640>, <__main__.Case object at 0x7f35f40d7760>, <__main__.Case object at 0x7f35f40d7880>, <__main__.Case object at 0x7f35f40d5e10>, <__main__.Case object at 0x7f35f40d7a60>, <__main__.Case object at 0x7f35f40d7b80>, <__main__.Case object at 0x7f35f40d7400>, <__main__.Case object at 0x7f35f40d6290>, <__main__.Case object at 0x7f35f40d7ee0>, <__main__.Case object at 0x7f35f40d7fa0>, <__main__.Case object at 0x7f35f40d8160>, <__main__.Case object at 0x7f35f40d8040>, <__main__.Case object at 0x7f35f40d71c0>, <__main__.Case object at 0x7f35f40d45e0>, <__main__.Case object at 0x7f35f40d85e0>, <__main__.Case object at 0x7f35f40d44c0>, <__main__.Case object at 0x7f35f40d81c0>, <__main__.Case object at 0x7f35f40d7ca0>, <__main__.Case object at 0x7f35f40d8a60>, <__main__.Case object at 0x7f35f40aee60>, <__main__.Case object at 0x7f35f40d8ca0>, <__main__.Case object at 0x7f35f40d8dc0>, <__main__.Case object at 0x7f35f40d8ee0>, <__main__.Case object at 0x7f35f40d8ac0>, <__main__.Case object at 0x7f35f40d90c0>, <__main__.Case object at 0x7f35f40d91e0>, <__main__.Case object at 0x7f35f40d9300>, <__main__.Case object at 0x7f35f40d8f40>, <__main__.Case object at 0x7f35f40d8640>, <__main__.Case object at 0x7f35f40d9660>, <__main__.Case object at 0x7f35f40d9780>, <__main__.Case object at 0x7f35f40d9360>, <__main__.Case object at 0x7f35f40d9960>, <__main__.Case object at 0x7f35f40d9a80>, <__main__.Case object at 0x7f35f40d9ba0>, <__main__.Case object at 0x7f35f40d83a0>, <__main__.Case object at 0x7f35f40d84c0>, <__main__.Case object at 0x7f35f40d9f00>, <__main__.Case object at 0x7f35f40da020>, <__main__.Case object at 0x7f35f40d8670>, <__main__.Case object at 0x7f35f40d8940>, <__main__.Case object at 0x7f35f40d8820>, <__main__.Case object at 0x7f35f40d9540>, <__main__.Case object at 0x7f35f40d8af0>, <__main__.Case object at 0x7f35f40da6e0>, <__main__.Case object at 0x7f35f40da800>, <__main__.Case object at 0x7f35f409b910>, <__main__.Case object at 0x7f35f40d8f70>, <__main__.Case object at 0x7f35f40dab00>, <__main__.Case object at 0x7f35f40da920>, <__main__.Case object at 0x7f35f40dad40>, <__main__.Case object at 0x7f35f40d9390>, <__main__.Case object at 0x7f35f40daf80>, <__main__.Case object at 0x7f35f40db0a0>, <__main__.Case object at 0x7f35f40db1c0>, <__main__.Case object at 0x7f35f40d9810>, <__main__.Case object at 0x7f35f40da4a0>, <__main__.Case object at 0x7f35f40db520>, <__main__.Case object at 0x7f35f409b1c0>, <__main__.Case object at 0x7f35f40dac20>, <__main__.Case object at 0x7f35f40d9c30>, <__main__.Case object at 0x7f35f40db640>, <__main__.Case object at 0x7f35f40dbac0>, <__main__.Case object at 0x7f35f40da0b0>, <__main__.Case object at 0x7f35f40db400>, <__main__.Case object at 0x7f35f40dbe20>, <__main__.Case object at 0x7f35f40dbf40>, <__main__.Case object at 0x7f35f40ec0a0>, <__main__.Case object at 0x7f35f40d81f0>, <__main__.Case object at 0x7f35f40ec2e0>, <__main__.Case object at 0x7f35f40ec400>, <__main__.Case object at 0x7f35f40ec070>, <__main__.Case object at 0x7f35f40ec640>, <__main__.Case object at 0x7f35f40ec760>, <__main__.Case object at 0x7f35f40d9de0>, <__main__.Case object at 0x7f35f40da260>, <__main__.Case object at 0x7f35f40da380>, <__main__.Case object at 0x7f35f40ecbe0>, <__main__.Case object at 0x7f35f40ecd00>, <__main__.Case object at 0x7f35f40bf970>, <__main__.Case object at 0x7f35f40ecf40>, <__main__.Case object at 0x7f35f40bf850>, <__main__.Case object at 0x7f35f40ed180>, <__main__.Case object at 0x7f35f40ed270>, <__main__.Case object at 0x7f35f40ed360>, <__main__.Case object at 0x7f35f40ed480>, <__main__.Case object at 0x7f35f40ecac0>, <__main__.Case object at 0x7f35f40ec8e0>, <__main__.Case object at 0x7f35f40dbd00>, <__main__.Case object at 0x7f35f40ed8a0>, <__main__.Case object at 0x7f35f40ed9c0>, <__main__.Case object at 0x7f35f40ed600>, <__main__.Case object at 0x7f35f40edc00>, <__main__.Case object at 0x7f35f40edd20>, <__main__.Case object at 0x7f35f40ec1c0>, <__main__.Case object at 0x7f35f40edde0>, <__main__.Case object at 0x7f35f40ec490>, <__main__.Case object at 0x7f35f40ee140>, <__main__.Case object at 0x7f35f40ee260>, <__main__.Case object at 0x7f35f40ec910>, <__main__.Case object at 0x7f35f40ee4a0>, <__main__.Case object at 0x7f35f40ee5c0>, <__main__.Case object at 0x7f35f40ee6e0>, <__main__.Case object at 0x7f35f40ecd90>, <__main__.Case object at 0x7f35f40ee920>, <__main__.Case object at 0x7f35f40ec880>, <__main__.Case object at 0x7f35f40ec460>, <__main__.Case object at 0x7f35f40ed1e0>, <__main__.Case object at 0x7f35f40eed40>, <__main__.Case object at 0x7f35f40ed780>, <__main__.Case object at 0x7f35f40eef80>, <__main__.Case object at 0x7f35f40ee020>, <__main__.Case object at 0x7f35f40ef1c0>, <__main__.Case object at 0x7f35f40ed630>, <__main__.Case object at 0x7f35f40eee60>, <__main__.Case object at 0x7f35f40eda50>, <__main__.Case object at 0x7f35f40ef640>, <__main__.Case object at 0x7f35f40ef760>, <__main__.Case object at 0x7f35f40ef820>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40efa60>, <__main__.Case object at 0x7f35f40ef2e0>, <__main__.Case object at 0x7f35f40eea40>, <__main__.Case object at 0x7f35f40ee2f0>, <__main__.Case object at 0x7f35f40efee0>, <__main__.Case object at 0x7f35f40db880>, <__main__.Case object at 0x7f35f40f4100>, <__main__.Case object at 0x7f35f40f4220>, <__main__.Case object at 0x7f35f40db9a0>, <__main__.Case object at 0x7f35f40f4460>, <__main__.Case object at 0x7f35f409b0d0>, <__main__.Case object at 0x7f35f40f47f0>, <__main__.Case object at 0x7f35f40f41c0>, <__main__.Case object at 0x7f35f40f4a60>, <__main__.Case object at 0x7f35f40f4c10>, <__main__.Case object at 0x7f35f40f4a90>, <__main__.Case object at 0x7f35f40f48e0>, <__main__.Case object at 0x7f35f40bfa90>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.7999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.7999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.8999999999999999, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 40\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.8999999999999999, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.8999999999999999, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.8999999999999999, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.8999999999999999, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.7999999999999999, time steps: 42\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.7999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.7999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.8999999999999999, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.8999999999999999, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999999, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.8999999999999999, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.8999999999999999, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.8999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.7999999999999999, time steps: 42\n",
      "Episode: 6, Total Steps: 352, Total Rewards: [-451, 86], Status Episode: False\n",
      "------------------------------------------End of episode 6 loop--------------------\n",
      "----- starting point of Episode 7 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.8999999999999999, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.8999999999999999, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.8999999999999999, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.8999999999999999, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.8999999999999999, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 0 to next state (7, 1): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 0 to next state (7, 1): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.8999999999999999, 40)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.8999999999999999, 41)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.7999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.7999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.7999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 3 to next state (3, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 186 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 187 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 188 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 189 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 190 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 191 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 192 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 193 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 194 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 195 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 196 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 7 in steps 197 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 198 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 199 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 200 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 201 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 202 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 3 to next state (2, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 203 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 204 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 205 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 206 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 2 to next state (2, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40651b0>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f40aedd0>, <__main__.Case object at 0x7f35f40af7c0>, <__main__.Case object at 0x7f35f40aebc0>, <__main__.Case object at 0x7f35f40ace80>, <__main__.Case object at 0x7f35f40afd00>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40af340>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40ae560>, <__main__.Case object at 0x7f35f40afb80>, <__main__.Case object at 0x7f35f40ad2a0>, <__main__.Case object at 0x7f35f40af430>, <__main__.Case object at 0x7f35f40acf10>, <__main__.Case object at 0x7f35f40ac6d0>, <__main__.Case object at 0x7f35f40ad330>, <__main__.Case object at 0x7f35f40ace20>, <__main__.Case object at 0x7f35f40af850>, <__main__.Case object at 0x7f35f40ac250>, <__main__.Case object at 0x7f35f40af4c0>, <__main__.Case object at 0x7f35f40af6a0>, <__main__.Case object at 0x7f35f40ae200>, <__main__.Case object at 0x7f35f40afe80>, <__main__.Case object at 0x7f35f40ac190>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40af9a0>, <__main__.Case object at 0x7f35f40adf30>, <__main__.Case object at 0x7f35f40aec20>, <__main__.Case object at 0x7f35f40af310>, <__main__.Case object at 0x7f35f40ae7d0>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40af700>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40adb40>, <__main__.Case object at 0x7f35f40af880>, <__main__.Case object at 0x7f35f40ae740>, <__main__.Case object at 0x7f35f40ad690>, <__main__.Case object at 0x7f35f40ad5d0>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40af1c0>, <__main__.Case object at 0x7f35f40afd60>, <__main__.Case object at 0x7f35f40ada50>, <__main__.Case object at 0x7f35f40bf6a0>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40bcee0>, <__main__.Case object at 0x7f35f40bec20>, <__main__.Case object at 0x7f35f40bf400>, <__main__.Case object at 0x7f35f40bcb50>, <__main__.Case object at 0x7f35f40bcd30>, <__main__.Case object at 0x7f35f40be230>, <__main__.Case object at 0x7f35f40be7d0>, <__main__.Case object at 0x7f35f40bd7b0>, <__main__.Case object at 0x7f35f40bd210>, <__main__.Case object at 0x7f35f40bc4f0>, <__main__.Case object at 0x7f35f40bcb80>, <__main__.Case object at 0x7f35f40bdcf0>, <__main__.Case object at 0x7f35f40bc0a0>, <__main__.Case object at 0x7f35f40bff40>, <__main__.Case object at 0x7f35f40bfca0>, <__main__.Case object at 0x7f35f40bfe50>, <__main__.Case object at 0x7f35f40bf610>, <__main__.Case object at 0x7f35f40bc970>, <__main__.Case object at 0x7f35f40bd120>, <__main__.Case object at 0x7f35f40be500>, <__main__.Case object at 0x7f35f40beb30>, <__main__.Case object at 0x7f35f40befb0>, <__main__.Case object at 0x7f35f40bfa30>, <__main__.Case object at 0x7f35f40bd360>, <__main__.Case object at 0x7f35f40bd4e0>, <__main__.Case object at 0x7f35f40bdd20>, <__main__.Case object at 0x7f35f40beb60>, <__main__.Case object at 0x7f35f40bf820>, <__main__.Case object at 0x7f35f40bd9f0>, <__main__.Case object at 0x7f35f40bd7e0>, <__main__.Case object at 0x7f35f40bdde0>, <__main__.Case object at 0x7f35f40bcfa0>, <__main__.Case object at 0x7f35f40bd1e0>, <__main__.Case object at 0x7f35f40bff70>, <__main__.Case object at 0x7f35f40bfc10>, <__main__.Case object at 0x7f35f40bc640>, <__main__.Case object at 0x7f35f40bd630>, <__main__.Case object at 0x7f35f40be200>, <__main__.Case object at 0x7f35f40bc760>, <__main__.Case object at 0x7f35f40bde10>, <__main__.Case object at 0x7f35f40bc610>, <__main__.Case object at 0x7f35f40bd810>, <__main__.Case object at 0x7f35f40bcac0>, <__main__.Case object at 0x7f35f40bdd50>, <__main__.Case object at 0x7f35f40bf190>, <__main__.Case object at 0x7f35f40bf790>, <__main__.Case object at 0x7f35f40bc370>, <__main__.Case object at 0x7f35f40bda50>, <__main__.Case object at 0x7f35f40be6b0>, <__main__.Case object at 0x7f35f40be920>, <__main__.Case object at 0x7f35f40bf040>, <__main__.Case object at 0x7f35f40bcdc0>, <__main__.Case object at 0x7f35f40bff10>, <__main__.Case object at 0x7f35f40bfc40>, <__main__.Case object at 0x7f35f40effa0>, <__main__.Case object at 0x7f35f40eeb90>, <__main__.Case object at 0x7f35f40ec4c0>, <__main__.Case object at 0x7f35f40ec7f0>, <__main__.Case object at 0x7f35f40eca60>, <__main__.Case object at 0x7f35f40ecfd0>, <__main__.Case object at 0x7f35f40ed420>, <__main__.Case object at 0x7f35f40ed930>, <__main__.Case object at 0x7f35f40edb70>, <__main__.Case object at 0x7f35f40ecb80>, <__main__.Case object at 0x7f35f40ee1d0>, <__main__.Case object at 0x7f35f40ee650>, <__main__.Case object at 0x7f35f40ee890>, <__main__.Case object at 0x7f35f40eedd0>, <__main__.Case object at 0x7f35f40ef130>, <__main__.Case object at 0x7f35f40ef4c0>, <__main__.Case object at 0x7f35f40ef6d0>, <__main__.Case object at 0x7f35f40efd00>, <__main__.Case object at 0x7f35f40ef010>, <__main__.Case object at 0x7f35f40ec340>, <__main__.Case object at 0x7f35f40ec580>, <__main__.Case object at 0x7f35f40ec2b0>, <__main__.Case object at 0x7f35f40ee320>, <__main__.Case object at 0x7f35f40ecca0>, <__main__.Case object at 0x7f35f40ed0c0>, <__main__.Case object at 0x7f35f40ed5a0>, <__main__.Case object at 0x7f35f40ed6c0>, <__main__.Case object at 0x7f35f40edae0>, <__main__.Case object at 0x7f35f40edcf0>, <__main__.Case object at 0x7f35f40ee110>, <__main__.Case object at 0x7f35f40ee470>, <__main__.Case object at 0x7f35f40ee800>, <__main__.Case object at 0x7f35f40eea10>, <__main__.Case object at 0x7f35f40eee30>, <__main__.Case object at 0x7f35f40ef190>, <__main__.Case object at 0x7f35f40ef520>, <__main__.Case object at 0x7f35f40ef730>, <__main__.Case object at 0x7f35f40efb50>, <__main__.Case object at 0x7f35f40efeb0>, <__main__.Case object at 0x7f35f40ec310>, <__main__.Case object at 0x7f35f40ec550>, <__main__.Case object at 0x7f35f40ecaf0>, <__main__.Case object at 0x7f35f40ecf70>, <__main__.Case object at 0x7f35f40ed270>, <__main__.Case object at 0x7f35f40ed480>, <__main__.Case object at 0x7f35f40ed9f0>, <__main__.Case object at 0x7f35f40edd50>, <__main__.Case object at 0x7f35f40ee050>, <__main__.Case object at 0x7f35f40ee290>, <__main__.Case object at 0x7f35f40ee710>, <__main__.Case object at 0x7f35f40eea70>, <__main__.Case object at 0x7f35f40eed70>, <__main__.Case object at 0x7f35f40eefb0>, <__main__.Case object at 0x7f35f40ef430>, <__main__.Case object at 0x7f35f40ef790>, <__main__.Case object at 0x7f35f40efa90>, <__main__.Case object at 0x7f35f40efcd0>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409b430>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f40ed6f0>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f409be50>, <__main__.Case object at 0x7f35f409b7f0>, <__main__.Case object at 0x7f35f409bdc0>, <__main__.Case object at 0x7f35f40f51b0>, <__main__.Case object at 0x7f35f40f42b0>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40f4940>, <__main__.Case object at 0x7f35f40f40d0>, <__main__.Case object at 0x7f35f40f4430>, <__main__.Case object at 0x7f35f40f4790>, <__main__.Case object at 0x7f35f40f4a30>, <__main__.Case object at 0x7f35f40f4f70>, <__main__.Case object at 0x7f35f40f4490>, <__main__.Case object at 0x7f35f40f41c0>, <__main__.Case object at 0x7f35f40f4c10>, <__main__.Case object at 0x7f35f40f4be0>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40f5cf0>, <__main__.Case object at 0x7f35f40f5e10>, <__main__.Case object at 0x7f35f40f5f90>, <__main__.Case object at 0x7f35f40f60b0>, <__main__.Case object at 0x7f35f40f61d0>, <__main__.Case object at 0x7f35f40f6290>, <__main__.Case object at 0x7f35f40f6410>, <__main__.Case object at 0x7f35f40f6530>, <__main__.Case object at 0x7f35f40f6650>, <__main__.Case object at 0x7f35f40f6770>, <__main__.Case object at 0x7f35f40f6800>, <__main__.Case object at 0x7f35f40f6950>, <__main__.Case object at 0x7f35f40f6a70>, <__main__.Case object at 0x7f35f40f6b90>, <__main__.Case object at 0x7f35f40f6c20>, <__main__.Case object at 0x7f35f40f6d70>, <__main__.Case object at 0x7f35f40f6e90>, <__main__.Case object at 0x7f35f40f7070>, <__main__.Case object at 0x7f35f40f70d0>, <__main__.Case object at 0x7f35f40f71f0>, <__main__.Case object at 0x7f35f40f7310>, <__main__.Case object at 0x7f35f40f73d0>, <__main__.Case object at 0x7f35f40f7550>, <__main__.Case object at 0x7f35f40f7670>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f40ae2f0>, <__main__.Case object at 0x7f35f40af040>, <__main__.Case object at 0x7f35f40acb80>, <__main__.Case object at 0x7f35f40af970>, <__main__.Case object at 0x7f35f40aef50>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40af280>, <__main__.Case object at 0x7f35f40af580>, <__main__.Case object at 0x7f35f40add80>, <__main__.Case object at 0x7f35f40af3a0>, <__main__.Case object at 0x7f35f40ac490>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40aece0>, <__main__.Case object at 0x7f35f40ae9e0>, <__main__.Case object at 0x7f35f40ae5f0>, <__main__.Case object at 0x7f35f40aeef0>, <__main__.Case object at 0x7f35f40afc10>, <__main__.Case object at 0x7f35f40ae500>, <__main__.Case object at 0x7f35f40ac9d0>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f40aea10>, <__main__.Case object at 0x7f35f40af3d0>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40af370>, <__main__.Case object at 0x7f35f40af070>, <__main__.Case object at 0x7f35f40afe50>, <__main__.Case object at 0x7f35f40aeb90>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40af8e0>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40af790>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40adc90>, <__main__.Case object at 0x7f35f40ae890>, <__main__.Case object at 0x7f35f40be800>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40bc8e0>, <__main__.Case object at 0x7f35f40bc2e0>, <__main__.Case object at 0x7f35f40be590>, <__main__.Case object at 0x7f35f40ada80>, <__main__.Case object at 0x7f35f40bf7f0>, <__main__.Case object at 0x7f35f40bd8a0>, <__main__.Case object at 0x7f35f40bca90>, <__main__.Case object at 0x7f35f40afa90>, <__main__.Case object at 0x7f35f40bebf0>, <__main__.Case object at 0x7f35f40bee00>, <__main__.Case object at 0x7f35f40bf760>, <__main__.Case object at 0x7f35f40aff70>, <__main__.Case object at 0x7f35f40bd840>, <__main__.Case object at 0x7f35f40be980>, <__main__.Case object at 0x7f35f40bd030>, <__main__.Case object at 0x7f35f40bda20>, <__main__.Case object at 0x7f35f40be6e0>, <__main__.Case object at 0x7f35f40bdcc0>, <__main__.Case object at 0x7f35f40bdb40>, <__main__.Case object at 0x7f35f40bc0d0>, <__main__.Case object at 0x7f35f40be290>, <__main__.Case object at 0x7f35f40beef0>, <__main__.Case object at 0x7f35f40bc400>, <__main__.Case object at 0x7f35f40bc520>, <__main__.Case object at 0x7f35f40bd4b0>, <__main__.Case object at 0x7f35f40be620>, <__main__.Case object at 0x7f35f40bec80>, <__main__.Case object at 0x7f35f40bedd0>, <__main__.Case object at 0x7f35f40bffa0>, <__main__.Case object at 0x7f35f40bfb20>, <__main__.Case object at 0x7f35f40bd690>, <__main__.Case object at 0x7f35f40bcc40>, <__main__.Case object at 0x7f35f40bca30>, <__main__.Case object at 0x7f35f40beb00>, <__main__.Case object at 0x7f35f40bf340>, <__main__.Case object at 0x7f35f40bd960>, <__main__.Case object at 0x7f35f40bd1b0>, <__main__.Case object at 0x7f35f40bd570>, <__main__.Case object at 0x7f35f40bed70>, <__main__.Case object at 0x7f35f40bc880>, <__main__.Case object at 0x7f35f40bd540>, <__main__.Case object at 0x7f35f40bd600>, <__main__.Case object at 0x7f35f40bdf30>, <__main__.Case object at 0x7f35f40bf3a0>, <__main__.Case object at 0x7f35f40bda80>, <__main__.Case object at 0x7f35f40bfb80>, <__main__.Case object at 0x7f35f40be320>, <__main__.Case object at 0x7f35f40ae650>, <__main__.Case object at 0x7f35f40ec280>, <__main__.Case object at 0x7f35f40ec6d0>, <__main__.Case object at 0x7f35f40bea70>, <__main__.Case object at 0x7f35f40eeb00>, <__main__.Case object at 0x7f35f40ed300>, <__main__.Case object at 0x7f35f40ed810>, <__main__.Case object at 0x7f35f40bdb10>, <__main__.Case object at 0x7f35f40bfd30>, <__main__.Case object at 0x7f35f40ee530>, <__main__.Case object at 0x7f35f40be1d0>, <__main__.Case object at 0x7f35f40edc90>, <__main__.Case object at 0x7f35f40ef040>, <__main__.Case object at 0x7f35f40ef370>, <__main__.Case object at 0x7f35f40bca60>, <__main__.Case object at 0x7f35f40ee9b0>, <__main__.Case object at 0x7f35f40ef880>, <__main__.Case object at 0x7f35f40ec190>, <__main__.Case object at 0x7f35f40ec6a0>, <__main__.Case object at 0x7f35f40ecc40>, <__main__.Case object at 0x7f35f40ed240>, <__main__.Case object at 0x7f35f40ece80>, <__main__.Case object at 0x7f35f40ed990>, <__main__.Case object at 0x7f35f40eddb0>, <__main__.Case object at 0x7f35f40ecc70>, <__main__.Case object at 0x7f35f40ee380>, <__main__.Case object at 0x7f35f40ee6b0>, <__main__.Case object at 0x7f35f40eead0>, <__main__.Case object at 0x7f35f40edd80>, <__main__.Case object at 0x7f35f40ef0a0>, <__main__.Case object at 0x7f35f40ef3d0>, <__main__.Case object at 0x7f35f40ef7f0>, <__main__.Case object at 0x7f35f40eeaa0>, <__main__.Case object at 0x7f35f40efd90>, <__main__.Case object at 0x7f35f40ec0a0>, <__main__.Case object at 0x7f35f40ec670>, <__main__.Case object at 0x7f35f40ef9d0>, <__main__.Case object at 0x7f35f40ecd00>, <__main__.Case object at 0x7f35f40ed180>, <__main__.Case object at 0x7f35f40ecac0>, <__main__.Case object at 0x7f35f40ec730>, <__main__.Case object at 0x7f35f40edc30>, <__main__.Case object at 0x7f35f40edf30>, <__main__.Case object at 0x7f35f40ee3b0>, <__main__.Case object at 0x7f35f40ed120>, <__main__.Case object at 0x7f35f40ee950>, <__main__.Case object at 0x7f35f40eec50>, <__main__.Case object at 0x7f35f40ef0d0>, <__main__.Case object at 0x7f35f40edf00>, <__main__.Case object at 0x7f35f40ef670>, <__main__.Case object at 0x7f35f40ef970>, <__main__.Case object at 0x7f35f40efdf0>, <__main__.Case object at 0x7f35f40eec20>, <__main__.Case object at 0x7f35f40ef910>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f40ec640>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f40ed750>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f40ec910>, <__main__.Case object at 0x7f35f409b1c0>, <__main__.Case object at 0x7f35f40f4070>, <__main__.Case object at 0x7f35f40f44f0>, <__main__.Case object at 0x7f35f40ee020>, <__main__.Case object at 0x7f35f40f5270>, <__main__.Case object at 0x7f35f40f4310>, <__main__.Case object at 0x7f35f40f46a0>, <__main__.Case object at 0x7f35f40ee2f0>, <__main__.Case object at 0x7f35f40f4b80>, <__main__.Case object at 0x7f35f40f4220>, <__main__.Case object at 0x7f35f40f47f0>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f40f5450>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f40f4a90>, <__main__.Case object at 0x7f35f40f6050>, <__main__.Case object at 0x7f35f40f6170>, <__main__.Case object at 0x7f35f409b0d0>, <__main__.Case object at 0x7f35f40f5e70>, <__main__.Case object at 0x7f35f40f64d0>, <__main__.Case object at 0x7f35f40f65f0>, <__main__.Case object at 0x7f35f40f5d50>, <__main__.Case object at 0x7f35f40f68f0>, <__main__.Case object at 0x7f35f40f6a10>, <__main__.Case object at 0x7f35f40f62f0>, <__main__.Case object at 0x7f35f40f6d10>, <__main__.Case object at 0x7f35f40f6e30>, <__main__.Case object at 0x7f35f40f6fb0>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f40f7190>, <__main__.Case object at 0x7f35f40f72b0>, <__main__.Case object at 0x7f35f40f7430>, <__main__.Case object at 0x7f35f40f4d90>, <__main__.Case object at 0x7f35f40f7610>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f40afd30>, <__main__.Case object at 0x7f35f40afaf0>, <__main__.Case object at 0x7f35f40ac400>, <__main__.Case object at 0x7f35f40afa30>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f40af670>, <__main__.Case object at 0x7f35f40afbb0>, <__main__.Case object at 0x7f35f40ae290>, <__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40af220>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f40ae1a0>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40adc60>, <__main__.Case object at 0x7f35f40af8b0>, <__main__.Case object at 0x7f35f40ac670>, <__main__.Case object at 0x7f35f40afc40>, <__main__.Case object at 0x7f35f40af730>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f40af0a0>, <__main__.Case object at 0x7f35f40ae800>, <__main__.Case object at 0x7f35f40aeb30>, <__main__.Case object at 0x7f35f40ae6e0>, <__main__.Case object at 0x7f35f40ac700>, <__main__.Case object at 0x7f35f40ad1b0>, <__main__.Case object at 0x7f35f40acc70>, <__main__.Case object at 0x7f35f40ac430>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40af400>, <__main__.Case object at 0x7f35f40ac8b0>, <__main__.Case object at 0x7f35f40afc70>, <__main__.Case object at 0x7f35f40af2b0>, <__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40aed40>, <__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40afe20>, <__main__.Case object at 0x7f35f40ac610>, <__main__.Case object at 0x7f35f40ae860>, <__main__.Case object at 0x7f35f40aff40>, <__main__.Case object at 0x7f35f40addb0>, <__main__.Case object at 0x7f35f40afdc0>, <__main__.Case object at 0x7f35f40bd450>, <__main__.Case object at 0x7f35f40ad060>, <__main__.Case object at 0x7f35f40be440>, <__main__.Case object at 0x7f35f40bef80>, <__main__.Case object at 0x7f35f40bd090>, <__main__.Case object at 0x7f35f40bd510>, <__main__.Case object at 0x7f35f40bd330>, <__main__.Case object at 0x7f35f40bde40>, <__main__.Case object at 0x7f35f40bf220>, <__main__.Case object at 0x7f35f40bf8b0>, <__main__.Case object at 0x7f35f40bd270>, <__main__.Case object at 0x7f35f40bc430>, <__main__.Case object at 0x7f35f40bce20>, <__main__.Case object at 0x7f35f40beaa0>, <__main__.Case object at 0x7f35f40bf4c0>, <__main__.Case object at 0x7f35f40bfdc0>, <__main__.Case object at 0x7f35f40bfa90>, <__main__.Case object at 0x7f35f40bdf90>, <__main__.Case object at 0x7f35f40bc160>, <__main__.Case object at 0x7f35f40bd5a0>, <__main__.Case object at 0x7f35f40bc2b0>, <__main__.Case object at 0x7f35f40be0b0>, <__main__.Case object at 0x7f35f40beec0>, <__main__.Case object at 0x7f35f40bc3a0>, <__main__.Case object at 0x7f35f40bcf10>, <__main__.Case object at 0x7f35f40bc6a0>, <__main__.Case object at 0x7f35f40be5c0>, <__main__.Case object at 0x7f35f40be740>, <__main__.Case object at 0x7f35f40af5e0>, <__main__.Case object at 0x7f35f40bd150>, <__main__.Case object at 0x7f35f40bf3d0>, <__main__.Case object at 0x7f35f40bcaf0>, <__main__.Case object at 0x7f35f40bcb20>, <__main__.Case object at 0x7f35f40be770>, <__main__.Case object at 0x7f35f40bfd00>, <__main__.Case object at 0x7f35f40bfdf0>, <__main__.Case object at 0x7f35f40bd5d0>, <__main__.Case object at 0x7f35f40bdbd0>, <__main__.Case object at 0x7f35f40bc550>, <__main__.Case object at 0x7f35f40bdfc0>, <__main__.Case object at 0x7f35f40bc9a0>, <__main__.Case object at 0x7f35f40bd990>, <__main__.Case object at 0x7f35f40bc3d0>, <__main__.Case object at 0x7f35f40bc940>, <__main__.Case object at 0x7f35f40bee30>, <__main__.Case object at 0x7f35f40bead0>, <__main__.Case object at 0x7f35f40bf430>, <__main__.Case object at 0x7f35f40bd3f0>, <__main__.Case object at 0x7f35f40bc7f0>, <__main__.Case object at 0x7f35f40bc820>, <__main__.Case object at 0x7f35f40be260>, <__main__.Case object at 0x7f35f40bf070>, <__main__.Case object at 0x7f35f40bf550>, <__main__.Case object at 0x7f35f40bfee0>, <__main__.Case object at 0x7f35f40bc910>, <__main__.Case object at 0x7f35f40bf970>, <__main__.Case object at 0x7f35f40bece0>, <__main__.Case object at 0x7f35f40ec160>, <__main__.Case object at 0x7f35f40ec5b0>, <__main__.Case object at 0x7f35f40bc1f0>, <__main__.Case object at 0x7f35f40eceb0>, <__main__.Case object at 0x7f35f40ed0f0>, <__main__.Case object at 0x7f35f40ed660>, <__main__.Case object at 0x7f35f40eda80>, <__main__.Case object at 0x7f35f40edf90>, <__main__.Case object at 0x7f35f40ee0b0>, <__main__.Case object at 0x7f35f40bcc70>, <__main__.Case object at 0x7f35f40ee7a0>, <__main__.Case object at 0x7f35f40bf1c0>, <__main__.Case object at 0x7f35f40eecb0>, <__main__.Case object at 0x7f35f40ef250>, <__main__.Case object at 0x7f35f40bd480>, <__main__.Case object at 0x7f35f40efb20>, <__main__.Case object at 0x7f35f40efc40>, <__main__.Case object at 0x7f35f40ec040>, <__main__.Case object at 0x7f35f40ef5b0>, <__main__.Case object at 0x7f35f40ec850>, <__main__.Case object at 0x7f35f40eeef0>, <__main__.Case object at 0x7f35f40ecb20>, <__main__.Case object at 0x7f35f40ecfa0>, <__main__.Case object at 0x7f35f40ed3c0>, <__main__.Case object at 0x7f35f40eca00>, <__main__.Case object at 0x7f35f40ed870>, <__main__.Case object at 0x7f35f40ee410>, <__main__.Case object at 0x7f35f40bc220>, <__main__.Case object at 0x7f35f40ee230>, <__main__.Case object at 0x7f35f40ee590>, <__main__.Case object at 0x7f35f40ee8f0>, <__main__.Case object at 0x7f35f40ec3a0>, <__main__.Case object at 0x7f35f40eef50>, <__main__.Case object at 0x7f35f40bf5b0>, <__main__.Case object at 0x7f35f40ef610>, <__main__.Case object at 0x7f35f40ef9a0>, <__main__.Case object at 0x7f35f40efc70>, <__main__.Case object at 0x7f35f40eff70>, <__main__.Case object at 0x7f35f40ec430>, <__main__.Case object at 0x7f35f40ec760>, <__main__.Case object at 0x7f35f40ecbe0>, <__main__.Case object at 0x7f35f40ed090>, <__main__.Case object at 0x7f35f40ed360>, <__main__.Case object at 0x7f35f40ed7b0>, <__main__.Case object at 0x7f35f40edb10>, <__main__.Case object at 0x7f35f40ede10>, <__main__.Case object at 0x7f35f40ee170>, <__main__.Case object at 0x7f35f40ee4a0>, <__main__.Case object at 0x7f35f40ee830>, <__main__.Case object at 0x7f35f40eeb30>, <__main__.Case object at 0x7f35f40eee90>, <__main__.Case object at 0x7f35f40ef2b0>, <__main__.Case object at 0x7f35f40ef550>, <__main__.Case object at 0x7f35f40ef850>, <__main__.Case object at 0x7f35f40edbd0>, <__main__.Case object at 0x7f35f40ed570>, <__main__.Case object at 0x7f35f40ec9a0>, <__main__.Case object at 0x7f35f409b3a0>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f40eff40>, <__main__.Case object at 0x7f35f40999c0>, <__main__.Case object at 0x7f35f409b070>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409bb20>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f40f4e80>, <__main__.Case object at 0x7f35f40f43d0>, <__main__.Case object at 0x7f35f40f4820>, <__main__.Case object at 0x7f35f40f4d60>, <__main__.Case object at 0x7f35f40f41f0>, <__main__.Case object at 0x7f35f40f4550>, <__main__.Case object at 0x7f35f40f4910>, <__main__.Case object at 0x7f35f40f4dc0>, <__main__.Case object at 0x7f35f40f4100>, <__main__.Case object at 0x7f35f40f45b0>, <__main__.Case object at 0x7f35f40f4340>, <__main__.Case object at 0x7f35f40f4eb0>, <__main__.Case object at 0x7f35f40f5300>, <__main__.Case object at 0x7f35f40f5360>, <__main__.Case object at 0x7f35f40f5db0>, <__main__.Case object at 0x7f35f40f5f00>, <__main__.Case object at 0x7f35f40f5ff0>, <__main__.Case object at 0x7f35f40f4a60>, <__main__.Case object at 0x7f35f40f6230>, <__main__.Case object at 0x7f35f40f6380>, <__main__.Case object at 0x7f35f40f6470>, <__main__.Case object at 0x7f35f40f4580>, <__main__.Case object at 0x7f35f40f6590>, <__main__.Case object at 0x7f35f40f6710>, <__main__.Case object at 0x7f35f40f6890>, <__main__.Case object at 0x7f35f40f69b0>, <__main__.Case object at 0x7f35f40f6ad0>, <__main__.Case object at 0x7f35f40f6b30>, <__main__.Case object at 0x7f35f40f66b0>, <__main__.Case object at 0x7f35f40f6dd0>, <__main__.Case object at 0x7f35f40f6ef0>, <__main__.Case object at 0x7f35f40f6f50>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f40f7250>, <__main__.Case object at 0x7f35f40f7370>, <__main__.Case object at 0x7f35f40f74c0>, <__main__.Case object at 0x7f35f40f4f40>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.9999999999999999, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 40\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.9999999999999999, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.9999999999999999, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.9999999999999999, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.9999999999999999, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.9999999999999999, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 42\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 43\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.9999999999999999, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.9999999999999999, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9999999999999999, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.9999999999999999, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.9999999999999999, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 42\n",
      "Episode: 7, Total Steps: 207, Total Rewards: [-306, 86], Status Episode: False\n",
      "------------------------------------------End of episode 7 loop--------------------\n",
      "----- starting point of Episode 8 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.9999999999999999, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.9999999999999999, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.9999999999999999, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.9999999999999999, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.9999999999999999, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.9999999999999999, 40)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.9999999999999999, 41)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.9999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.8999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 4) with action 1 to next state (5, 3): pull reward: -0.1\n",
      "----- starting point of Episode 8 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.8999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.8999999999999999, 43)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 2 to next state (2, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f40ae2f0>, <__main__.Case object at 0x7f35f40ac730>, <__main__.Case object at 0x7f35f40af280>, <__main__.Case object at 0x7f35f40af3a0>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f40af190>, <__main__.Case object at 0x7f35f40accd0>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40affa0>, <__main__.Case object at 0x7f35f40ae5c0>, <__main__.Case object at 0x7f35f40af790>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40af940>, <__main__.Case object at 0x7f35f40ace80>, <__main__.Case object at 0x7f35f40afb20>, <__main__.Case object at 0x7f35f40adb70>, <__main__.Case object at 0x7f35f40ae680>, <__main__.Case object at 0x7f35f40ae710>, <__main__.Case object at 0x7f35f40ad240>, <__main__.Case object at 0x7f35f40afd90>, <__main__.Case object at 0x7f35f40add20>, <__main__.Case object at 0x7f35f40adfc0>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40aef80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f40651b0>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40add80>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40aeef0>, <__main__.Case object at 0x7f35f40afbe0>, <__main__.Case object at 0x7f35f40aee00>, <__main__.Case object at 0x7f35f40ac880>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40adc90>, <__main__.Case object at 0x7f35f40ae650>, <__main__.Case object at 0x7f35f40af820>, <__main__.Case object at 0x7f35f40ac640>, <__main__.Case object at 0x7f35f40afca0>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40afd00>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40af7f0>, <__main__.Case object at 0x7f35f40af100>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f40ae3e0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 42\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f40acb80>, <__main__.Case object at 0x7f35f40af040>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40af580>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f40ae500>, <__main__.Case object at 0x7f35f40af460>, <__main__.Case object at 0x7f35f40ae110>, <__main__.Case object at 0x7f35f40ad9c0>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40aeb90>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40ac1f0>, <__main__.Case object at 0x7f35f40aefe0>, <__main__.Case object at 0x7f35f40ac8e0>, <__main__.Case object at 0x7f35f40af4f0>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40af6d0>, <__main__.Case object at 0x7f35f40af4c0>, <__main__.Case object at 0x7f35f40af340>, <__main__.Case object at 0x7f35f40ad7e0>, <__main__.Case object at 0x7f35f40ac5e0>, <__main__.Case object at 0x7f35f40aded0>, <__main__.Case object at 0x7f35f40ac5b0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.9999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.9999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.9999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 40\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 1, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 1, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.9999999999999999, time steps: 42\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 4), 0, 0.5, 27)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 3), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 3), 3, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 1, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 3, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 3, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 1), 1, 0.5, 27)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "Episode: 8, Total Steps: 27, Total Rewards: [-126, 88], Status Episode: False\n",
      "------------------------------------------End of episode 8 loop--------------------\n",
      "----- starting point of Episode 9 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 4) with action 3 to next state (5, 4): pull reward: 0.1\n",
      "----- starting point of Episode 9 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 9 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0.0402672460770966\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 2 to next state (5, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 0 to next state (5, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 2 to next state (5, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 0 to next state (5, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 4 to next state (6, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 9 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 2) with action 2 to next state (6, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40af760>, <__main__.Case object at 0x7f35f40aeef0>, <__main__.Case object at 0x7f35f40ac880>, <__main__.Case object at 0x7f35f40af7c0>, <__main__.Case object at 0x7f35f40ae560>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40af7f0>, <__main__.Case object at 0x7f35f40ac730>, <__main__.Case object at 0x7f35f40af3a0>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40af790>, <__main__.Case object at 0x7f35f40ace80>, <__main__.Case object at 0x7f35f40adb70>, <__main__.Case object at 0x7f35f40afd90>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40af970>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40aed70>, <__main__.Case object at 0x7f35f40afdf0>, <__main__.Case object at 0x7f35f40ad2a0>, <__main__.Case object at 0x7f35f40ac190>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40adff0>, <__main__.Case object at 0x7f35f40afcd0>, <__main__.Case object at 0x7f35f40af730>, <__main__.Case object at 0x7f35f40ac280>, <__main__.Case object at 0x7f35f40ae1a0>, <__main__.Case object at 0x7f35f40af220>, <__main__.Case object at 0x7f35f40af670>, <__main__.Case object at 0x7f35f40ae950>, <__main__.Case object at 0x7f35f40ada50>, <__main__.Case object at 0x7f35f40ac430>, <__main__.Case object at 0x7f35f40aca90>, <__main__.Case object at 0x7f35f40af1f0>, <__main__.Case object at 0x7f35f40acd30>, <__main__.Case object at 0x7f35f40ae0e0>, <__main__.Case object at 0x7f35f40afdc0>, <__main__.Case object at 0x7f35f40bec50>, <__main__.Case object at 0x7f35f40bef50>, <__main__.Case object at 0x7f35f40bf5e0>, <__main__.Case object at 0x7f35f40bdf00>, <__main__.Case object at 0x7f35f40bd840>, <__main__.Case object at 0x7f35f40be6e0>, <__main__.Case object at 0x7f35f40be290>, <__main__.Case object at 0x7f35f40bd4b0>, <__main__.Case object at 0x7f35f40bedd0>, <__main__.Case object at 0x7f35f40bd690>, <__main__.Case object at 0x7f35f40bca30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40651b0>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40aee00>, <__main__.Case object at 0x7f35f40ada80>, <__main__.Case object at 0x7f35f40af430>, <__main__.Case object at 0x7f35f40ae3b0>, <__main__.Case object at 0x7f35f40ae2f0>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ae5c0>, <__main__.Case object at 0x7f35f40af940>, <__main__.Case object at 0x7f35f40ae680>, <__main__.Case object at 0x7f35f40ac250>, <__main__.Case object at 0x7f35f40adfc0>, <__main__.Case object at 0x7f35f40af910>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f40ad9c0>, <__main__.Case object at 0x7f35f40aedd0>, <__main__.Case object at 0x7f35f40ac6d0>, <__main__.Case object at 0x7f35f40ae770>, <__main__.Case object at 0x7f35f40ae7d0>, <__main__.Case object at 0x7f35f40acd90>, <__main__.Case object at 0x7f35f40ae800>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40af8b0>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40ae320>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f40ac400>, <__main__.Case object at 0x7f35f40ad510>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40ac940>, <__main__.Case object at 0x7f35f40ae140>, <__main__.Case object at 0x7f35f40adb10>, <__main__.Case object at 0x7f35f40ace20>, <__main__.Case object at 0x7f35f40ae5f0>, <__main__.Case object at 0x7f35f40be590>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40bf280>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40af0a0>, <__main__.Case object at 0x7f35f40bc190>, <__main__.Case object at 0x7f35f40bec80>, <__main__.Case object at 0x7f35f40bfb20>, <__main__.Case object at 0x7f35f40ac0a0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 69\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.6, time steps: 66\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.4, time steps: 42\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.4, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.4, time steps: 40\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.4, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.4, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.4, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.4, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.4, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.4, time steps: 42\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 3), 2, 0.5, 53)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 3), 3, 0.5, 53)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 1, 0.5, 53)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 53)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 2, 0.5)\n",
      "Episode succeeded, case (5, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 3, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.5)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.5)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 45\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 47\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 46\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 44\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 35\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 34\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (2, 1), solution: 3, tv: 0.5, time steps: 31\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 0\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f40afa90>, <__main__.Case object at 0x7f35f40ae3e0>, <__main__.Case object at 0x7f35f40afa00>, <__main__.Case object at 0x7f35f40adc90>, <__main__.Case object at 0x7f35f40afbe0>, <__main__.Case object at 0x7f35f40ae830>, <__main__.Case object at 0x7f35f40af160>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40affd0>, <__main__.Case object at 0x7f35f40affa0>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40afb20>, <__main__.Case object at 0x7f35f40ac7c0>, <__main__.Case object at 0x7f35f40add20>, <__main__.Case object at 0x7f35f40aef80>, <__main__.Case object at 0x7f35f40ae9b0>, <__main__.Case object at 0x7f35f40ac9d0>, <__main__.Case object at 0x7f35f40af280>, <__main__.Case object at 0x7f35f40ae890>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40af6d0>, <__main__.Case object at 0x7f35f40adf30>, <__main__.Case object at 0x7f35f40aea40>, <__main__.Case object at 0x7f35f40aeb30>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f40afeb0>, <__main__.Case object at 0x7f35f40ae230>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f40ae740>, <__main__.Case object at 0x7f35f40ae110>, <__main__.Case object at 0x7f35f40aee30>, <__main__.Case object at 0x7f35f40ad750>, <__main__.Case object at 0x7f35f40af2e0>, <__main__.Case object at 0x7f35f40ae4a0>, <__main__.Case object at 0x7f35f40afee0>, <__main__.Case object at 0x7f35f40bcf40>, <__main__.Case object at 0x7f35f40bc2e0>, <__main__.Case object at 0x7f35f40bd000>, <__main__.Case object at 0x7f35f40bdd80>, <__main__.Case object at 0x7f35f40bc850>, <__main__.Case object at 0x7f35f40be380>, <__main__.Case object at 0x7f35f40bc0d0>, <__main__.Case object at 0x7f35f40bf9d0>, <__main__.Case object at 0x7f35f40be620>, <__main__.Case object at 0x7f35f40bffa0>, <__main__.Case object at 0x7f35f40bcc40>, <__main__.Case object at 0x7f35f40bd060>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 27\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6, time steps: 27\n",
      "Episode: 9, Total Steps: 53, Total Rewards: [48, 88], Status Episode: True\n",
      "------------------------------------------End of episode 9 loop--------------------\n",
      "----- starting point of Episode 10 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 34)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 36)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 41)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 42)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 1), 2, 0.5, 44)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 2), 4, 0.5, 46)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 47)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.6, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 3) with action 0 to next state (6, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.6, 45)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 53)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 4), 1, 0.5, 53)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 4), 1, 0.5, 53)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 4), 1, 0.5, 53)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40ad330>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40af9a0>, <__main__.Case object at 0x7f35f40ad9c0>, <__main__.Case object at 0x7f35f40acd90>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40ad510>, <__main__.Case object at 0x7f35f40ac940>, <__main__.Case object at 0x7f35f40af0a0>, <__main__.Case object at 0x7f35f40ac880>, <__main__.Case object at 0x7f35f40af7f0>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40ad5d0>, <__main__.Case object at 0x7f35f40af250>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f40ac4c0>, <__main__.Case object at 0x7f35f40af820>, <__main__.Case object at 0x7f35f40af190>, <__main__.Case object at 0x7f35f40ae710>, <__main__.Case object at 0x7f35f40aedd0>, <__main__.Case object at 0x7f35f40ae7d0>, <__main__.Case object at 0x7f35f40af8b0>, <__main__.Case object at 0x7f35f40ac400>, <__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40aeef0>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40aff70>, <__main__.Case object at 0x7f35f40af310>, <__main__.Case object at 0x7f35f40af370>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.6, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.6, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 47\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 46\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.6, time steps: 44\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.6, time steps: 42\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 35\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 34\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.4, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 3, tv: 0.4, time steps: 31\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 3), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 3), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 1, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 15)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 2), 4, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 1), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 4, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 0), 4, 0.5, 15)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.6)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.6)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.6)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.6)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 3, 0.6)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.6)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.6)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 1, 0.6)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1, time steps: 22\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40ad1b0>, <__main__.Case object at 0x7f35f40aca30>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40af040>, <__main__.Case object at 0x7f35f40af610>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40ad060>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40ae560>, <__main__.Case object at 0x7f35f40af3a0>, <__main__.Case object at 0x7f35f40ad240>, <__main__.Case object at 0x7f35f40af460>, <__main__.Case object at 0x7f35f40af6a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f35f40ad8d0>, <__main__.Case object at 0x7f35f40ac550>, <__main__.Case object at 0x7f35f40aee60>, <__main__.Case object at 0x7f35f40af880>, <__main__.Case object at 0x7f35f40af850>, <__main__.Case object at 0x7f35f40ae800>, <__main__.Case object at 0x7f35f40ae320>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40ace20>, <__main__.Case object at 0x7f35f40af7c0>, <__main__.Case object at 0x7f35f40ac730>, <__main__.Case object at 0x7f35f40adb70>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40ac8e0>]\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.7, time steps: 27\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 4), 0, 0.5, 15)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 3), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 3), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 1, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 1), 1, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 1), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 0), 2, 0.5, 15)\n",
      "Integrated case process. comm case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.5)\n",
      "Integrated case process. comm case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.5)\n",
      "Integrated case process. comm case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 47\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 46\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 44\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 35\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 34\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 0\n",
      "Episode: 10, Total Steps: 15, Total Rewards: [86, 88], Status Episode: True\n",
      "------------------------------------------End of episode 10 loop--------------------\n",
      "----- starting point of Episode 11 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 0 to next state (5, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 1), 2, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 2), 4, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 4) with action 1 to next state (5, 3): pull reward: 0.1\n",
      "----- starting point of Episode 11 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 11 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.5, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40ac8e0>, <__main__.Case object at 0x7f35f40adb40>, <__main__.Case object at 0x7f35f40accd0>, <__main__.Case object at 0x7f35f40ae2f0>, <__main__.Case object at 0x7f35f40ae770>, <__main__.Case object at 0x7f35f40ae860>, <__main__.Case object at 0x7f35f40ae140>, <__main__.Case object at 0x7f35f40af430>, <__main__.Case object at 0x7f35f40aeb90>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40afe50>, <__main__.Case object at 0x7f35f40ac7c0>, <__main__.Case object at 0x7f35f40affd0>, <__main__.Case object at 0x7f35f40af160>, <__main__.Case object at 0x7f35f40af070>, <__main__.Case object at 0x7f35f40acd30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f40adcf0>, <__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40afd00>, <__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40ad660>, <__main__.Case object at 0x7f35f40ad510>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f40aded0>, <__main__.Case object at 0x7f35f40af2b0>, <__main__.Case object at 0x7f35f40ad3c0>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40ad540>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "Episode succeeded, case (4, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.5)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7, time steps: 0\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40af970>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40af910>, <__main__.Case object at 0x7f35f40aff40>, <__main__.Case object at 0x7f35f40ac640>, <__main__.Case object at 0x7f35f40af5e0>, <__main__.Case object at 0x7f35f40ac250>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40aed70>, <__main__.Case object at 0x7f35f40aebc0>, <__main__.Case object at 0x7f35f40aefe0>, <__main__.Case object at 0x7f35f40ae650>, <__main__.Case object at 0x7f35f40af9d0>, <__main__.Case object at 0x7f35f40ac8b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f35f40af6a0>, <__main__.Case object at 0x7f35f40afb80>, <__main__.Case object at 0x7f35f40afd90>, <__main__.Case object at 0x7f35f40af940>, <__main__.Case object at 0x7f35f40af760>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40afb20>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f40afca0>, <__main__.Case object at 0x7f35f40aece0>, <__main__.Case object at 0x7f35f40ada20>]\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.4, time steps: 47\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.4, time steps: 46\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.4, time steps: 44\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.4, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4, time steps: 35\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4, time steps: 34\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 1), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.6)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6, time steps: 15\n",
      "Episode: 11, Total Steps: 16, Total Rewards: [85, 88], Status Episode: True\n",
      "------------------------------------------End of episode 11 loop--------------------\n",
      "----- starting point of Episode 12 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7, 0)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 0) with action 4 to next state (9, 0): pull reward: 0.0\n",
      "----- starting point of Episode 12 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.6, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 0 to next state (8, 1): pull reward: 0.0\n",
      "----- starting point of Episode 12 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 1, 0.6, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 2, 0.6, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 3 to next state (4, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 0) with action 1 to next state (7, 0): pull reward: 0.0\n",
      "----- starting point of Episode 12 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 4 to next state (4, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 0) with action 3 to next state (6, 0): pull reward: 0.1\n",
      "----- starting point of Episode 12 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 3 to next state (4, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0.0901394155679433\n",
      "----- starting point of Episode 12 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 4 to next state (4, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 1 to next state (5, 0): pull reward: 0.0\n",
      "----- starting point of Episode 12 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 4 to next state (4, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 2 to next state (5, 1): pull reward: 0.1\n",
      "----- starting point of Episode 12 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 3 to next state (4, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 3 to next state (4, 1): pull reward: -0.007086693948455208\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40ac0a0>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40af2b0>, <__main__.Case object at 0x7f35f40ad8a0>, <__main__.Case object at 0x7f35f40af940>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f40affa0>, <__main__.Case object at 0x7f35f40adb40>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40ac880>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f40aca90>, <__main__.Case object at 0x7f35f40ad660>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40afd90>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.29999999999999993, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.6)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40afd30>, <__main__.Case object at 0x7f35f40aded0>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40aeef0>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ade40>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40ad300>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40acc70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f35f40ac8b0>, <__main__.Case object at 0x7f35f40aca00>, <__main__.Case object at 0x7f35f40ad3c0>, <__main__.Case object at 0x7f35f40ac190>, <__main__.Case object at 0x7f35f40af760>]\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 15\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.6)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.6)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.7)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7, time steps: 0\n",
      "Episode: 12, Total Steps: 12, Total Rewards: [-106, -111], Status Episode: False\n",
      "------------------------------------------End of episode 12 loop--------------------\n",
      "----- starting point of Episode 13 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 0) with action 3 to next state (8, 0): pull reward: 0.1\n",
      "----- starting point of Episode 13 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 0 to next state (8, 0): pull reward: 0.0\n",
      "----- starting point of Episode 13 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 2 to next state (8, 1): pull reward: -0.1\n",
      "----- starting point of Episode 13 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 3 to next state (7, 1): pull reward: 0.1\n",
      "----- starting point of Episode 13 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 0 to next state (6, 3): pull reward: 0.0\n",
      "----- starting point of Episode 13 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 4) with action 1 to next state (5, 3): pull reward: 0.1\n",
      "----- starting point of Episode 13 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 13 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 13 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 13 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 4 to next state (1, 4): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40ad660>, <__main__.Case object at 0x7f35f40acc70>, <__main__.Case object at 0x7f35f40af6a0>, <__main__.Case object at 0x7f35f40ad540>, <__main__.Case object at 0x7f35f40add50>, <__main__.Case object at 0x7f35f40affa0>, <__main__.Case object at 0x7f35f40ac880>, <__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40acac0>, <__main__.Case object at 0x7f35f40ac040>, <__main__.Case object at 0x7f35f40ad300>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40acb80>, <__main__.Case object at 0x7f35f40ad1b0>, <__main__.Case object at 0x7f35f40aff40>, <__main__.Case object at 0x7f35f40ac2b0>, <__main__.Case object at 0x7f35f40af1f0>, <__main__.Case object at 0x7f35f40ac7c0>, <__main__.Case object at 0x7f35f40afd60>, <__main__.Case object at 0x7f35f40ae950>, <__main__.Case object at 0x7f35f40ae380>, <__main__.Case object at 0x7f35f40ae0e0>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f40af280>, <__main__.Case object at 0x7f35f40ae9b0>, <__main__.Case object at 0x7f35f40ae1a0>, <__main__.Case object at 0x7f35f40adff0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f40afd90>, <__main__.Case object at 0x7f35f40af3a0>, <__main__.Case object at 0x7f35f40adb40>, <__main__.Case object at 0x7f35f40aded0>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40afe80>, <__main__.Case object at 0x7f35f40ad240>, <__main__.Case object at 0x7f35f40af610>, <__main__.Case object at 0x7f35f40af5e0>, <__main__.Case object at 0x7f35f40ad330>, <__main__.Case object at 0x7f35f40af160>, <__main__.Case object at 0x7f35f40aec20>, <__main__.Case object at 0x7f35f40afe50>, <__main__.Case object at 0x7f35f40ad930>, <__main__.Case object at 0x7f35f40ae560>, <__main__.Case object at 0x7f35f40ac1f0>, <__main__.Case object at 0x7f35f40af580>, <__main__.Case object at 0x7f35f40aeb30>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.6)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.6)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.6)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.6)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40ac490>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40afd00>, <__main__.Case object at 0x7f35f40af640>, <__main__.Case object at 0x7f35f40ad8a0>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40afd30>, <__main__.Case object at 0x7f35f40acf70>, <__main__.Case object at 0x7f35f40aff70>, <__main__.Case object at 0x7f35f40ad090>, <__main__.Case object at 0x7f35f40aebc0>, <__main__.Case object at 0x7f35f40ae650>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40acd90>, <__main__.Case object at 0x7f35f40ac730>, <__main__.Case object at 0x7f35f40ae3e0>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40ac430>, <__main__.Case object at 0x7f35f40ad0f0>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40ad690>, <__main__.Case object at 0x7f35f40af4c0>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40af3d0>, <__main__.Case object at 0x7f35f40ad6c0>, <__main__.Case object at 0x7f35f40afc40>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 0\n",
      "Episode: 13, Total Steps: 27, Total Rewards: [-126, 88], Status Episode: False\n",
      "------------------------------------------End of episode 13 loop--------------------\n",
      "----- starting point of Episode 14 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.5, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 0 to next state (8, 1): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.5, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 14 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 4 to next state (1, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f40adb10>, <__main__.Case object at 0x7f35f40afa90>, <__main__.Case object at 0x7f35f40ac520>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40affd0>, <__main__.Case object at 0x7f35f40afa00>, <__main__.Case object at 0x7f35f40ad7e0>, <__main__.Case object at 0x7f35f40ad9c0>, <__main__.Case object at 0x7f35f40accd0>, <__main__.Case object at 0x7f35f40ade40>, <__main__.Case object at 0x7f35f40af8e0>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40afbe0>, <__main__.Case object at 0x7f35f40ae050>, <__main__.Case object at 0x7f35f40af4f0>, <__main__.Case object at 0x7f35f40af790>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f40ae200>, <__main__.Case object at 0x7f35f40ae2f0>, <__main__.Case object at 0x7f35f40ac6a0>, <__main__.Case object at 0x7f35f40aef80>, <__main__.Case object at 0x7f35f40ad780>, <__main__.Case object at 0x7f35f40aff10>, <__main__.Case object at 0x7f35f40ae890>, <__main__.Case object at 0x7f35f40addb0>, <__main__.Case object at 0x7f35f40af2e0>, <__main__.Case object at 0x7f35f40aec80>, <__main__.Case object at 0x7f35f40ad480>, <__main__.Case object at 0x7f35f40adc30>, <__main__.Case object at 0x7f35f40ac550>, <__main__.Case object at 0x7f35f40ae800>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f40920b0>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f40af550>, <__main__.Case object at 0x7f35f40bec50>, <__main__.Case object at 0x7f35f40bf2b0>, <__main__.Case object at 0x7f35f40bf7f0>, <__main__.Case object at 0x7f35f40bec80>, <__main__.Case object at 0x7f35f40bdf00>, <__main__.Case object at 0x7f35f40bedd0>, <__main__.Case object at 0x7f35f40bca30>, <__main__.Case object at 0x7f35f40bdd80>, <__main__.Case object at 0x7f35f40bc0d0>, <__main__.Case object at 0x7f35f40bffa0>, <__main__.Case object at 0x7f35f40bf370>, <__main__.Case object at 0x7f35f40bcb50>, <__main__.Case object at 0x7f35f40bcee0>, <__main__.Case object at 0x7f35f40bca60>, <__main__.Case object at 0x7f35f40be320>, <__main__.Case object at 0x7f35f40bcca0>, <__main__.Case object at 0x7f35f40bc880>, <__main__.Case object at 0x7f35f40bdc00>, <__main__.Case object at 0x7f35f40beef0>, <__main__.Case object at 0x7f35f40bdcf0>, <__main__.Case object at 0x7f35f40bfca0>, <__main__.Case object at 0x7f35f40bc970>, <__main__.Case object at 0x7f35f40bc8b0>, <__main__.Case object at 0x7f35f40bc5e0>, <__main__.Case object at 0x7f35f40bf520>, <__main__.Case object at 0x7f35f40bd390>, <__main__.Case object at 0x7f35f40bef20>, <__main__.Case object at 0x7f35f40bcfd0>, <__main__.Case object at 0x7f35f40bf0d0>, <__main__.Case object at 0x7f35f40bd0c0>, <__main__.Case object at 0x7f35f40be9e0>, <__main__.Case object at 0x7f35f40bf310>, <__main__.Case object at 0x7f35f40bf9a0>, <__main__.Case object at 0x7f35f40bfaf0>, <__main__.Case object at 0x7f35f40bdcc0>, <__main__.Case object at 0x7f35f40bd090>, <__main__.Case object at 0x7f35f40bde40>, <__main__.Case object at 0x7f35f40bd270>, <__main__.Case object at 0x7f35f40bfdc0>, <__main__.Case object at 0x7f35f40bfa90>, <__main__.Case object at 0x7f35f40bc160>, <__main__.Case object at 0x7f35f40bcf70>, <__main__.Case object at 0x7f35f40bc6a0>, <__main__.Case object at 0x7f35f40be5c0>, <__main__.Case object at 0x7f35f40bd750>, <__main__.Case object at 0x7f35f40bf250>, <__main__.Case object at 0x7f35f40be350>, <__main__.Case object at 0x7f35f40bddb0>, <__main__.Case object at 0x7f35f40bd9c0>, <__main__.Case object at 0x7f35f40be050>, <__main__.Case object at 0x7f35f40bd2a0>, <__main__.Case object at 0x7f35f40bc130>, <__main__.Case object at 0x7f35f40bfcd0>, <__main__.Case object at 0x7f35f40bc1f0>, <__main__.Case object at 0x7f35f40ed060>, <__main__.Case object at 0x7f35f40efb80>, <__main__.Case object at 0x7f35f40ecdc0>, <__main__.Case object at 0x7f35f40eda20>, <__main__.Case object at 0x7f35f40ee740>, <__main__.Case object at 0x7f35f40efa00>, <__main__.Case object at 0x7f35f40ec3d0>, <__main__.Case object at 0x7f35f40ed330>, <__main__.Case object at 0x7f35f40edb40>, <__main__.Case object at 0x7f35f40ee860>, <__main__.Case object at 0x7f35f40ef220>, <__main__.Case object at 0x7f35f40efbe0>, <__main__.Case object at 0x7f35f40ec2e0>, <__main__.Case object at 0x7f35f40ed390>, <__main__.Case object at 0x7f35f40edd20>, <__main__.Case object at 0x7f35f40ee6e0>, <__main__.Case object at 0x7f35f40bd3f0>, <__main__.Case object at 0x7f35f40ef760>, <__main__.Case object at 0x7f35f40ef910>, <__main__.Case object at 0x7f35f40effa0>, <__main__.Case object at 0x7f35f40ec4c0>, <__main__.Case object at 0x7f35f40ed420>, <__main__.Case object at 0x7f35f40edb70>, <__main__.Case object at 0x7f35f40ee1d0>, <__main__.Case object at 0x7f35f40eedd0>, <__main__.Case object at 0x7f35f40efd00>, <__main__.Case object at 0x7f35f40ec580>, <__main__.Case object at 0x7f35f40ecca0>, <__main__.Case object at 0x7f35f40ed5a0>, <__main__.Case object at 0x7f35f40ee110>, <__main__.Case object at 0x7f35f40eea10>, <__main__.Case object at 0x7f35f40ef520>, <__main__.Case object at 0x7f35f40ec550>, <__main__.Case object at 0x7f35f40ecaf0>, <__main__.Case object at 0x7f35f40ed480>, <__main__.Case object at 0x7f35f40ee050>, <__main__.Case object at 0x7f35f40ee710>, <__main__.Case object at 0x7f35f40ef430>, <__main__.Case object at 0x7f35f40efcd0>, <__main__.Case object at 0x7f35f40ec160>, <__main__.Case object at 0x7f35f40edba0>, <__main__.Case object at 0x7f35f40edfc0>, <__main__.Case object at 0x7f35f40ee7a0>, <__main__.Case object at 0x7f35f40ef7c0>, <__main__.Case object at 0x7f35f40efc40>, <__main__.Case object at 0x7f35f40eca30>, <__main__.Case object at 0x7f35f40ecb20>, <__main__.Case object at 0x7f35f40ed870>, <__main__.Case object at 0x7f35f40ee3e0>, <__main__.Case object at 0x7f35f40ef100>, <__main__.Case object at 0x7f35f40ef9a0>, <__main__.Case object at 0x7f35f40ec430>, <__main__.Case object at 0x7f35f40ecbe0>, <__main__.Case object at 0x7f35f40edb10>, <__main__.Case object at 0x7f35f40ee4a0>, <__main__.Case object at 0x7f35f40eee90>, <__main__.Case object at 0x7f35f40ef550>, <__main__.Case object at 0x7f35f409b040>, <__main__.Case object at 0x7f35f40ec460>, <__main__.Case object at 0x7f35f409bd90>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f409ace0>, <__main__.Case object at 0x7f35f409b610>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409b2b0>, <__main__.Case object at 0x7f35f409b520>, <__main__.Case object at 0x7f35f40f45e0>, <__main__.Case object at 0x7f35f40f6fe0>, <__main__.Case object at 0x7f35f40f7610>, <__main__.Case object at 0x7f35f40f4730>, <__main__.Case object at 0x7f35f40f4310>, <__main__.Case object at 0x7f35f40f4460>, <__main__.Case object at 0x7f35f40f5ed0>, <__main__.Case object at 0x7f35f40f6200>, <__main__.Case object at 0x7f35f40f64d0>, <__main__.Case object at 0x7f35f40f6a10>, <__main__.Case object at 0x7f35f40f6e30>, <__main__.Case object at 0x7f35f40f7190>, <__main__.Case object at 0x7f35f40f7430>, <__main__.Case object at 0x7f35f40f4400>, <__main__.Case object at 0x7f35f40f4280>, <__main__.Case object at 0x7f35f40f4e50>, <__main__.Case object at 0x7f35f40f46d0>, <__main__.Case object at 0x7f35f40f5cf0>, <__main__.Case object at 0x7f35f40f60b0>, <__main__.Case object at 0x7f35f40f6410>, <__main__.Case object at 0x7f35f40f6650>, <__main__.Case object at 0x7f35f40f6a70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f40880a0>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f40af2b0>, <__main__.Case object at 0x7f35f40ac640>, <__main__.Case object at 0x7f35f40afc40>, <__main__.Case object at 0x7f35f40ada50>, <__main__.Case object at 0x7f35f40af520>, <__main__.Case object at 0x7f35f40acd00>, <__main__.Case object at 0x7f35f40ad060>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40ae860>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40ac070>, <__main__.Case object at 0x7f35f40aea40>, <__main__.Case object at 0x7f35f40af670>, <__main__.Case object at 0x7f35f40aea10>, <__main__.Case object at 0x7f35f40af0a0>, <__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f40ac250>, <__main__.Case object at 0x7f35f40aeb90>, <__main__.Case object at 0x7f35f40add20>, <__main__.Case object at 0x7f35f40aed70>, <__main__.Case object at 0x7f35f40ae4a0>, <__main__.Case object at 0x7f35f40af490>, <__main__.Case object at 0x7f35f40ae230>, <__main__.Case object at 0x7f35f40af5e0>, <__main__.Case object at 0x7f35f40ada80>, <__main__.Case object at 0x7f35f40ae680>, <__main__.Case object at 0x7f35f40ae9e0>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f4073340>, <__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40be4d0>, <__main__.Case object at 0x7f35f40bef50>, <__main__.Case object at 0x7f35f40bd4b0>, <__main__.Case object at 0x7f35f40af3d0>, <__main__.Case object at 0x7f35f40bf280>, <__main__.Case object at 0x7f35f40be380>, <__main__.Case object at 0x7f35f40be620>, <__main__.Case object at 0x7f35f40afeb0>, <__main__.Case object at 0x7f35f40bcf40>, <__main__.Case object at 0x7f35f40bf1f0>, <__main__.Case object at 0x7f35f40be1d0>, <__main__.Case object at 0x7f35f40ace20>, <__main__.Case object at 0x7f35f40be7d0>, <__main__.Case object at 0x7f35f40bd540>, <__main__.Case object at 0x7f35f40be3b0>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f40bfb80>, <__main__.Case object at 0x7f35f40bff40>, <__main__.Case object at 0x7f35f40bf610>, <__main__.Case object at 0x7f35f40bf0a0>, <__main__.Case object at 0x7f35f40bd210>, <__main__.Case object at 0x7f35f40bea10>, <__main__.Case object at 0x7f35f40bcdf0>, <__main__.Case object at 0x7f35f40bf880>, <__main__.Case object at 0x7f35f40bfb50>, <__main__.Case object at 0x7f35f40be0e0>, <__main__.Case object at 0x7f35f40bfac0>, <__main__.Case object at 0x7f35f40be410>, <__main__.Case object at 0x7f35f40be800>, <__main__.Case object at 0x7f35f40beb90>, <__main__.Case object at 0x7f35f40bd240>, <__main__.Case object at 0x7f35f40bf940>, <__main__.Case object at 0x7f35f40bd330>, <__main__.Case object at 0x7f35f40bf8b0>, <__main__.Case object at 0x7f35f40beaa0>, <__main__.Case object at 0x7f35f40bfbe0>, <__main__.Case object at 0x7f35f40beec0>, <__main__.Case object at 0x7f35f40bc1c0>, <__main__.Case object at 0x7f35f40bd660>, <__main__.Case object at 0x7f35f40be7a0>, <__main__.Case object at 0x7f35f40bc790>, <__main__.Case object at 0x7f35f40befb0>, <__main__.Case object at 0x7f35f40bd6f0>, <__main__.Case object at 0x7f35f40bf700>, <__main__.Case object at 0x7f35f40bff70>, <__main__.Case object at 0x7f35f40bffd0>, <__main__.Case object at 0x7f35f40bf970>, <__main__.Case object at 0x7f35f40bf790>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f40ec6d0>, <__main__.Case object at 0x7f35f40ed510>, <__main__.Case object at 0x7f35f40bd900>, <__main__.Case object at 0x7f35f40eec80>, <__main__.Case object at 0x7f35f40ef490>, <__main__.Case object at 0x7f35f40ecf10>, <__main__.Case object at 0x7f35f40bccd0>, <__main__.Case object at 0x7f35f40edc90>, <__main__.Case object at 0x7f35f40eeec0>, <__main__.Case object at 0x7f35f40ef940>, <__main__.Case object at 0x7f35f40bd5d0>, <__main__.Case object at 0x7f35f40eded0>, <__main__.Case object at 0x7f35f40ed9c0>, <__main__.Case object at 0x7f35f40ee4d0>, <__main__.Case object at 0x7f35f40bc670>, <__main__.Case object at 0x7f35f40eff10>, <__main__.Case object at 0x7f35f40ee020>, <__main__.Case object at 0x7f35f40be950>, <__main__.Case object at 0x7f35f40ef1f0>, <__main__.Case object at 0x7f35f40ec790>, <__main__.Case object at 0x7f35f40ef130>, <__main__.Case object at 0x7f35f40ec7f0>, <__main__.Case object at 0x7f35f40ec340>, <__main__.Case object at 0x7f35f40ee320>, <__main__.Case object at 0x7f35f40ed6c0>, <__main__.Case object at 0x7f35f40eee00>, <__main__.Case object at 0x7f35f40ee800>, <__main__.Case object at 0x7f35f40ef190>, <__main__.Case object at 0x7f35f40efeb0>, <__main__.Case object at 0x7f35f40ecc70>, <__main__.Case object at 0x7f35f40ed270>, <__main__.Case object at 0x7f35f40edd50>, <__main__.Case object at 0x7f35f40eea70>, <__main__.Case object at 0x7f35f40ef9d0>, <__main__.Case object at 0x7f35f40efa90>, <__main__.Case object at 0x7f35f40ed210>, <__main__.Case object at 0x7f35f40ed3f0>, <__main__.Case object at 0x7f35f40edf00>, <__main__.Case object at 0x7f35f40ee560>, <__main__.Case object at 0x7f35f40ef460>, <__main__.Case object at 0x7f35f40ec040>, <__main__.Case object at 0x7f35f40ee650>, <__main__.Case object at 0x7f35f40eca00>, <__main__.Case object at 0x7f35f40ee680>, <__main__.Case object at 0x7f35f40ef280>, <__main__.Case object at 0x7f35f40ef610>, <__main__.Case object at 0x7f35f40eff70>, <__main__.Case object at 0x7f35f40ed090>, <__main__.Case object at 0x7f35f40ed900>, <__main__.Case object at 0x7f35f40ee170>, <__main__.Case object at 0x7f35f40eeb30>, <__main__.Case object at 0x7f35f40ef850>, <__main__.Case object at 0x7f35f40ec0d0>, <__main__.Case object at 0x7f35f40ecfa0>, <__main__.Case object at 0x7f35f40ed0f0>, <__main__.Case object at 0x7f35f409b790>, <__main__.Case object at 0x7f35f40ec610>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f40ee590>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40ed5d0>, <__main__.Case object at 0x7f35f40f4f40>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40eea40>, <__main__.Case object at 0x7f35f40f7460>, <__main__.Case object at 0x7f35f40f5450>, <__main__.Case object at 0x7f35f40f60e0>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f40f46a0>, <__main__.Case object at 0x7f35f40f6d10>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f409bf70>, <__main__.Case object at 0x7f35f40f65f0>, <__main__.Case object at 0x7f35f40f4df0>, <__main__.Case object at 0x7f35f40f49a0>, <__main__.Case object at 0x7f35f409afe0>, <__main__.Case object at 0x7f35f40f4d90>, <__main__.Case object at 0x7f35f40f5f90>, <__main__.Case object at 0x7f35f40f6290>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f40f4af0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.7)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.7)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.7)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.7)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.7)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40aece0>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40aefe0>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40afe20>, <__main__.Case object at 0x7f35f40af6d0>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40add50>, <__main__.Case object at 0x7f35f40af220>, <__main__.Case object at 0x7f35f40ac8e0>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40af1f0>, <__main__.Case object at 0x7f35f40afdc0>, <__main__.Case object at 0x7f35f40af460>, <__main__.Case object at 0x7f35f40ac9d0>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40ae770>, <__main__.Case object at 0x7f35f40acf10>, <__main__.Case object at 0x7f35f40ae830>, <__main__.Case object at 0x7f35f40acd90>, <__main__.Case object at 0x7f35f40af070>, <__main__.Case object at 0x7f35f40ac580>, <__main__.Case object at 0x7f35f40ac700>, <__main__.Case object at 0x7f35f40adf30>, <__main__.Case object at 0x7f35f40aed40>, <__main__.Case object at 0x7f35f40aef50>, <__main__.Case object at 0x7f35f40ae530>, <__main__.Case object at 0x7f35f40ae470>, <__main__.Case object at 0x7f35f40ad8d0>, <__main__.Case object at 0x7f35f40af880>, <__main__.Case object at 0x7f35f40acdf0>, <__main__.Case object at 0x7f35f40ac2b0>, <__main__.Case object at 0x7f35f40ad6c0>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f4073160>, <__main__.Case object at 0x7f35f40adff0>, <__main__.Case object at 0x7f35f40bf130>, <__main__.Case object at 0x7f35f40bf460>, <__main__.Case object at 0x7f35f40bc5b0>, <__main__.Case object at 0x7f35f40acca0>, <__main__.Case object at 0x7f35f40bfb20>, <__main__.Case object at 0x7f35f40be290>, <__main__.Case object at 0x7f35f40bd690>, <__main__.Case object at 0x7f35f40bdf60>, <__main__.Case object at 0x7f35f40bc850>, <__main__.Case object at 0x7f35f40bf9d0>, <__main__.Case object at 0x7f35f40bcc40>, <__main__.Case object at 0x7f35f40bcd30>, <__main__.Case object at 0x7f35f40bc400>, <__main__.Case object at 0x7f35f40bc070>, <__main__.Case object at 0x7f35f40bdb10>, <__main__.Case object at 0x7f35f40bf640>, <__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f40bed70>, <__main__.Case object at 0x7f35f40ae290>, <__main__.Case object at 0x7f35f40bc4c0>, <__main__.Case object at 0x7f35f40bc0a0>, <__main__.Case object at 0x7f35f40be500>, <__main__.Case object at 0x7f35f40bd120>, <__main__.Case object at 0x7f35f40bfa30>, <__main__.Case object at 0x7f35f40be470>, <__main__.Case object at 0x7f35f40bfe50>, <__main__.Case object at 0x7f35f40bcd90>, <__main__.Case object at 0x7f35f40bfc10>, <__main__.Case object at 0x7f35f40bd3c0>, <__main__.Case object at 0x7f35f40bcc10>, <__main__.Case object at 0x7f35f40bdff0>, <__main__.Case object at 0x7f35f40bc370>, <__main__.Case object at 0x7f35f40be3e0>, <__main__.Case object at 0x7f35f40bfe20>, <__main__.Case object at 0x7f35f40be650>, <__main__.Case object at 0x7f35f40bd1b0>, <__main__.Case object at 0x7f35f40bd510>, <__main__.Case object at 0x7f35f40bf220>, <__main__.Case object at 0x7f35f40bc430>, <__main__.Case object at 0x7f35f40bce20>, <__main__.Case object at 0x7f35f40bdf90>, <__main__.Case object at 0x7f35f40bf7c0>, <__main__.Case object at 0x7f35f40bc2b0>, <__main__.Case object at 0x7f35f40be0b0>, <__main__.Case object at 0x7f35f40be740>, <__main__.Case object at 0x7f35f40bd5a0>, <__main__.Case object at 0x7f35f40bfeb0>, <__main__.Case object at 0x7f35f40bdbd0>, <__main__.Case object at 0x7f35f40bf6d0>, <__main__.Case object at 0x7f35f40bd2d0>, <__main__.Case object at 0x7f35f40beda0>, <__main__.Case object at 0x7f35f40bc3a0>, <__main__.Case object at 0x7f35f40be5f0>, <__main__.Case object at 0x7f35f40bf850>, <__main__.Case object at 0x7f35f40bf1c0>, <__main__.Case object at 0x7f35f40edf60>, <__main__.Case object at 0x7f35f40ec280>, <__main__.Case object at 0x7f35f40ed000>, <__main__.Case object at 0x7f35f40ee350>, <__main__.Case object at 0x7f35f40ef160>, <__main__.Case object at 0x7f35f40efd60>, <__main__.Case object at 0x7f35f40ec7c0>, <__main__.Case object at 0x7f35f40bdae0>, <__main__.Case object at 0x7f35f40ee380>, <__main__.Case object at 0x7f35f40eebf0>, <__main__.Case object at 0x7f35f40ed720>, <__main__.Case object at 0x7f35f40efe80>, <__main__.Case object at 0x7f35f40ecd00>, <__main__.Case object at 0x7f35f40ec8e0>, <__main__.Case object at 0x7f35f40ec490>, <__main__.Case object at 0x7f35f40ec880>, <__main__.Case object at 0x7f35f40eed40>, <__main__.Case object at 0x7f35f40ef580>, <__main__.Case object at 0x7f35f40ed750>, <__main__.Case object at 0x7f35f40eeb90>, <__main__.Case object at 0x7f35f40ecdf0>, <__main__.Case object at 0x7f35f40ed930>, <__main__.Case object at 0x7f35f40ecb80>, <__main__.Case object at 0x7f35f40ee890>, <__main__.Case object at 0x7f35f40ef5e0>, <__main__.Case object at 0x7f35f40ef010>, <__main__.Case object at 0x7f35f40ec2b0>, <__main__.Case object at 0x7f35f40ed0c0>, <__main__.Case object at 0x7f35f40bdc60>, <__main__.Case object at 0x7f35f40ee470>, <__main__.Case object at 0x7f35f40eee30>, <__main__.Case object at 0x7f35f40ef730>, <__main__.Case object at 0x7f35f40efb50>, <__main__.Case object at 0x7f35f40ecf70>, <__main__.Case object at 0x7f35f40edc60>, <__main__.Case object at 0x7f35f40ee290>, <__main__.Case object at 0x7f35f40ed780>, <__main__.Case object at 0x7f35f40ef790>, <__main__.Case object at 0x7f35f40efa60>, <__main__.Case object at 0x7f35f40ec5b0>, <__main__.Case object at 0x7f35f40ecee0>, <__main__.Case object at 0x7f35f40ee2c0>, <__main__.Case object at 0x7f35f40ef070>, <__main__.Case object at 0x7f35f40efb20>, <__main__.Case object at 0x7f35f40ec970>, <__main__.Case object at 0x7f35f40eeef0>, <__main__.Case object at 0x7f35f40ed3c0>, <__main__.Case object at 0x7f35f40ed6f0>, <__main__.Case object at 0x7f35f40ee8f0>, <__main__.Case object at 0x7f35f40ef3a0>, <__main__.Case object at 0x7f35f40efc70>, <__main__.Case object at 0x7f35f40ec760>, <__main__.Case object at 0x7f35f40ed8a0>, <__main__.Case object at 0x7f35f40ede10>, <__main__.Case object at 0x7f35f40ed9f0>, <__main__.Case object at 0x7f35f40ef2b0>, <__main__.Case object at 0x7f35f40ed570>, <__main__.Case object at 0x7f35f409ac80>, <__main__.Case object at 0x7f35f409ad10>, <__main__.Case object at 0x7f35f409ae90>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f40ee830>, <__main__.Case object at 0x7f35f409b1c0>, <__main__.Case object at 0x7f35f409b3d0>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f40ee410>, <__main__.Case object at 0x7f35f409b8e0>, <__main__.Case object at 0x7f35f409bcd0>, <__main__.Case object at 0x7f35f409b310>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409b910>, <__main__.Case object at 0x7f35f40f6cb0>, <__main__.Case object at 0x7f35f40f6bc0>, <__main__.Case object at 0x7f35f40ac5e0>, <__main__.Case object at 0x7f35f40f5270>, <__main__.Case object at 0x7f35f40f4b80>, <__main__.Case object at 0x7f35f40f4970>, <__main__.Case object at 0x7f35f40f5fc0>, <__main__.Case object at 0x7f35f40f5e70>, <__main__.Case object at 0x7f35f40f6860>, <__main__.Case object at 0x7f35f40ad5d0>, <__main__.Case object at 0x7f35f40f6fb0>, <__main__.Case object at 0x7f35f40f72b0>, <__main__.Case object at 0x7f35f40f51b0>, <__main__.Case object at 0x7f35f40f48b0>, <__main__.Case object at 0x7f35f40f4520>, <__main__.Case object at 0x7f35f40f4250>, <__main__.Case object at 0x7f35f40f5390>, <__main__.Case object at 0x7f35f40f5e10>, <__main__.Case object at 0x7f35f40f61d0>, <__main__.Case object at 0x7f35f40f6530>, <__main__.Case object at 0x7f35f40bdc90>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.5, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 0\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 0\n",
      "Episode: 14, Total Steps: 186, Total Rewards: [-285, 89], Status Episode: False\n",
      "------------------------------------------End of episode 14 loop--------------------\n",
      "----- starting point of Episode 15 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 0) with action 4 to next state (9, 0): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.6, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.6, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 3) with action 0 to next state (5, 3): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 3 to next state (3, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 186 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 187 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 188 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 189 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 190 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 191 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 192 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 193 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 194 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 195 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 196 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 197 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 198 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 199 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 200 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 201 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 202 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 203 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 204 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 205 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 206 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 207 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 208 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 209 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 210 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 211 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 212 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 213 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 214 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 215 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 216 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 217 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 218 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 219 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 220 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 221 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 222 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 223 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 224 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 225 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 226 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 227 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 228 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 229 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 230 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 231 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 232 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 233 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 234 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 235 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 236 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 237 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 238 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 239 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 240 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 241 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 242 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 243 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 244 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 245 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 246 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 247 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 248 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 249 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 250 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 251 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 252 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 253 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 254 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 255 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 256 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 257 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 258 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 259 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 260 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 261 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 262 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 263 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 264 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 265 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 266 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 267 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 268 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 269 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 270 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 271 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 272 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 273 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 274 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 275 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 276 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 277 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 278 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 279 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 280 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 281 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 282 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 283 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 284 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 285 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 286 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 287 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 288 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 289 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 290 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 291 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 292 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 293 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 294 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 295 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 296 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 297 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 298 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 299 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 300 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 301 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 302 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 303 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 304 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 305 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 306 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 307 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 308 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 309 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 310 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 311 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 312 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 313 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 314 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 15 in steps 315 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 316 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 317 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 318 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 319 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 320 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 321 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 322 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 323 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 324 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 325 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 326 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 327 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 328 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 2 to next state (1, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f40bf340>, <__main__.Case object at 0x7f35f40bd4b0>, <__main__.Case object at 0x7f35f40be620>, <__main__.Case object at 0x7f35f40bc220>, <__main__.Case object at 0x7f35f40bf100>, <__main__.Case object at 0x7f35f40bfb80>, <__main__.Case object at 0x7f35f40bf8e0>, <__main__.Case object at 0x7f35f40bf6a0>, <__main__.Case object at 0x7f35f40bcac0>, <__main__.Case object at 0x7f35f40be6b0>, <__main__.Case object at 0x7f35f40bd330>, <__main__.Case object at 0x7f35f40bf880>, <__main__.Case object at 0x7f35f40bf3d0>, <__main__.Case object at 0x7f35f40bd8d0>, <__main__.Case object at 0x7f35f40bc3d0>, <__main__.Case object at 0x7f35f40bf970>, <__main__.Case object at 0x7f35f40bc730>, <__main__.Case object at 0x7f35f40bec80>, <__main__.Case object at 0x7f35f40bca30>, <__main__.Case object at 0x7f35f40bc0d0>, <__main__.Case object at 0x7f35f40bf790>, <__main__.Case object at 0x7f35f40bd180>, <__main__.Case object at 0x7f35f40bcca0>, <__main__.Case object at 0x7f35f40be710>, <__main__.Case object at 0x7f35f40bca00>, <__main__.Case object at 0x7f35f40bcfa0>, <__main__.Case object at 0x7f35f40bc610>, <__main__.Case object at 0x7f35f40bc6d0>, <__main__.Case object at 0x7f35f40bef80>, <__main__.Case object at 0x7f35f40be170>, <__main__.Case object at 0x7f35f40be2f0>, <__main__.Case object at 0x7f35f40bd0f0>, <__main__.Case object at 0x7f35f40be560>, <__main__.Case object at 0x7f35f40bead0>, <__main__.Case object at 0x7f35f40bc910>, <__main__.Case object at 0x7f35f40bc5b0>, <__main__.Case object at 0x7f35f40bd8a0>, <__main__.Case object at 0x7f35f40bca90>, <__main__.Case object at 0x7f35f40bd720>, <__main__.Case object at 0x7f35f40be8c0>, <__main__.Case object at 0x7f35f40be680>, <__main__.Case object at 0x7f35f40bfd90>, <__main__.Case object at 0x7f35f40bd360>, <__main__.Case object at 0x7f35f40bd1e0>, <__main__.Case object at 0x7f35f40bc880>, <__main__.Case object at 0x7f35f40bd810>, <__main__.Case object at 0x7f35f40bfc40>, <__main__.Case object at 0x7f35f40bc460>, <__main__.Case object at 0x7f35f40bf160>, <__main__.Case object at 0x7f35f40be9b0>, <__main__.Case object at 0x7f35f40be110>, <__main__.Case object at 0x7f35f40bee30>, <__main__.Case object at 0x7f35f40bf430>, <__main__.Case object at 0x7f35f40bece0>, <__main__.Case object at 0x7f35f40ef1c0>, <__main__.Case object at 0x7f35f40ed510>, <__main__.Case object at 0x7f35f40ece80>, <__main__.Case object at 0x7f35f40ef940>, <__main__.Case object at 0x7f35f40edf30>, <__main__.Case object at 0x7f35f40eff10>, <__main__.Case object at 0x7f35f40ec6a0>, <__main__.Case object at 0x7f35f40ef4c0>, <__main__.Case object at 0x7f35f40edae0>, <__main__.Case object at 0x7f35f40eed10>, <__main__.Case object at 0x7f35f40ed8d0>, <__main__.Case object at 0x7f35f40efa90>, <__main__.Case object at 0x7f35f40edf00>, <__main__.Case object at 0x7f35f40ef460>, <__main__.Case object at 0x7f35f40ee9e0>, <__main__.Case object at 0x7f35f40ec9d0>, <__main__.Case object at 0x7f35f40ee920>, <__main__.Case object at 0x7f35f40edbd0>, <__main__.Case object at 0x7f35f40ed060>, <__main__.Case object at 0x7f35f40eda20>, <__main__.Case object at 0x7f35f40ec3d0>, <__main__.Case object at 0x7f35f40edb40>, <__main__.Case object at 0x7f35f40ec2e0>, <__main__.Case object at 0x7f35f40ee6e0>, <__main__.Case object at 0x7f35f40efdf0>, <__main__.Case object at 0x7f35f40efe50>, <__main__.Case object at 0x7f35f40ede40>, <__main__.Case object at 0x7f35f40eca90>, <__main__.Case object at 0x7f35f40ee620>, <__main__.Case object at 0x7f35f40efa30>, <__main__.Case object at 0x7f35f40ee5f0>, <__main__.Case object at 0x7f35f40efca0>, <__main__.Case object at 0x7f35f40ee0b0>, <__main__.Case object at 0x7f35f40ee770>, <__main__.Case object at 0x7f35f40ee080>, <__main__.Case object at 0x7f35f40ec1f0>, <__main__.Case object at 0x7f35f40ee260>, <__main__.Case object at 0x7f35f40ef640>, <__main__.Case object at 0x7f35f40ed810>, <__main__.Case object at 0x7f35f40ec190>, <__main__.Case object at 0x7f35f40ee380>, <__main__.Case object at 0x7f35f40ec8e0>, <__main__.Case object at 0x7f35f40ed0f0>, <__main__.Case object at 0x7f35f40ec880>, <__main__.Case object at 0x7f35f40eeb90>, <__main__.Case object at 0x7f35f40ed930>, <__main__.Case object at 0x7f35f40ef010>, <__main__.Case object at 0x7f35f40ee980>, <__main__.Case object at 0x7f35f40ec8b0>, <__main__.Case object at 0x7f35f40ec1c0>, <__main__.Case object at 0x7f35f40ec370>, <__main__.Case object at 0x7f35f40ee8c0>, <__main__.Case object at 0x7f35f40ecbb0>, <__main__.Case object at 0x7f35f40ed960>, <__main__.Case object at 0x7f35f40ec070>, <__main__.Case object at 0x7f35f40ee5c0>, <__main__.Case object at 0x7f35f40eff40>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f409b430>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f409bf70>, <__main__.Case object at 0x7f35f40ed180>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409bbb0>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409b3d0>, <__main__.Case object at 0x7f35f409bca0>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f40adb70>, <__main__.Case object at 0x7f35f40afc40>, <__main__.Case object at 0x7f35f40af520>, <__main__.Case object at 0x7f35f40ae860>, <__main__.Case object at 0x7f35f40ac070>, <__main__.Case object at 0x7f35f40af670>, <__main__.Case object at 0x7f35f40ac940>, <__main__.Case object at 0x7f35f40afee0>, <__main__.Case object at 0x7f35f40aeb00>, <__main__.Case object at 0x7f35f40af340>, <__main__.Case object at 0x7f35f409b6d0>, <__main__.Case object at 0x7f35f40afa90>, <__main__.Case object at 0x7f35f40afe50>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f40afd60>, <__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40aebc0>, <__main__.Case object at 0x7f35f40ae3e0>, <__main__.Case object at 0x7f35f40ac610>, <__main__.Case object at 0x7f35f40ac670>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40aece0>, <__main__.Case object at 0x7f35f40af160>, <__main__.Case object at 0x7f35f40af6a0>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40ac7c0>, <__main__.Case object at 0x7f35f40ad8a0>, <__main__.Case object at 0x7f35f40ae650>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40ad7b0>, <__main__.Case object at 0x7f35f40ace80>, <__main__.Case object at 0x7f35f40af7f0>, <__main__.Case object at 0x7f35f40ae290>, <__main__.Case object at 0x7f35f40af7c0>, <__main__.Case object at 0x7f35f40f68c0>, <__main__.Case object at 0x7f35f40f7580>, <__main__.Case object at 0x7f35f40f4f40>, <__main__.Case object at 0x7f35f40f4d30>, <__main__.Case object at 0x7f35f40f67d0>, <__main__.Case object at 0x7f35f40f6d10>, <__main__.Case object at 0x7f35f40f4430>, <__main__.Case object at 0x7f35f40f4d90>, <__main__.Case object at 0x7f35f40f4c40>, <__main__.Case object at 0x7f35f40f40a0>, <__main__.Case object at 0x7f35f40f6050>, <__main__.Case object at 0x7f35f40f68f0>, <__main__.Case object at 0x7f35f40f5f90>, <__main__.Case object at 0x7f35f40f4940>, <__main__.Case object at 0x7f35f40f5f30>, <__main__.Case object at 0x7f35f40f65c0>, <__main__.Case object at 0x7f35f40f6bc0>, <__main__.Case object at 0x7f35f40f4220>, <__main__.Case object at 0x7f35f40f6170>, <__main__.Case object at 0x7f35f40f6da0>, <__main__.Case object at 0x7f35f40f48b0>, <__main__.Case object at 0x7f35f40f5390>, <__main__.Case object at 0x7f35f40f6530>, <__main__.Case object at 0x7f35f40f53c0>, <__main__.Case object at 0x7f35f40f4100>, <__main__.Case object at 0x7f35f40f4550>, <__main__.Case object at 0x7f35f40f4040>, <__main__.Case object at 0x7f35f40f4e80>, <__main__.Case object at 0x7f35f40f73a0>, <__main__.Case object at 0x7f35f40f70d0>, <__main__.Case object at 0x7f35f40f6d70>, <__main__.Case object at 0x7f35f40f5360>, <__main__.Case object at 0x7f35f40f4a60>, <__main__.Case object at 0x7f35f40f6470>, <__main__.Case object at 0x7f35f40f6710>, <__main__.Case object at 0x7f35f40f69b0>, <__main__.Case object at 0x7f35f40f6dd0>, <__main__.Case object at 0x7f35f40f72e0>, <__main__.Case object at 0x7f35f40f7640>, <__main__.Case object at 0x7f35f40f7f40>, <__main__.Case object at 0x7f35f40f7e20>, <__main__.Case object at 0x7f35f40f7cd0>, <__main__.Case object at 0x7f35f40f7b80>, <__main__.Case object at 0x7f35f40f79a0>, <__main__.Case object at 0x7f35f40f7490>, <__main__.Case object at 0x7f35f40f6560>, <__main__.Case object at 0x7f35f40f7850>, <__main__.Case object at 0x7f35f40f0070>, <__main__.Case object at 0x7f35f40f0190>, <__main__.Case object at 0x7f35f40f02b0>, <__main__.Case object at 0x7f35f40f0370>, <__main__.Case object at 0x7f35f40f0130>, <__main__.Case object at 0x7f35f40f05b0>, <__main__.Case object at 0x7f35f40f06d0>, <__main__.Case object at 0x7f35f40f07f0>, <__main__.Case object at 0x7f35f40f08b0>, <__main__.Case object at 0x7f35f40f0a30>, <__main__.Case object at 0x7f35f40f0b50>, <__main__.Case object at 0x7f35f40f0c70>, <__main__.Case object at 0x7f35f40f0d30>, <__main__.Case object at 0x7f35f40f0eb0>, <__main__.Case object at 0x7f35f40f0fd0>, <__main__.Case object at 0x7f35f40f10f0>, <__main__.Case object at 0x7f35f40f7a60>, <__main__.Case object at 0x7f35f40f12a0>, <__main__.Case object at 0x7f35f40f13f0>, <__main__.Case object at 0x7f35f40f1510>, <__main__.Case object at 0x7f35f40f15d0>, <__main__.Case object at 0x7f35f40f1750>, <__main__.Case object at 0x7f35f40f1870>, <__main__.Case object at 0x7f35f40f1990>, <__main__.Case object at 0x7f35f40f1a50>, <__main__.Case object at 0x7f35f40f1bd0>, <__main__.Case object at 0x7f35f40f1cf0>, <__main__.Case object at 0x7f35f40f1e10>, <__main__.Case object at 0x7f35f40f1ed0>, <__main__.Case object at 0x7f35f40f0430>, <__main__.Case object at 0x7f35f40f0d90>, <__main__.Case object at 0x7f35f40f2170>, <__main__.Case object at 0x7f35f40f2290>, <__main__.Case object at 0x7f35f40f0940>, <__main__.Case object at 0x7f35f40f2470>, <__main__.Case object at 0x7f35f40f25f0>, <__main__.Case object at 0x7f35f40f2710>, <__main__.Case object at 0x7f35f40f24d0>, <__main__.Case object at 0x7f35f40f2830>, <__main__.Case object at 0x7f35f40f29b0>, <__main__.Case object at 0x7f35f40f2a70>, <__main__.Case object at 0x7f35f40f1240>, <__main__.Case object at 0x7f35f40f2c50>, <__main__.Case object at 0x7f35f40f2d70>, <__main__.Case object at 0x7f35f40f2cb0>, <__main__.Case object at 0x7f35f40f2fb0>, <__main__.Case object at 0x7f35f40f30d0>, <__main__.Case object at 0x7f35f40f31f0>, <__main__.Case object at 0x7f35f40f32b0>, <__main__.Case object at 0x7f35f40f3430>, <__main__.Case object at 0x7f35f40f3550>, <__main__.Case object at 0x7f35f40f3670>, <__main__.Case object at 0x7f35f40f3850>, <__main__.Case object at 0x7f35f40f38b0>, <__main__.Case object at 0x7f35f40f39d0>, <__main__.Case object at 0x7f35f40f3af0>, <__main__.Case object at 0x7f35f40f3bb0>, <__main__.Case object at 0x7f35f40f3d30>, <__main__.Case object at 0x7f35f40f3df0>, <__main__.Case object at 0x7f35f40f3eb0>, <__main__.Case object at 0x7f35f40f2740>, <__main__.Case object at 0x7f35f40d80d0>, <__main__.Case object at 0x7f35f40d8250>, <__main__.Case object at 0x7f35f40d8370>, <__main__.Case object at 0x7f35f40d8430>, <__main__.Case object at 0x7f35f40d85b0>, <__main__.Case object at 0x7f35f40d86d0>, <__main__.Case object at 0x7f35f40d87f0>, <__main__.Case object at 0x7f35f40d88b0>, <__main__.Case object at 0x7f35f40d8a30>, <__main__.Case object at 0x7f35f40d8b50>, <__main__.Case object at 0x7f35f40d8c70>, <__main__.Case object at 0x7f35f40d8d30>, <__main__.Case object at 0x7f35f40d8eb0>, <__main__.Case object at 0x7f35f40d8fd0>, <__main__.Case object at 0x7f35f40d90f0>, <__main__.Case object at 0x7f35f40d91b0>, <__main__.Case object at 0x7f35f40d9330>, <__main__.Case object at 0x7f35f40d9450>, <__main__.Case object at 0x7f35f40d9570>, <__main__.Case object at 0x7f35f40d9630>, <__main__.Case object at 0x7f35f40d97b0>, <__main__.Case object at 0x7f35f40d98d0>, <__main__.Case object at 0x7f35f40d99f0>, <__main__.Case object at 0x7f35f40d9ab0>, <__main__.Case object at 0x7f35f40d9c30>, <__main__.Case object at 0x7f35f40d9d50>, <__main__.Case object at 0x7f35f40d9e70>, <__main__.Case object at 0x7f35f40d9f30>, <__main__.Case object at 0x7f35f40da0b0>, <__main__.Case object at 0x7f35f40da1d0>, <__main__.Case object at 0x7f35f40da2f0>, <__main__.Case object at 0x7f35f40da3b0>, <__main__.Case object at 0x7f35f40da530>, <__main__.Case object at 0x7f35f40da650>, <__main__.Case object at 0x7f35f40da770>, <__main__.Case object at 0x7f35f40da950>, <__main__.Case object at 0x7f35f40da9b0>, <__main__.Case object at 0x7f35f40daad0>, <__main__.Case object at 0x7f35f40dabf0>, <__main__.Case object at 0x7f35f40dad10>, <__main__.Case object at 0x7f35f40d8130>, <__main__.Case object at 0x7f35f40d9240>, <__main__.Case object at 0x7f35f40daef0>, <__main__.Case object at 0x7f35f40db010>, <__main__.Case object at 0x7f35f40db190>, <__main__.Case object at 0x7f35f40db2b0>, <__main__.Case object at 0x7f35f40db3d0>, <__main__.Case object at 0x7f35f40db5b0>, <__main__.Case object at 0x7f35f40db610>, <__main__.Case object at 0x7f35f40db730>, <__main__.Case object at 0x7f35f40db850>, <__main__.Case object at 0x7f35f40db910>, <__main__.Case object at 0x7f35f40dba90>, <__main__.Case object at 0x7f35f40dbbb0>, <__main__.Case object at 0x7f35f40dbcd0>, <__main__.Case object at 0x7f35f40dbd90>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f40bef50>, <__main__.Case object at 0x7f35f40be380>, <__main__.Case object at 0x7f35f40be1d0>, <__main__.Case object at 0x7f35f40bdf30>, <__main__.Case object at 0x7f35f40bf610>, <__main__.Case object at 0x7f35f40bfd60>, <__main__.Case object at 0x7f35f40bde10>, <__main__.Case object at 0x7f35f40be440>, <__main__.Case object at 0x7f35f40bcbe0>, <__main__.Case object at 0x7f35f40bed10>, <__main__.Case object at 0x7f35f40bc9d0>, <__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f40bfbe0>, <__main__.Case object at 0x7f35f40bf7f0>, <__main__.Case object at 0x7f35f40bedd0>, <__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f40be320>, <__main__.Case object at 0x7f35f40bf490>, <__main__.Case object at 0x7f35f40bffa0>, <__main__.Case object at 0x7f35f40bd9f0>, <__main__.Case object at 0x7f35f40be200>, <__main__.Case object at 0x7f35f40be920>, <__main__.Case object at 0x7f35f40bfd30>, <__main__.Case object at 0x7f35f40bcd00>, <__main__.Case object at 0x7f35f40bc340>, <__main__.Case object at 0x7f35f40bf730>, <__main__.Case object at 0x7f35f40bfb50>, <__main__.Case object at 0x7f35f40bc940>, <__main__.Case object at 0x7f35f40bf070>, <__main__.Case object at 0x7f35f40bf130>, <__main__.Case object at 0x7f35f40beec0>, <__main__.Case object at 0x7f35f40bd960>, <__main__.Case object at 0x7f35f40bebc0>, <__main__.Case object at 0x7f35f40bf670>, <__main__.Case object at 0x7f35f40bd900>, <__main__.Case object at 0x7f35f40be020>, <__main__.Case object at 0x7f35f40beb30>, <__main__.Case object at 0x7f35f40be2c0>, <__main__.Case object at 0x7f35f40bf040>, <__main__.Case object at 0x7f35f40be860>, <__main__.Case object at 0x7f35f40bfca0>, <__main__.Case object at 0x7f35f40bf190>, <__main__.Case object at 0x7f35f40bc550>, <__main__.Case object at 0x7f35f40bf9a0>, <__main__.Case object at 0x7f35f40bf550>, <__main__.Case object at 0x7f35f40bdae0>, <__main__.Case object at 0x7f35f40bd750>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f40eeec0>, <__main__.Case object at 0x7f35f40ecac0>, <__main__.Case object at 0x7f35f40bc070>, <__main__.Case object at 0x7f35f40bd150>, <__main__.Case object at 0x7f35f40ed030>, <__main__.Case object at 0x7f35f40bcd90>, <__main__.Case object at 0x7f35f40ee020>, <__main__.Case object at 0x7f35f40ef9d0>, <__main__.Case object at 0x7f35f40ed3f0>, <__main__.Case object at 0x7f35f40bd510>, <__main__.Case object at 0x7f35f40edcc0>, <__main__.Case object at 0x7f35f40efe20>, <__main__.Case object at 0x7f35f40edde0>, <__main__.Case object at 0x7f35f40bdbd0>, <__main__.Case object at 0x7f35f40ec040>, <__main__.Case object at 0x7f35f40ecdc0>, <__main__.Case object at 0x7f35f40efa00>, <__main__.Case object at 0x7f35f40ee860>, <__main__.Case object at 0x7f35f40ecfa0>, <__main__.Case object at 0x7f35f40edd20>, <__main__.Case object at 0x7f35f40ec250>, <__main__.Case object at 0x7f35f40ef040>, <__main__.Case object at 0x7f35f40ec100>, <__main__.Case object at 0x7f35f40edcf0>, <__main__.Case object at 0x7f35f40ec400>, <__main__.Case object at 0x7f35f40ede70>, <__main__.Case object at 0x7f35f40ef820>, <__main__.Case object at 0x7f35f40ed660>, <__main__.Case object at 0x7f35f40ec850>, <__main__.Case object at 0x7f35f40efeb0>, <__main__.Case object at 0x7f35f40ef700>, <__main__.Case object at 0x7f35f40ed7b0>, <__main__.Case object at 0x7f35f40ec9a0>, <__main__.Case object at 0x7f35f40ef5b0>, <__main__.Case object at 0x7f35f40ef4f0>, <__main__.Case object at 0x7f35f40eddb0>, <__main__.Case object at 0x7f35f40efe80>, <__main__.Case object at 0x7f35f40ecfd0>, <__main__.Case object at 0x7f35f40ed750>, <__main__.Case object at 0x7f35f40ecb80>, <__main__.Case object at 0x7f35f40edd80>, <__main__.Case object at 0x7f35f40ed0c0>, <__main__.Case object at 0x7f35f40efdc0>, <__main__.Case object at 0x7f35f40ecd90>, <__main__.Case object at 0x7f35f40ed420>, <__main__.Case object at 0x7f35f40eda80>, <__main__.Case object at 0x7f35f40ec220>, <__main__.Case object at 0x7f35f40ee230>, <__main__.Case object at 0x7f35f40ecaf0>, <__main__.Case object at 0x7f35f40edc00>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40eca30>, <__main__.Case object at 0x7f35f40bd060>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40eed40>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f40eefe0>, <__main__.Case object at 0x7f35f409bd90>, <__main__.Case object at 0x7f35f409abc0>, <__main__.Case object at 0x7f35f409b1c0>, <__main__.Case object at 0x7f35f40ee290>, <__main__.Case object at 0x7f35f409b2b0>, <__main__.Case object at 0x7f35f409bc70>, <__main__.Case object at 0x7f35f40ac640>, <__main__.Case object at 0x7f35f40ed6f0>, <__main__.Case object at 0x7f35f40ad510>, <__main__.Case object at 0x7f35f409b790>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f40acd00>, <__main__.Case object at 0x7f35f40ae740>, <__main__.Case object at 0x7f35f40aee60>, <__main__.Case object at 0x7f35f40aea10>, <__main__.Case object at 0x7f35f40ad330>, <__main__.Case object at 0x7f35f40acc70>, <__main__.Case object at 0x7f35f40ac430>, <__main__.Case object at 0x7f35f40ae9b0>, <__main__.Case object at 0x7f35f40aff70>, <__main__.Case object at 0x7f35f409bcd0>, <__main__.Case object at 0x7f35f40ad1b0>, <__main__.Case object at 0x7f35f40ac850>, <__main__.Case object at 0x7f35f40ae500>, <__main__.Case object at 0x7f35f40adb40>, <__main__.Case object at 0x7f35f40ad0f0>, <__main__.Case object at 0x7f35f40af580>, <__main__.Case object at 0x7f35f40acac0>, <__main__.Case object at 0x7f35f40ae950>, <__main__.Case object at 0x7f35f40ac3d0>, <__main__.Case object at 0x7f35f40ad090>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40ad750>, <__main__.Case object at 0x7f35f40aeb90>, <__main__.Case object at 0x7f35f40af850>, <__main__.Case object at 0x7f35f40adff0>, <__main__.Case object at 0x7f35f40ac190>, <__main__.Case object at 0x7f35f40adb10>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40f76d0>, <__main__.Case object at 0x7f35f40aff10>, <__main__.Case object at 0x7f35f40aeb30>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40af010>, <__main__.Case object at 0x7f35f40f47f0>, <__main__.Case object at 0x7f35f40f4070>, <__main__.Case object at 0x7f35f40f48e0>, <__main__.Case object at 0x7f35f40afdc0>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f40f4be0>, <__main__.Case object at 0x7f35f40aed40>, <__main__.Case object at 0x7f35f40f6c80>, <__main__.Case object at 0x7f35f40f4790>, <__main__.Case object at 0x7f35f40ad0c0>, <__main__.Case object at 0x7f35f40f6950>, <__main__.Case object at 0x7f35f40f4250>, <__main__.Case object at 0x7f35f40f61d0>, <__main__.Case object at 0x7f35f40f4eb0>, <__main__.Case object at 0x7f35f40f6fb0>, <__main__.Case object at 0x7f35f40f4ee0>, <__main__.Case object at 0x7f35f40f4820>, <__main__.Case object at 0x7f35f40f5960>, <__main__.Case object at 0x7f35f40f5450>, <__main__.Case object at 0x7f35f40f71f0>, <__main__.Case object at 0x7f35f40f6e00>, <__main__.Case object at 0x7f35f40f5db0>, <__main__.Case object at 0x7f35f40f6140>, <__main__.Case object at 0x7f35f40f6380>, <__main__.Case object at 0x7f35f40f6590>, <__main__.Case object at 0x7f35f40f6ad0>, <__main__.Case object at 0x7f35f40f6e30>, <__main__.Case object at 0x7f35f40f6f50>, <__main__.Case object at 0x7f35f40f7520>, <__main__.Case object at 0x7f35f40f7e80>, <__main__.Case object at 0x7f35f40f6a70>, <__main__.Case object at 0x7f35f40f7d30>, <__main__.Case object at 0x7f35f40f7c10>, <__main__.Case object at 0x7f35f40f7a90>, <__main__.Case object at 0x7f35f40f78b0>, <__main__.Case object at 0x7f35f40f5d20>, <__main__.Case object at 0x7f35f40ec460>, <__main__.Case object at 0x7f35f40f0250>, <__main__.Case object at 0x7f35f40f7790>, <__main__.Case object at 0x7f35f40f5f60>, <__main__.Case object at 0x7f35f40f0670>, <__main__.Case object at 0x7f35f40f0790>, <__main__.Case object at 0x7f35f40f6c50>, <__main__.Case object at 0x7f35f40f0550>, <__main__.Case object at 0x7f35f40f0af0>, <__main__.Case object at 0x7f35f40f0c10>, <__main__.Case object at 0x7f35f40f7ee0>, <__main__.Case object at 0x7f35f40f0910>, <__main__.Case object at 0x7f35f40f0f70>, <__main__.Case object at 0x7f35f40f1090>, <__main__.Case object at 0x7f35f40f0490>, <__main__.Case object at 0x7f35f40f1390>, <__main__.Case object at 0x7f35f40f14b0>, <__main__.Case object at 0x7f35f40f7700>, <__main__.Case object at 0x7f35f40f1210>, <__main__.Case object at 0x7f35f40f1810>, <__main__.Case object at 0x7f35f40f1930>, <__main__.Case object at 0x7f35f40f1ab0>, <__main__.Case object at 0x7f35f40f1630>, <__main__.Case object at 0x7f35f40f1c90>, <__main__.Case object at 0x7f35f40f1db0>, <__main__.Case object at 0x7f35f40f1f30>, <__main__.Case object at 0x7f35f40f2110>, <__main__.Case object at 0x7f35f40f22f0>, <__main__.Case object at 0x7f35f40f21d0>, <__main__.Case object at 0x7f35f40f2590>, <__main__.Case object at 0x7f35f40f0dc0>, <__main__.Case object at 0x7f35f40f2950>, <__main__.Case object at 0x7f35f40f2ad0>, <__main__.Case object at 0x7f35f40f2890>, <__main__.Case object at 0x7f35f40f2e30>, <__main__.Case object at 0x7f35f40f1660>, <__main__.Case object at 0x7f35f40f3070>, <__main__.Case object at 0x7f35f40f3190>, <__main__.Case object at 0x7f35f40f3310>, <__main__.Case object at 0x7f35f40f1ae0>, <__main__.Case object at 0x7f35f40f34f0>, <__main__.Case object at 0x7f35f40f3610>, <__main__.Case object at 0x7f35f40f3790>, <__main__.Case object at 0x7f35f40f1f60>, <__main__.Case object at 0x7f35f40f3970>, <__main__.Case object at 0x7f35f40f3a90>, <__main__.Case object at 0x7f35f40f3c10>, <__main__.Case object at 0x7f35f40f2320>, <__main__.Case object at 0x7f35f40f2e90>, <__main__.Case object at 0x7f35f40f3f10>, <__main__.Case object at 0x7f35f40f7730>, <__main__.Case object at 0x7f35f40d8310>, <__main__.Case object at 0x7f35f40f2ec0>, <__main__.Case object at 0x7f35f40d81f0>, <__main__.Case object at 0x7f35f40d8670>, <__main__.Case object at 0x7f35f40d8790>, <__main__.Case object at 0x7f35f40f3340>, <__main__.Case object at 0x7f35f40d8490>, <__main__.Case object at 0x7f35f40d8af0>, <__main__.Case object at 0x7f35f40d8c10>, <__main__.Case object at 0x7f35f40f37c0>, <__main__.Case object at 0x7f35f40d8910>, <__main__.Case object at 0x7f35f40d8f70>, <__main__.Case object at 0x7f35f40d9090>, <__main__.Case object at 0x7f35f40f3c40>, <__main__.Case object at 0x7f35f40d8d90>, <__main__.Case object at 0x7f35f40d93f0>, <__main__.Case object at 0x7f35f40d9510>, <__main__.Case object at 0x7f35f40f2b00>, <__main__.Case object at 0x7f35f40d9210>, <__main__.Case object at 0x7f35f40d9870>, <__main__.Case object at 0x7f35f40d9990>, <__main__.Case object at 0x7f35f40d9b10>, <__main__.Case object at 0x7f35f40d9690>, <__main__.Case object at 0x7f35f40d9cf0>, <__main__.Case object at 0x7f35f40d9e10>, <__main__.Case object at 0x7f35f40d9f90>, <__main__.Case object at 0x7f35f40d84c0>, <__main__.Case object at 0x7f35f40da170>, <__main__.Case object at 0x7f35f40da290>, <__main__.Case object at 0x7f35f40da410>, <__main__.Case object at 0x7f35f40d8940>, <__main__.Case object at 0x7f35f40da5f0>, <__main__.Case object at 0x7f35f40da710>, <__main__.Case object at 0x7f35f40da890>, <__main__.Case object at 0x7f35f40d8dc0>, <__main__.Case object at 0x7f35f40daa70>, <__main__.Case object at 0x7f35f40dab90>, <__main__.Case object at 0x7f35f40dae90>, <__main__.Case object at 0x7f35f40db070>, <__main__.Case object at 0x7f35f40d96c0>, <__main__.Case object at 0x7f35f40db250>, <__main__.Case object at 0x7f35f40db370>, <__main__.Case object at 0x7f35f40db4f0>, <__main__.Case object at 0x7f35f40d9b40>, <__main__.Case object at 0x7f35f40db6d0>, <__main__.Case object at 0x7f35f40db7f0>, <__main__.Case object at 0x7f35f40db970>, <__main__.Case object at 0x7f35f40d9fc0>, <__main__.Case object at 0x7f35f40dbb50>, <__main__.Case object at 0x7f35f40dbc70>, <__main__.Case object at 0x7f35f40dbdf0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.9, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9, time steps: 22\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40880a0>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f4090b80>, <__main__.Case object at 0x7f35f407c6a0>, <__main__.Case object at 0x7f35f4073160>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f40bf760>, <__main__.Case object at 0x7f35f40bc2e0>, <__main__.Case object at 0x7f35f40be1a0>, <__main__.Case object at 0x7f35f40be4d0>, <__main__.Case object at 0x7f35f40bdb70>, <__main__.Case object at 0x7f35f40bf820>, <__main__.Case object at 0x7f35f40bd4e0>, <__main__.Case object at 0x7f35f40bcdf0>, <__main__.Case object at 0x7f35f40bfa60>, <__main__.Case object at 0x7f35f40bcdc0>, <__main__.Case object at 0x7f35f40bded0>, <__main__.Case object at 0x7f35f40beaa0>, <__main__.Case object at 0x7f35f40bf8b0>, <__main__.Case object at 0x7f35f40bfee0>, <__main__.Case object at 0x7f35f40be260>, <__main__.Case object at 0x7f35f40be590>, <__main__.Case object at 0x7f35f40bdba0>, <__main__.Case object at 0x7f35f40bdf00>, <__main__.Case object at 0x7f35f40bdd80>, <__main__.Case object at 0x7f35f40be230>, <__main__.Case object at 0x7f35f40bcee0>, <__main__.Case object at 0x7f35f40beef0>, <__main__.Case object at 0x7f35f40bdc00>, <__main__.Case object at 0x7f35f40bca60>, <__main__.Case object at 0x7f35f40bd5d0>, <__main__.Case object at 0x7f35f40bf910>, <__main__.Case object at 0x7f35f40bdfc0>, <__main__.Case object at 0x7f35f40bfaf0>, <__main__.Case object at 0x7f35f40be530>, <__main__.Case object at 0x7f35f40bff40>, <__main__.Case object at 0x7f35f40bf4f0>, <__main__.Case object at 0x7f35f40bf250>, <__main__.Case object at 0x7f35f40bc9a0>, <__main__.Case object at 0x7f35f40bc7f0>, <__main__.Case object at 0x7f35f40bd480>, <__main__.Case object at 0x7f35f40bfc70>, <__main__.Case object at 0x7f35f40be4a0>, <__main__.Case object at 0x7f35f40bed40>, <__main__.Case object at 0x7f35f40bd6c0>, <__main__.Case object at 0x7f35f40bdb10>, <__main__.Case object at 0x7f35f40bed70>, <__main__.Case object at 0x7f35f40bc040>, <__main__.Case object at 0x7f35f40becb0>, <__main__.Case object at 0x7f35f40bf400>, <__main__.Case object at 0x7f35f40bfc10>, <__main__.Case object at 0x7f35f40be770>, <__main__.Case object at 0x7f35f40beb60>, <__main__.Case object at 0x7f35f40bf220>, <__main__.Case object at 0x7f35f40bc250>, <__main__.Case object at 0x7f35f40bd7e0>, <__main__.Case object at 0x7f35f40bcb20>, <__main__.Case object at 0x7f35f40bf6d0>, <__main__.Case object at 0x7f35f40bc820>, <__main__.Case object at 0x7f35f40bda50>, <__main__.Case object at 0x7f35f40ec6d0>, <__main__.Case object at 0x7f35f40bf5b0>, <__main__.Case object at 0x7f35f40edc90>, <__main__.Case object at 0x7f35f40ecc10>, <__main__.Case object at 0x7f35f40ee950>, <__main__.Case object at 0x7f35f40ef1f0>, <__main__.Case object at 0x7f35f40ee7d0>, <__main__.Case object at 0x7f35f40ec820>, <__main__.Case object at 0x7f35f40ee9b0>, <__main__.Case object at 0x7f35f40eed70>, <__main__.Case object at 0x7f35f40ee140>, <__main__.Case object at 0x7f35f40ed210>, <__main__.Case object at 0x7f35f40ef6a0>, <__main__.Case object at 0x7f35f40ed4e0>, <__main__.Case object at 0x7f35f40eef50>, <__main__.Case object at 0x7f35f40ed360>, <__main__.Case object at 0x7f35f40ed630>, <__main__.Case object at 0x7f35f40ee590>, <__main__.Case object at 0x7f35f40efb80>, <__main__.Case object at 0x7f35f40ee740>, <__main__.Case object at 0x7f35f40ed330>, <__main__.Case object at 0x7f35f40ef7f0>, <__main__.Case object at 0x7f35f40ed390>, <__main__.Case object at 0x7f35f40ee560>, <__main__.Case object at 0x7f35f40ecc70>, <__main__.Case object at 0x7f35f40ee200>, <__main__.Case object at 0x7f35f40edb70>, <__main__.Case object at 0x7f35f40ed450>, <__main__.Case object at 0x7f35f40eef20>, <__main__.Case object at 0x7f35f40ed480>, <__main__.Case object at 0x7f35f40eefb0>, <__main__.Case object at 0x7f35f40ecb50>, <__main__.Case object at 0x7f35f40eecb0>, <__main__.Case object at 0x7f35f40ecb20>, <__main__.Case object at 0x7f35f40eeda0>, <__main__.Case object at 0x7f35f40ece50>, <__main__.Case object at 0x7f35f40ed1e0>, <__main__.Case object at 0x7f35f40edf60>, <__main__.Case object at 0x7f35f40eebc0>, <__main__.Case object at 0x7f35f40ed240>, <__main__.Case object at 0x7f35f40eebf0>, <__main__.Case object at 0x7f35f40ed720>, <__main__.Case object at 0x7f35f40ef6d0>, <__main__.Case object at 0x7f35f40ec490>, <__main__.Case object at 0x7f35f40ecdf0>, <__main__.Case object at 0x7f35f40efaf0>, <__main__.Case object at 0x7f35f40ec2b0>, <__main__.Case object at 0x7f35f40ef580>, <__main__.Case object at 0x7f35f40ef340>, <__main__.Case object at 0x7f35f40ed780>, <__main__.Case object at 0x7f35f40ec910>, <__main__.Case object at 0x7f35f40eceb0>, <__main__.Case object at 0x7f35f40ef250>, <__main__.Case object at 0x7f35f40ee8f0>, <__main__.Case object at 0x7f35f40ed2a0>, <__main__.Case object at 0x7f35f40eef80>, <__main__.Case object at 0x7f35f40eee60>, <__main__.Case object at 0x7f35f409ba30>, <__main__.Case object at 0x7f35f409b130>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409b850>, <__main__.Case object at 0x7f35f409b610>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f409b640>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f40ee830>, <__main__.Case object at 0x7f35f40afa60>, <__main__.Case object at 0x7f35f40af2b0>, <__main__.Case object at 0x7f35f40ada50>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40aea70>, <__main__.Case object at 0x7f35f40aea40>, <__main__.Case object at 0x7f35f40af0a0>, <__main__.Case object at 0x7f35f40add20>, <__main__.Case object at 0x7f35f40afc70>, <__main__.Case object at 0x7f35f40afbb0>, <__main__.Case object at 0x7f35f40ad870>, <__main__.Case object at 0x7f35f40afeb0>, <__main__.Case object at 0x7f35f40ad240>, <__main__.Case object at 0x7f35f40ed150>, <__main__.Case object at 0x7f35f40ac1f0>, <__main__.Case object at 0x7f35f40ad300>, <__main__.Case object at 0x7f35f40ae0e0>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40af970>, <__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40ae890>, <__main__.Case object at 0x7f35f40ac280>, <__main__.Case object at 0x7f35f40ae320>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40ad930>, <__main__.Case object at 0x7f35f40affa0>, <__main__.Case object at 0x7f35f40aff40>, <__main__.Case object at 0x7f35f40aee30>, <__main__.Case object at 0x7f35f40af460>, <__main__.Case object at 0x7f35f40ac730>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40aee00>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f40afd30>, <__main__.Case object at 0x7f35f40ac2b0>, <__main__.Case object at 0x7f35f40af760>, <__main__.Case object at 0x7f35f40f5180>, <__main__.Case object at 0x7f35f40f6b00>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40ad5d0>, <__main__.Case object at 0x7f35f40f60e0>, <__main__.Case object at 0x7f35f40f52d0>, <__main__.Case object at 0x7f35f40f4f70>, <__main__.Case object at 0x7f35f40f64a0>, <__main__.Case object at 0x7f35f40f67a0>, <__main__.Case object at 0x7f35f40f4130>, <__main__.Case object at 0x7f35f40f6440>, <__main__.Case object at 0x7f35f40f46a0>, <__main__.Case object at 0x7f35f40f7190>, <__main__.Case object at 0x7f35f40f4490>, <__main__.Case object at 0x7f35f40f6260>, <__main__.Case object at 0x7f35f40f6cb0>, <__main__.Case object at 0x7f35f40f44c0>, <__main__.Case object at 0x7f35f40f4f10>, <__main__.Case object at 0x7f35f40f5d80>, <__main__.Case object at 0x7f35f40f42b0>, <__main__.Case object at 0x7f35f40f4520>, <__main__.Case object at 0x7f35f40f5e10>, <__main__.Case object at 0x7f35f40f7f70>, <__main__.Case object at 0x7f35f40f6980>, <__main__.Case object at 0x7f35f40f45b0>, <__main__.Case object at 0x7f35f40f4700>, <__main__.Case object at 0x7f35f40f43d0>, <__main__.Case object at 0x7f35f40f75e0>, <__main__.Case object at 0x7f35f40f7280>, <__main__.Case object at 0x7f35f40f4370>, <__main__.Case object at 0x7f35f40f7070>, <__main__.Case object at 0x7f35f40f6080>, <__main__.Case object at 0x7f35f40f6230>, <__main__.Case object at 0x7f35f40f4580>, <__main__.Case object at 0x7f35f40f6890>, <__main__.Case object at 0x7f35f40f6d40>, <__main__.Case object at 0x7f35f40f6ef0>, <__main__.Case object at 0x7f35f40f6c20>, <__main__.Case object at 0x7f35f40f7400>, <__main__.Case object at 0x7f35f40f7eb0>, <__main__.Case object at 0x7f35f40f7d90>, <__main__.Case object at 0x7f35f40f7c70>, <__main__.Case object at 0x7f35f40f7b20>, <__main__.Case object at 0x7f35f40f6320>, <__main__.Case object at 0x7f35f40f7a00>, <__main__.Case object at 0x7f35f40f78e0>, <__main__.Case object at 0x7f35f40f5ea0>, <__main__.Case object at 0x7f35f40f00d0>, <__main__.Case object at 0x7f35f40f01f0>, <__main__.Case object at 0x7f35f40f7970>, <__main__.Case object at 0x7f35f40f0310>, <__main__.Case object at 0x7f35f40f04f0>, <__main__.Case object at 0x7f35f40f0610>, <__main__.Case object at 0x7f35f40f0730>, <__main__.Case object at 0x7f35f40f0850>, <__main__.Case object at 0x7f35f40f03d0>, <__main__.Case object at 0x7f35f40f09a0>, <__main__.Case object at 0x7f35f40f0bb0>, <__main__.Case object at 0x7f35f40f0cd0>, <__main__.Case object at 0x7f35f40f0e20>, <__main__.Case object at 0x7f35f40f0f10>, <__main__.Case object at 0x7f35f40f0a90>, <__main__.Case object at 0x7f35f40f1150>, <__main__.Case object at 0x7f35f40f1030>, <__main__.Case object at 0x7f35f40f11b0>, <__main__.Case object at 0x7f35f40f1450>, <__main__.Case object at 0x7f35f40f1330>, <__main__.Case object at 0x7f35f40f16c0>, <__main__.Case object at 0x7f35f40f17b0>, <__main__.Case object at 0x7f35f40f18d0>, <__main__.Case object at 0x7f35f40f19f0>, <__main__.Case object at 0x7f35f40f1b40>, <__main__.Case object at 0x7f35f40f1c30>, <__main__.Case object at 0x7f35f40f1d50>, <__main__.Case object at 0x7f35f40f1e70>, <__main__.Case object at 0x7f35f40f1fc0>, <__main__.Case object at 0x7f35f40f2050>, <__main__.Case object at 0x7f35f40f20b0>, <__main__.Case object at 0x7f35f40f2230>, <__main__.Case object at 0x7f35f40f1570>, <__main__.Case object at 0x7f35f40ed4b0>, <__main__.Case object at 0x7f35f40f2410>, <__main__.Case object at 0x7f35f40f2650>, <__main__.Case object at 0x7f35f40f26b0>, <__main__.Case object at 0x7f35f40f27a0>, <__main__.Case object at 0x7f35f40f28f0>, <__main__.Case object at 0x7f35f40f2380>, <__main__.Case object at 0x7f35f40f2b60>, <__main__.Case object at 0x7f35f40f2bf0>, <__main__.Case object at 0x7f35f40f2d10>, <__main__.Case object at 0x7f35f40f2a10>, <__main__.Case object at 0x7f35f40f2f20>, <__main__.Case object at 0x7f35f40f2530>, <__main__.Case object at 0x7f35f40bc310>, <__main__.Case object at 0x7f35f40f3250>, <__main__.Case object at 0x7f35f40f33a0>, <__main__.Case object at 0x7f35f40f3490>, <__main__.Case object at 0x7f35f40f35b0>, <__main__.Case object at 0x7f35f40bd450>, <__main__.Case object at 0x7f35f40f3730>, <__main__.Case object at 0x7f35f40f3910>, <__main__.Case object at 0x7f35f40f3a30>, <__main__.Case object at 0x7f35f40f36d0>, <__main__.Case object at 0x7f35f40f3ca0>, <__main__.Case object at 0x7f35f40f3d90>, <__main__.Case object at 0x7f35f40f3130>, <__main__.Case object at 0x7f35f40f3f70>, <__main__.Case object at 0x7f35f40f3010>, <__main__.Case object at 0x7f35f40d8190>, <__main__.Case object at 0x7f35f40f2dd0>, <__main__.Case object at 0x7f35f40d83d0>, <__main__.Case object at 0x7f35f40d8520>, <__main__.Case object at 0x7f35f40d8610>, <__main__.Case object at 0x7f35f40d8730>, <__main__.Case object at 0x7f35f40d8850>, <__main__.Case object at 0x7f35f40d89a0>, <__main__.Case object at 0x7f35f40d8a90>, <__main__.Case object at 0x7f35f40d8bb0>, <__main__.Case object at 0x7f35f40d8cd0>, <__main__.Case object at 0x7f35f40d8e50>, <__main__.Case object at 0x7f35f40d8f10>, <__main__.Case object at 0x7f35f40d9030>, <__main__.Case object at 0x7f35f40d9150>, <__main__.Case object at 0x7f35f40d92a0>, <__main__.Case object at 0x7f35f40d9390>, <__main__.Case object at 0x7f35f40d94b0>, <__main__.Case object at 0x7f35f40d82b0>, <__main__.Case object at 0x7f35f40d9720>, <__main__.Case object at 0x7f35f40d9810>, <__main__.Case object at 0x7f35f40d9930>, <__main__.Case object at 0x7f35f40d9a50>, <__main__.Case object at 0x7f35f40d95d0>, <__main__.Case object at 0x7f35f40d9c90>, <__main__.Case object at 0x7f35f40d9db0>, <__main__.Case object at 0x7f35f40d9ed0>, <__main__.Case object at 0x7f35f40da020>, <__main__.Case object at 0x7f35f40da110>, <__main__.Case object at 0x7f35f40da230>, <__main__.Case object at 0x7f35f40da350>, <__main__.Case object at 0x7f35f40d9ba0>, <__main__.Case object at 0x7f35f40d8070>, <__main__.Case object at 0x7f35f40da6b0>, <__main__.Case object at 0x7f35f40da7d0>, <__main__.Case object at 0x7f35f40da830>, <__main__.Case object at 0x7f35f40daa10>, <__main__.Case object at 0x7f35f40dab30>, <__main__.Case object at 0x7f35f40dac50>, <__main__.Case object at 0x7f35f40dacb0>, <__main__.Case object at 0x7f35f40dada0>, <__main__.Case object at 0x7f35f40da590>, <__main__.Case object at 0x7f35f40dafb0>, <__main__.Case object at 0x7f35f40db100>, <__main__.Case object at 0x7f35f40db1f0>, <__main__.Case object at 0x7f35f40db310>, <__main__.Case object at 0x7f35f40da4a0>, <__main__.Case object at 0x7f35f40db490>, <__main__.Case object at 0x7f35f40db670>, <__main__.Case object at 0x7f35f40db790>, <__main__.Case object at 0x7f35f40db8b0>, <__main__.Case object at 0x7f35f40dba00>, <__main__.Case object at 0x7f35f40bd930>, <__main__.Case object at 0x7f35f40dbc10>, <__main__.Case object at 0x7f35f40f6b90>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 0\n",
      "Episode: 15, Total Steps: 329, Total Rewards: [-428, 88], Status Episode: False\n",
      "------------------------------------------End of episode 15 loop--------------------\n",
      "----- starting point of Episode 16 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.7, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 4) with action 1 to next state (5, 3): pull reward: 0.1\n",
      "----- starting point of Episode 16 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.05135591857282217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04548153726833638\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.03800123505302955\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 16 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 0 to next state (5, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 4 to next state (6, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 0) with action 4 to next state (7, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 1) with action 3 to next state (6, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 1) with action 0 to next state (6, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 1) with action 4 to next state (7, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f40880a0>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f4073160>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f409afe0>, <__main__.Case object at 0x7f35f409b520>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f409bf70>, <__main__.Case object at 0x7f35f4099a20>, <__main__.Case object at 0x7f35f409ad40>, <__main__.Case object at 0x7f35f409ac50>, <__main__.Case object at 0x7f35f409b3a0>, <__main__.Case object at 0x7f35f409b850>, <__main__.Case object at 0x7f35f409ba00>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f40f76d0>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40f6fe0>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f40f5e70>, <__main__.Case object at 0x7f35f40f4250>, <__main__.Case object at 0x7f35f40f4eb0>, <__main__.Case object at 0x7f35f40f4820>, <__main__.Case object at 0x7f35f40f6e00>, <__main__.Case object at 0x7f35f40f6380>, <__main__.Case object at 0x7f35f40f6e30>, <__main__.Case object at 0x7f35f40f59c0>, <__main__.Case object at 0x7f35f40f7bb0>, <__main__.Case object at 0x7f35f40f7790>, <__main__.Case object at 0x7f35f40f68c0>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f40f67d0>, <__main__.Case object at 0x7f35f40f4c40>, <__main__.Case object at 0x7f35f40f68f0>, <__main__.Case object at 0x7f35f40f4940>, <__main__.Case object at 0x7f35f40f4220>, <__main__.Case object at 0x7f35f40f48b0>, <__main__.Case object at 0x7f35f40f53c0>, <__main__.Case object at 0x7f35f40f4550>, <__main__.Case object at 0x7f35f40f70d0>, <__main__.Case object at 0x7f35f40f4a60>, <__main__.Case object at 0x7f35f40f69b0>, <__main__.Case object at 0x7f35f40f72e0>, <__main__.Case object at 0x7f35f40f7cd0>, <__main__.Case object at 0x7f35f40f7490>, <__main__.Case object at 0x7f35f40f7a60>, <__main__.Case object at 0x7f35f40f6b00>, <__main__.Case object at 0x7f35f40f5de0>, <__main__.Case object at 0x7f35f40f5ed0>, <__main__.Case object at 0x7f35f40f4280>, <__main__.Case object at 0x7f35f40f6650>, <__main__.Case object at 0x7f35f40f7100>, <__main__.Case object at 0x7f35f40f66e0>, <__main__.Case object at 0x7f35f40f4ac0>, <__main__.Case object at 0x7f35f40f4190>, <__main__.Case object at 0x7f35f40f5e40>, <__main__.Case object at 0x7f35f40f6830>, <__main__.Case object at 0x7f35f40f71c0>, <__main__.Case object at 0x7f35f40f7f10>, <__main__.Case object at 0x7f35f40f7ac0>, <__main__.Case object at 0x7f35f40f7820>, <__main__.Case object at 0x7f35f40ee1a0>, <__main__.Case object at 0x7f35f40ee4d0>, <__main__.Case object at 0x7f35f40ef9d0>, <__main__.Case object at 0x7f35f40ef280>, <__main__.Case object at 0x7f35f40ec040>, <__main__.Case object at 0x7f35f40efa00>, <__main__.Case object at 0x7f35f40ec250>, <__main__.Case object at 0x7f35f40edcf0>, <__main__.Case object at 0x7f35f40ede70>, <__main__.Case object at 0x7f35f40f7fd0>, <__main__.Case object at 0x7f35f40efeb0>, <__main__.Case object at 0x7f35f40ef5b0>, <__main__.Case object at 0x7f35f40eddb0>, <__main__.Case object at 0x7f35f40ef700>, <__main__.Case object at 0x7f35f40ed0c0>, <__main__.Case object at 0x7f35f40ed420>, <__main__.Case object at 0x7f35f40ee230>, <__main__.Case object at 0x7f35f40edc00>, <__main__.Case object at 0x7f35f40ed6f0>, <__main__.Case object at 0x7f35f40ece80>, <__main__.Case object at 0x7f35f40edf30>, <__main__.Case object at 0x7f35f40ed750>, <__main__.Case object at 0x7f35f40ed8d0>, <__main__.Case object at 0x7f35f40ef460>, <__main__.Case object at 0x7f35f40ee920>, <__main__.Case object at 0x7f35f40ed060>, <__main__.Case object at 0x7f35f40ec2e0>, <__main__.Case object at 0x7f35f40efe50>, <__main__.Case object at 0x7f35f40ef520>, <__main__.Case object at 0x7f35f40efca0>, <__main__.Case object at 0x7f35f40ec1f0>, <__main__.Case object at 0x7f35f40ed810>, <__main__.Case object at 0x7f35f40ec8e0>, <__main__.Case object at 0x7f35f40ec880>, <__main__.Case object at 0x7f35f40ee980>, <__main__.Case object at 0x7f35f40ec370>, <__main__.Case object at 0x7f35f40efac0>, <__main__.Case object at 0x7f35f40ed9f0>, <__main__.Case object at 0x7f35f40edc90>, <__main__.Case object at 0x7f35f40ef1f0>, <__main__.Case object at 0x7f35f40ee9b0>, <__main__.Case object at 0x7f35f40eff70>, <__main__.Case object at 0x7f35f40ee170>, <__main__.Case object at 0x7f35f40ed300>, <__main__.Case object at 0x7f35f40ec0a0>, <__main__.Case object at 0x7f35f40ec4c0>, <__main__.Case object at 0x7f35f40ec580>, <__main__.Case object at 0x7f35f40ed480>, <__main__.Case object at 0x7f35f40ecb20>, <__main__.Case object at 0x7f35f40ed1e0>, <__main__.Case object at 0x7f35f40ed120>, <__main__.Case object at 0x7f35f40efaf0>, <__main__.Case object at 0x7f35f40ef340>, <__main__.Case object at 0x7f35f40ec910>, <__main__.Case object at 0x7f35f40ed2a0>, <__main__.Case object at 0x7f35f40ed4b0>, <__main__.Case object at 0x7f35f40bef50>, <__main__.Case object at 0x7f35f40be1d0>, <__main__.Case object at 0x7f35f40bde10>, <__main__.Case object at 0x7f35f40bed10>, <__main__.Case object at 0x7f35f40bec50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f40679d0>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409b2e0>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f40651b0>, <__main__.Case object at 0x7f35f409b8e0>, <__main__.Case object at 0x7f35f409b7f0>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f40f62f0>, <__main__.Case object at 0x7f35f40f4310>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40f4be0>, <__main__.Case object at 0x7f35f40f6140>, <__main__.Case object at 0x7f35f40f6ad0>, <__main__.Case object at 0x7f35f409af50>, <__main__.Case object at 0x7f35f40f5960>, <__main__.Case object at 0x7f35f40f77f0>, <__main__.Case object at 0x7f35f40f7700>, <__main__.Case object at 0x7f35f40f6fb0>, <__main__.Case object at 0x7f35f40f4d90>, <__main__.Case object at 0x7f35f40f6050>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f40f4d30>, <__main__.Case object at 0x7f35f40f6da0>, <__main__.Case object at 0x7f35f40f6530>, <__main__.Case object at 0x7f35f40f4040>, <__main__.Case object at 0x7f35f40f5f30>, <__main__.Case object at 0x7f35f40f5360>, <__main__.Case object at 0x7f35f40f6710>, <__main__.Case object at 0x7f35f40f7640>, <__main__.Case object at 0x7f35f40f60b0>, <__main__.Case object at 0x7f35f40f79a0>, <__main__.Case object at 0x7f35f40f7850>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40f7670>, <__main__.Case object at 0x7f35f40f4730>, <__main__.Case object at 0x7f35f40f7430>, <__main__.Case object at 0x7f35f40f7760>, <__main__.Case object at 0x7f35f40f6a70>, <__main__.Case object at 0x7f35f40f5cc0>, <__main__.Case object at 0x7f35f40f4850>, <__main__.Case object at 0x7f35f40f74f0>, <__main__.Case object at 0x7f35f40f6350>, <__main__.Case object at 0x7f35f40f6500>, <__main__.Case object at 0x7f35f40f6e60>, <__main__.Case object at 0x7f35f40f7e50>, <__main__.Case object at 0x7f35f40f6410>, <__main__.Case object at 0x7f35f40f7910>, <__main__.Case object at 0x7f35f40f4640>, <__main__.Case object at 0x7f35f40f7fa0>, <__main__.Case object at 0x7f35f40ec130>, <__main__.Case object at 0x7f35f40ee650>, <__main__.Case object at 0x7f35f40eeb30>, <__main__.Case object at 0x7f35f40f5d50>, <__main__.Case object at 0x7f35f40ec340>, <__main__.Case object at 0x7f35f40ec100>, <__main__.Case object at 0x7f35f409bca0>, <__main__.Case object at 0x7f35f40ec9a0>, <__main__.Case object at 0x7f35f40f75e0>, <__main__.Case object at 0x7f35f40ec850>, <__main__.Case object at 0x7f35f40ecd90>, <__main__.Case object at 0x7f35f40ec220>, <__main__.Case object at 0x7f35f40f7eb0>, <__main__.Case object at 0x7f35f40ecfd0>, <__main__.Case object at 0x7f35f40ed510>, <__main__.Case object at 0x7f35f40ec6a0>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40edf00>, <__main__.Case object at 0x7f35f40ec9d0>, <__main__.Case object at 0x7f35f40eda20>, <__main__.Case object at 0x7f35f40ed030>, <__main__.Case object at 0x7f35f40efdf0>, <__main__.Case object at 0x7f35f40ed5a0>, <__main__.Case object at 0x7f35f40ee0b0>, <__main__.Case object at 0x7f35f40ef220>, <__main__.Case object at 0x7f35f40ef640>, <__main__.Case object at 0x7f35f40ee380>, <__main__.Case object at 0x7f35f40eeb90>, <__main__.Case object at 0x7f35f40ed660>, <__main__.Case object at 0x7f35f40ec1c0>, <__main__.Case object at 0x7f35f40eeef0>, <__main__.Case object at 0x7f35f40ee410>, <__main__.Case object at 0x7f35f40ed2d0>, <__main__.Case object at 0x7f35f40ee950>, <__main__.Case object at 0x7f35f40ec820>, <__main__.Case object at 0x7f35f40efc10>, <__main__.Case object at 0x7f35f40ed570>, <__main__.Case object at 0x7f35f40eea40>, <__main__.Case object at 0x7f35f40ee500>, <__main__.Case object at 0x7f35f40ef4c0>, <__main__.Case object at 0x7f35f40eef20>, <__main__.Case object at 0x7f35f40eecb0>, <__main__.Case object at 0x7f35f40edf60>, <__main__.Case object at 0x7f35f40ef370>, <__main__.Case object at 0x7f35f40ecdf0>, <__main__.Case object at 0x7f35f40ef580>, <__main__.Case object at 0x7f35f40eceb0>, <__main__.Case object at 0x7f35f40eebf0>, <__main__.Case object at 0x7f35f40ee830>, <__main__.Case object at 0x7f35f40ed7e0>, <__main__.Case object at 0x7f35f40ed180>, <__main__.Case object at 0x7f35f40bf0a0>, <__main__.Case object at 0x7f35f40bcbe0>, <__main__.Case object at 0x7f35f40bccd0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.4, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.4, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 1, tv: 0.4, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.8, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.8, time steps: 22\n",
      "Episode succeeded, case (4, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 4, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 0.5)\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 1, 0.5)\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 1, 0.5)\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 3, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.7)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.7)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.8, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.8, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 1), solution: 4, tv: 0.5, time steps: 115\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 111\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 110\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 108\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 103\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 98\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 91\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.5, time steps: 90\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5, time steps: 88\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 85\n",
      "cases content after RETAIN, problem: (0, 2), solution: 1, tv: 0.5, time steps: 84\n",
      "cases content after RETAIN, problem: (1, 2), solution: 3, tv: 0.5, time steps: 72\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 40\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f4070b80>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f409bc70>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f409b2b0>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409bdf0>, <__main__.Case object at 0x7f35f409b790>, <__main__.Case object at 0x7f35f409b610>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f409ae90>, <__main__.Case object at 0x7f35f40f4a90>, <__main__.Case object at 0x7f35f409b640>, <__main__.Case object at 0x7f35f40f6290>, <__main__.Case object at 0x7f35f40f44f0>, <__main__.Case object at 0x7f35f40f6950>, <__main__.Case object at 0x7f35f40f6200>, <__main__.Case object at 0x7f35f40f61d0>, <__main__.Case object at 0x7f35f40f73d0>, <__main__.Case object at 0x7f35f409ad10>, <__main__.Case object at 0x7f35f40f7520>, <__main__.Case object at 0x7f35f40f6f50>, <__main__.Case object at 0x7f35f40f7d30>, <__main__.Case object at 0x7f35f40f7af0>, <__main__.Case object at 0x7f35f40f6c50>, <__main__.Case object at 0x7f35f40f4ee0>, <__main__.Case object at 0x7f35f40f4f40>, <__main__.Case object at 0x7f35f40f7580>, <__main__.Case object at 0x7f35f40f40a0>, <__main__.Case object at 0x7f35f40f5f90>, <__main__.Case object at 0x7f35f40f6110>, <__main__.Case object at 0x7f35f40f6170>, <__main__.Case object at 0x7f35f40f5390>, <__main__.Case object at 0x7f35f40f6590>, <__main__.Case object at 0x7f35f40f7550>, <__main__.Case object at 0x7f35f40f6d70>, <__main__.Case object at 0x7f35f40f6470>, <__main__.Case object at 0x7f35f40f5db0>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f40f7b80>, <__main__.Case object at 0x7f35f40f6560>, <__main__.Case object at 0x7f35f40f5180>, <__main__.Case object at 0x7f35f40f60e0>, <__main__.Case object at 0x7f35f40f45e0>, <__main__.Case object at 0x7f35f40f7df0>, <__main__.Case object at 0x7f35f40f5cf0>, <__main__.Case object at 0x7f35f40f4430>, <__main__.Case object at 0x7f35f40f40d0>, <__main__.Case object at 0x7f35f40f44c0>, <__main__.Case object at 0x7f35f40f4d60>, <__main__.Case object at 0x7f35f40f7280>, <__main__.Case object at 0x7f35f40f61a0>, <__main__.Case object at 0x7f35f40f6b60>, <__main__.Case object at 0x7f35f40f4c70>, <__main__.Case object at 0x7f35f40f74c0>, <__main__.Case object at 0x7f35f40f79d0>, <__main__.Case object at 0x7f35f40f6dd0>, <__main__.Case object at 0x7f35f40f5ea0>, <__main__.Case object at 0x7f35f40ec310>, <__main__.Case object at 0x7f35f40f7d90>, <__main__.Case object at 0x7f35f40ed090>, <__main__.Case object at 0x7f35f40ed3f0>, <__main__.Case object at 0x7f35f40f6a10>, <__main__.Case object at 0x7f35f40ef040>, <__main__.Case object at 0x7f35f40ec400>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f40ee860>, <__main__.Case object at 0x7f35f40ed7b0>, <__main__.Case object at 0x7f35f40ef4f0>, <__main__.Case object at 0x7f35f40efe80>, <__main__.Case object at 0x7f35f40ee890>, <__main__.Case object at 0x7f35f40efdc0>, <__main__.Case object at 0x7f35f40eda80>, <__main__.Case object at 0x7f35f40ecaf0>, <__main__.Case object at 0x7f35f40eed40>, <__main__.Case object at 0x7f35f40ef1c0>, <__main__.Case object at 0x7f35f40ef940>, <__main__.Case object at 0x7f35f40ecdc0>, <__main__.Case object at 0x7f35f40ecf40>, <__main__.Case object at 0x7f35f40efa90>, <__main__.Case object at 0x7f35f40efd90>, <__main__.Case object at 0x7f35f40ee9e0>, <__main__.Case object at 0x7f35f40ed990>, <__main__.Case object at 0x7f35f40eca90>, <__main__.Case object at 0x7f35f40ef430>, <__main__.Case object at 0x7f35f40ed600>, <__main__.Case object at 0x7f35f40ed690>, <__main__.Case object at 0x7f35f40ee260>, <__main__.Case object at 0x7f35f40ec190>, <__main__.Case object at 0x7f35f40ed0f0>, <__main__.Case object at 0x7f35f40ef5e0>, <__main__.Case object at 0x7f35f40ecbb0>, <__main__.Case object at 0x7f35f40ee8c0>, <__main__.Case object at 0x7f35f40ec760>, <__main__.Case object at 0x7f35f40ec6d0>, <__main__.Case object at 0x7f35f40ecc10>, <__main__.Case object at 0x7f35f40ee140>, <__main__.Case object at 0x7f35f40eed70>, <__main__.Case object at 0x7f35f40ed840>, <__main__.Case object at 0x7f35f40ec0d0>, <__main__.Case object at 0x7f35f40ef880>, <__main__.Case object at 0x7f35f40ee200>, <__main__.Case object at 0x7f35f40ef910>, <__main__.Case object at 0x7f35f40ecb50>, <__main__.Case object at 0x7f35f40ef550>, <__main__.Case object at 0x7f35f40ee6b0>, <__main__.Case object at 0x7f35f40ee440>, <__main__.Case object at 0x7f35f40ec490>, <__main__.Case object at 0x7f35f40ec5e0>, <__main__.Case object at 0x7f35f40ef3a0>, <__main__.Case object at 0x7f35f40eef80>, <__main__.Case object at 0x7f35f40bdd20>, <__main__.Case object at 0x7f35f40be380>, <__main__.Case object at 0x7f35f40bd210>, <__main__.Case object at 0x7f35f40be440>, <__main__.Case object at 0x7f35f40bc9d0>, <__main__.Case object at 0x7f35f40bd780>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 0\n",
      "Episode: 16, Total Steps: 124, Total Rewards: [-23, 90], Status Episode: True\n",
      "------------------------------------------End of episode 16 loop--------------------\n",
      "----- starting point of Episode 17 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999999, 0)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 40)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7999999999999999, 2)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.5, 88)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.7999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.5, 90)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 91)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 98)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 103)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 108)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 110)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 111)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 0), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 4) with action 0 to next state (5, 4): pull reward: 0.0\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 17 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.6, 27)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f409bd90>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f409b910>, <__main__.Case object at 0x7f35f409b070>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f40eeec0>, <__main__.Case object at 0x7f35f40ecfa0>, <__main__.Case object at 0x7f35f40ec850>, <__main__.Case object at 0x7f35f40eead0>, <__main__.Case object at 0x7f35f40eea70>, <__main__.Case object at 0x7f35f40ee0e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f4090b80>, <__main__.Case object at 0x7f35f407ec80>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b7f0>, <__main__.Case object at 0x7f35f409abc0>, <__main__.Case object at 0x7f35f409ac50>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409bc70>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f409ae90>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f40eeb30>, <__main__.Case object at 0x7f35f40ec9a0>, <__main__.Case object at 0x7f35f40edb70>, <__main__.Case object at 0x7f35f40ec730>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.7000000000000001, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.7000000000000001, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 4, tv: 0.4, time steps: 115\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.6, time steps: 111\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 110\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 108\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 103\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 98\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 1, tv: 0.6, time steps: 91\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.6, time steps: 90\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.6, time steps: 88\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.4, time steps: 85\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 1, tv: 0.4, time steps: 84\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 3, tv: 0.4, time steps: 72\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 40\n",
      "Episode succeeded, case (4, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 3, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 4, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 4, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 4, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 1), 1, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 1), 4, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 0), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 0), 4, 0.5, 19)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.7000000000000001, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7000000000000001, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f409be50>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f409b790>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f40ee650>, <__main__.Case object at 0x7f35f40eea10>, <__main__.Case object at 0x7f35f40ec220>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40ec3d0>, <__main__.Case object at 0x7f35f40ee770>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f4073340>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f409b2e0>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f409b430>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f409b2b0>, <__main__.Case object at 0x7f35f409bdf0>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f40ec130>, <__main__.Case object at 0x7f35f40edab0>, <__main__.Case object at 0x7f35f40ecd90>, <__main__.Case object at 0x7f35f40ecfd0>, <__main__.Case object at 0x7f35f40ef850>, <__main__.Case object at 0x7f35f40efa30>]\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.8999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.8999999999999999, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.6)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.8999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8999999999999999, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 0\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 111\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 110\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 108\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 103\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 98\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 91\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.5, time steps: 90\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5, time steps: 88\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 40\n",
      "Episode: 17, Total Steps: 19, Total Rewards: [82, 90], Status Episode: True\n",
      "------------------------------------------End of episode 17 loop--------------------\n",
      "----- starting point of Episode 18 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8999999999999999, 0)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.8999999999999999, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.04470319335538647\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.8999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 18 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f409b040>, <__main__.Case object at 0x7f35f409b3a0>, <__main__.Case object at 0x7f35f409ba30>, <__main__.Case object at 0x7f35f409bd90>, <__main__.Case object at 0x7f35f409b070>, <__main__.Case object at 0x7f35f409ae60>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f40ef640>, <__main__.Case object at 0x7f35f40edb70>, <__main__.Case object at 0x7f35f40ec430>, <__main__.Case object at 0x7f35f40effd0>, <__main__.Case object at 0x7f35f40edae0>, <__main__.Case object at 0x7f35f40ec220>, <__main__.Case object at 0x7f35f40ed7e0>, <__main__.Case object at 0x7f35f40edff0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f4065390>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f409ad10>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f409b910>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f409bcd0>, <__main__.Case object at 0x7f35f40ec9a0>, <__main__.Case object at 0x7f35f409b640>, <__main__.Case object at 0x7f35f40eca00>, <__main__.Case object at 0x7f35f40eea10>, <__main__.Case object at 0x7f35f40ee1a0>, <__main__.Case object at 0x7f35f40ef580>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 1, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "Episode succeeded, case (4, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 3, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 1), 1, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 1), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 0), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 0), 4, 0.5, 18)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f409bca0>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f40eeb30>, <__main__.Case object at 0x7f35f40efe20>, <__main__.Case object at 0x7f35f40ee6e0>, <__main__.Case object at 0x7f35f40efa60>, <__main__.Case object at 0x7f35f40ee650>, <__main__.Case object at 0x7f35f40ec3d0>, <__main__.Case object at 0x7f35f40eca30>, <__main__.Case object at 0x7f35f40edf60>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f409b0d0>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f409bbb0>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409bac0>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f409ad40>, <__main__.Case object at 0x7f35f409b790>, <__main__.Case object at 0x7f35f40ef220>, <__main__.Case object at 0x7f35f40ecc70>, <__main__.Case object at 0x7f35f40ef850>, <__main__.Case object at 0x7f35f40ec100>, <__main__.Case object at 0x7f35f40ee0b0>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40ef250>, <__main__.Case object at 0x7f35f40ed720>]\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.9999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.9999999999999999, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.4, time steps: 111\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4, time steps: 110\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4, time steps: 108\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4, time steps: 103\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4, time steps: 98\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 1, tv: 0.4, time steps: 91\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.4, time steps: 90\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.4, time steps: 88\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4, time steps: 40\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.9999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.9999999999999999, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 0\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "Episode: 18, Total Steps: 18, Total Rewards: [83, 90], Status Episode: True\n",
      "------------------------------------------End of episode 18 loop--------------------\n",
      "----- starting point of Episode 19 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9999999999999999, 0)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.9999999999999999, 2)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 3) with action 4 to next state (8, 3): pull reward: 0.021985042129093982\n",
      "----- starting point of Episode 19 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 2 to next state (8, 4): pull reward: -0.1\n",
      "----- starting point of Episode 19 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 4) with action 0 to next state (8, 4): pull reward: 0.0\n",
      "----- starting point of Episode 19 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 4) with action 1 to next state (8, 3): pull reward: 0.1\n",
      "----- starting point of Episode 19 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 1 to next state (8, 2): pull reward: 0.1\n",
      "----- starting point of Episode 19 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 3 to next state (7, 2): pull reward: 0.011571775657104919\n",
      "----- starting point of Episode 19 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 3) with action 1 to next state (7, 2): pull reward: 0.1\n",
      "----- starting point of Episode 19 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 1, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f409b130>, <__main__.Case object at 0x7f35f409b040>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f409bd30>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f40edf60>, <__main__.Case object at 0x7f35f40eca00>, <__main__.Case object at 0x7f35f40ecc70>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40edb70>, <__main__.Case object at 0x7f35f40ed7e0>, <__main__.Case object at 0x7f35f40ecd90>, <__main__.Case object at 0x7f35f40ee830>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f409ad10>, <__main__.Case object at 0x7f35f409b610>, <__main__.Case object at 0x7f35f409abc0>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f409bc70>, <__main__.Case object at 0x7f35f409bdc0>, <__main__.Case object at 0x7f35f409ac50>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f409ae90>, <__main__.Case object at 0x7f35f409b400>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f40ef220>, <__main__.Case object at 0x7f35f40ee0b0>, <__main__.Case object at 0x7f35f40ec430>, <__main__.Case object at 0x7f35f40ec220>, <__main__.Case object at 0x7f35f40ee470>, <__main__.Case object at 0x7f35f40ed000>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 1, tv: 0.09999999999999998, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.09999999999999998, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.09999999999999998, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.09999999999999998, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f409b850>, <__main__.Case object at 0x7f35f409b640>, <__main__.Case object at 0x7f35f409afe0>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f409b520>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f40ede40>, <__main__.Case object at 0x7f35f40ec9d0>, <__main__.Case object at 0x7f35f40ef580>, <__main__.Case object at 0x7f35f40ec100>, <__main__.Case object at 0x7f35f40ef640>, <__main__.Case object at 0x7f35f40ecfd0>, <__main__.Case object at 0x7f35f40ee770>, <__main__.Case object at 0x7f35f40eea70>, <__main__.Case object at 0x7f35f40ec700>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409b3d0>, <__main__.Case object at 0x7f35f40999c0>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f409bd90>, <__main__.Case object at 0x7f35f409be50>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f409aef0>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f40efa30>, <__main__.Case object at 0x7f35f40ee1a0>, <__main__.Case object at 0x7f35f40ef850>, <__main__.Case object at 0x7f35f40ef250>, <__main__.Case object at 0x7f35f40ec850>, <__main__.Case object at 0x7f35f40edff0>, <__main__.Case object at 0x7f35f40ecfa0>, <__main__.Case object at 0x7f35f40ecdf0>]\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.4, time steps: 27\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 1, 0.5)\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 1, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 1, 0.5)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (8, 3), solution: 1, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (8, 4), solution: 1, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "Episode: 19, Total Steps: 20, Total Rewards: [-102, 81], Status Episode: False\n",
      "------------------------------------------End of episode 19 loop--------------------\n",
      "----- starting point of Episode 20 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 3) with action 2 to next state (4, 4): pull reward: -0.1\n",
      "----- starting point of Episode 20 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 186 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 187 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 188 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 189 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 190 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 191 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 192 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 193 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 194 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 195 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 196 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 197 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 198 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 199 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 200 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 201 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 202 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 203 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 204 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 205 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 206 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 207 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 208 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 209 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 210 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 211 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 212 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 213 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 214 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 215 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 216 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 217 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 218 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 219 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 220 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 20 in steps 221 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 222 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4070b80>, <__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f409bf40>, <__main__.Case object at 0x7f35f409ad40>, <__main__.Case object at 0x7f35f409ac50>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409bca0>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f409bf70>, <__main__.Case object at 0x7f35f4099a20>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f40ed030>, <__main__.Case object at 0x7f35f40eed10>, <__main__.Case object at 0x7f35f40efdf0>, <__main__.Case object at 0x7f35f40effd0>, <__main__.Case object at 0x7f35f40ef370>, <__main__.Case object at 0x7f35f40eead0>, <__main__.Case object at 0x7f35f40ef6a0>, <__main__.Case object at 0x7f35f40ec9a0>, <__main__.Case object at 0x7f35f40eebc0>, <__main__.Case object at 0x7f35f40eecb0>, <__main__.Case object at 0x7f35f40efee0>, <__main__.Case object at 0x7f35f40ec130>, <__main__.Case object at 0x7f35f40efa00>, <__main__.Case object at 0x7f35f40ecbe0>, <__main__.Case object at 0x7f35f40ef7f0>, <__main__.Case object at 0x7f35f40eef20>, <__main__.Case object at 0x7f35f40ec7f0>, <__main__.Case object at 0x7f35f40edea0>, <__main__.Case object at 0x7f35f40ed930>, <__main__.Case object at 0x7f35f40ef2b0>, <__main__.Case object at 0x7f35f40ed8d0>, <__main__.Case object at 0x7f35f40eedd0>, <__main__.Case object at 0x7f35f40ed1b0>, <__main__.Case object at 0x7f35f40ee650>, <__main__.Case object at 0x7f35f40ed0c0>, <__main__.Case object at 0x7f35f40ee2c0>, <__main__.Case object at 0x7f35f40ecd60>, <__main__.Case object at 0x7f35f40ef610>, <__main__.Case object at 0x7f35f40ec7c0>, <__main__.Case object at 0x7f35f40ecf70>, <__main__.Case object at 0x7f35f40ec640>, <__main__.Case object at 0x7f35f40ed630>, <__main__.Case object at 0x7f35f40ed6c0>, <__main__.Case object at 0x7f35f40ef6d0>, <__main__.Case object at 0x7f35f40eee60>, <__main__.Case object at 0x7f35f40eec80>, <__main__.Case object at 0x7f35f40ecc40>, <__main__.Case object at 0x7f35f40ecca0>, <__main__.Case object at 0x7f35f40ef4f0>, <__main__.Case object at 0x7f35f40ec970>, <__main__.Case object at 0x7f35f40ef310>, <__main__.Case object at 0x7f35f40ef130>, <__main__.Case object at 0x7f35f40ed900>, <__main__.Case object at 0x7f35f40effa0>, <__main__.Case object at 0x7f35f40edba0>, <__main__.Case object at 0x7f35f40eeaa0>, <__main__.Case object at 0x7f35f40ef2e0>, <__main__.Case object at 0x7f35f40ed960>, <__main__.Case object at 0x7f35f40ed210>, <__main__.Case object at 0x7f35f40ed330>, <__main__.Case object at 0x7f35f40ee050>, <__main__.Case object at 0x7f35f40edc60>, <__main__.Case object at 0x7f35f40efb20>, <__main__.Case object at 0x7f35f40f4550>, <__main__.Case object at 0x7f35f40f6bf0>, <__main__.Case object at 0x7f35f40f72b0>, <__main__.Case object at 0x7f35f40f7dc0>, <__main__.Case object at 0x7f35f40f4c10>, <__main__.Case object at 0x7f35f40f7340>, <__main__.Case object at 0x7f35f40f4d30>, <__main__.Case object at 0x7f35f40f5360>, <__main__.Case object at 0x7f35f40f60b0>, <__main__.Case object at 0x7f35f40f79a0>, <__main__.Case object at 0x7f35f40f6530>, <__main__.Case object at 0x7f35f40f6a70>, <__main__.Case object at 0x7f35f40f74f0>, <__main__.Case object at 0x7f35f40f6e60>, <__main__.Case object at 0x7f35f40f7e50>, <__main__.Case object at 0x7f35f40f7eb0>, <__main__.Case object at 0x7f35f40f6fe0>, <__main__.Case object at 0x7f35f40f4250>, <__main__.Case object at 0x7f35f40f4eb0>, <__main__.Case object at 0x7f35f40f59c0>, <__main__.Case object at 0x7f35f40f68c0>, <__main__.Case object at 0x7f35f40f4460>, <__main__.Case object at 0x7f35f40f4400>, <__main__.Case object at 0x7f35f40f5300>, <__main__.Case object at 0x7f35f40f7e20>, <__main__.Case object at 0x7f35f40f75b0>, <__main__.Case object at 0x7f35f40f4df0>, <__main__.Case object at 0x7f35f40f42b0>, <__main__.Case object at 0x7f35f40f6e90>, <__main__.Case object at 0x7f35f40f6c20>, <__main__.Case object at 0x7f35f40f7be0>, <__main__.Case object at 0x7f35f40f6290>, <__main__.Case object at 0x7f35f40f6200>, <__main__.Case object at 0x7f35f40f6b30>, <__main__.Case object at 0x7f35f40f7e80>, <__main__.Case object at 0x7f35f40f6aa0>, <__main__.Case object at 0x7f35f40f46d0>, <__main__.Case object at 0x7f35f40f4880>, <__main__.Case object at 0x7f35f40f41f0>, <__main__.Case object at 0x7f35f40f7d60>, <__main__.Case object at 0x7f35f40f5180>, <__main__.Case object at 0x7f35f40f7df0>, <__main__.Case object at 0x7f35f40f5cf0>, <__main__.Case object at 0x7f35f40f7280>, <__main__.Case object at 0x7f35f40f4c70>, <__main__.Case object at 0x7f35f40f5ea0>, <__main__.Case object at 0x7f35f40f48b0>, <__main__.Case object at 0x7f35f40bf5e0>, <__main__.Case object at 0x7f35f40bfe80>, <__main__.Case object at 0x7f35f40bdd50>, <__main__.Case object at 0x7f35f40bf2e0>, <__main__.Case object at 0x7f35f40bd300>, <__main__.Case object at 0x7f35f40bcf10>, <__main__.Case object at 0x7f35f40bf9a0>, <__main__.Case object at 0x7f35f40bce80>, <__main__.Case object at 0x7f35f40bd900>, <__main__.Case object at 0x7f35f40bebc0>, <__main__.Case object at 0x7f35f40bf130>, <__main__.Case object at 0x7f35f40bf070>, <__main__.Case object at 0x7f35f40bcd00>, <__main__.Case object at 0x7f35f40be200>, <__main__.Case object at 0x7f35f40bfbb0>, <__main__.Case object at 0x7f35f40bf490>, <__main__.Case object at 0x7f35f40bd150>, <__main__.Case object at 0x7f35f40bf340>, <__main__.Case object at 0x7f35f40bc220>, <__main__.Case object at 0x7f35f40bf100>, <__main__.Case object at 0x7f35f40be0e0>, <__main__.Case object at 0x7f35f40be6b0>, <__main__.Case object at 0x7f35f40bc3d0>, <__main__.Case object at 0x7f35f40bf970>, <__main__.Case object at 0x7f35f40bf790>, <__main__.Case object at 0x7f35f40be710>, <__main__.Case object at 0x7f35f40bc610>, <__main__.Case object at 0x7f35f40bc6d0>, <__main__.Case object at 0x7f35f40be560>, <__main__.Case object at 0x7f35f40bc5b0>, <__main__.Case object at 0x7f35f40bd720>, <__main__.Case object at 0x7f35f40bd1e0>, <__main__.Case object at 0x7f35f40bc880>, <__main__.Case object at 0x7f35f40bc460>, <__main__.Case object at 0x7f35f40be110>, <__main__.Case object at 0x7f35f40bf760>, <__main__.Case object at 0x7f35f40bc2e0>, <__main__.Case object at 0x7f35f40bdb70>, <__main__.Case object at 0x7f35f40bf820>, <__main__.Case object at 0x7f35f40bf880>, <__main__.Case object at 0x7f35f40bf8b0>, <__main__.Case object at 0x7f35f40be590>, <__main__.Case object at 0x7f35f40bdba0>, <__main__.Case object at 0x7f35f40bdd80>, <__main__.Case object at 0x7f35f40bcdc0>, <__main__.Case object at 0x7f35f40bf910>, <__main__.Case object at 0x7f35f40be530>, <__main__.Case object at 0x7f35f40bff40>, <__main__.Case object at 0x7f35f40bd480>, <__main__.Case object at 0x7f35f40bed40>, <__main__.Case object at 0x7f35f40bed70>, <__main__.Case object at 0x7f35f40bc040>, <__main__.Case object at 0x7f35f40beb60>, <__main__.Case object at 0x7f35f40bd7e0>, <__main__.Case object at 0x7f35f40bc820>, <__main__.Case object at 0x7f35f40f4280>, <__main__.Case object at 0x7f35f40daf50>, <__main__.Case object at 0x7f35f40dbe50>, <__main__.Case object at 0x7f35f40d81f0>, <__main__.Case object at 0x7f35f40bf4c0>, <__main__.Case object at 0x7f35f40d8670>, <__main__.Case object at 0x7f35f40d8f70>, <__main__.Case object at 0x7f35f40d9360>, <__main__.Case object at 0x7f35f40bd600>, <__main__.Case object at 0x7f35f40d9480>, <__main__.Case object at 0x7f35f40d9cf0>, <__main__.Case object at 0x7f35f40d84c0>, <__main__.Case object at 0x7f35f40da170>, <__main__.Case object at 0x7f35f40da710>, <__main__.Case object at 0x7f35f40daa70>, <__main__.Case object at 0x7f35f40db070>, <__main__.Case object at 0x7f35f40d96c0>, <__main__.Case object at 0x7f35f40db6d0>, <__main__.Case object at 0x7f35f40d9fc0>, <__main__.Case object at 0x7f35f40d80a0>, <__main__.Case object at 0x7f35f40d81c0>, <__main__.Case object at 0x7f35f40d8760>, <__main__.Case object at 0x7f35f40d8ac0>, <__main__.Case object at 0x7f35f40d8e20>, <__main__.Case object at 0x7f35f40d8f40>, <__main__.Case object at 0x7f35f40d94e0>, <__main__.Case object at 0x7f35f40d9840>, <__main__.Case object at 0x7f35f40d9bd0>, <__main__.Case object at 0x7f35f40d9cc0>, <__main__.Case object at 0x7f35f40da260>, <__main__.Case object at 0x7f35f40da5c0>, <__main__.Case object at 0x7f35f40da920>, <__main__.Case object at 0x7f35f40daa40>, <__main__.Case object at 0x7f35f40daec0>, <__main__.Case object at 0x7f35f40db220>, <__main__.Case object at 0x7f35f40db580>, <__main__.Case object at 0x7f35f40dba90>, <__main__.Case object at 0x7f35f40dba30>, <__main__.Case object at 0x7f35f40dbc40>, <__main__.Case object at 0x7f35f40d8460>, <__main__.Case object at 0x7f35f40d8580>, <__main__.Case object at 0x7f35f40d8b20>, <__main__.Case object at 0x7f35f40d8e80>, <__main__.Case object at 0x7f35f40d91e0>, <__main__.Case object at 0x7f35f40d9300>, <__main__.Case object at 0x7f35f40d98a0>, <__main__.Case object at 0x7f35f40d9c00>, <__main__.Case object at 0x7f35f40d9f60>, <__main__.Case object at 0x7f35f40da080>, <__main__.Case object at 0x7f35f40da620>, <__main__.Case object at 0x7f35f40da980>, <__main__.Case object at 0x7f35f40dace0>, <__main__.Case object at 0x7f35f40f52a0>, <__main__.Case object at 0x7f35f40dadd0>, <__main__.Case object at 0x7f35f40db4c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f4073340>, <__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f409b160>, <__main__.Case object at 0x7f35f4098520>, <__main__.Case object at 0x7f35f409b7f0>, <__main__.Case object at 0x7f35f409ace0>, <__main__.Case object at 0x7f35f409b790>, <__main__.Case object at 0x7f35f409b5e0>, <__main__.Case object at 0x7f35f409b340>, <__main__.Case object at 0x7f35f409af50>, <__main__.Case object at 0x7f35f409b1c0>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f40ec460>, <__main__.Case object at 0x7f35f409ba30>, <__main__.Case object at 0x7f35f40ecb80>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f40edb70>, <__main__.Case object at 0x7f35f40efa60>, <__main__.Case object at 0x7f35f40ee0e0>, <__main__.Case object at 0x7f35f409add0>, <__main__.Case object at 0x7f35f40ec040>, <__main__.Case object at 0x7f35f40ee020>, <__main__.Case object at 0x7f35f40ef9a0>, <__main__.Case object at 0x7f35f409b040>, <__main__.Case object at 0x7f35f40ed270>, <__main__.Case object at 0x7f35f40ee950>, <__main__.Case object at 0x7f35f40ec5b0>, <__main__.Case object at 0x7f35f409b520>, <__main__.Case object at 0x7f35f40edb40>, <__main__.Case object at 0x7f35f40ece20>, <__main__.Case object at 0x7f35f40ee4a0>, <__main__.Case object at 0x7f35f40ecd00>, <__main__.Case object at 0x7f35f40eeb00>, <__main__.Case object at 0x7f35f40ef760>, <__main__.Case object at 0x7f35f40ec1f0>, <__main__.Case object at 0x7f35f40ef0d0>, <__main__.Case object at 0x7f35f40ee5c0>, <__main__.Case object at 0x7f35f40eefe0>, <__main__.Case object at 0x7f35f40ec550>, <__main__.Case object at 0x7f35f40ef730>, <__main__.Case object at 0x7f35f40edcc0>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40ee860>, <__main__.Case object at 0x7f35f40ee7a0>, <__main__.Case object at 0x7f35f40ef490>, <__main__.Case object at 0x7f35f40ecd30>, <__main__.Case object at 0x7f35f40ed570>, <__main__.Case object at 0x7f35f40ef100>, <__main__.Case object at 0x7f35f40ee2f0>, <__main__.Case object at 0x7f35f40ef3d0>, <__main__.Case object at 0x7f35f40ecc10>, <__main__.Case object at 0x7f35f40eef50>, <__main__.Case object at 0x7f35f40ee560>, <__main__.Case object at 0x7f35f40ec670>, <__main__.Case object at 0x7f35f40ed870>, <__main__.Case object at 0x7f35f40ed8a0>, <__main__.Case object at 0x7f35f40ed300>, <__main__.Case object at 0x7f35f40ec610>, <__main__.Case object at 0x7f35f40f6620>, <__main__.Case object at 0x7f35f40f7a90>, <__main__.Case object at 0x7f35f40f65f0>, <__main__.Case object at 0x7f35f40eda80>, <__main__.Case object at 0x7f35f40f4e80>, <__main__.Case object at 0x7f35f40f6710>, <__main__.Case object at 0x7f35f40efd00>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40f5cc0>, <__main__.Case object at 0x7f35f40f6350>, <__main__.Case object at 0x7f35f40ec6d0>, <__main__.Case object at 0x7f35f40f5d50>, <__main__.Case object at 0x7f35f40f76d0>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f40ee440>, <__main__.Case object at 0x7f35f40f6920>, <__main__.Case object at 0x7f35f40f7bb0>, <__main__.Case object at 0x7f35f40f6d10>, <__main__.Case object at 0x7f35f40f6860>, <__main__.Case object at 0x7f35f40f53c0>, <__main__.Case object at 0x7f35f40f6740>, <__main__.Case object at 0x7f35f40f7b50>, <__main__.Case object at 0x7f35f40f6440>, <__main__.Case object at 0x7f35f40f6650>, <__main__.Case object at 0x7f35f40f7f70>, <__main__.Case object at 0x7f35f40f6080>, <__main__.Case object at 0x7f35f40f7970>, <__main__.Case object at 0x7f35f40f4a90>, <__main__.Case object at 0x7f35f40f44f0>, <__main__.Case object at 0x7f35f40f61d0>, <__main__.Case object at 0x7f35f40f5d20>, <__main__.Case object at 0x7f35f40f6c50>, <__main__.Case object at 0x7f35f40f6800>, <__main__.Case object at 0x7f35f40f5270>, <__main__.Case object at 0x7f35f40f5ff0>, <__main__.Case object at 0x7f35f40f6470>, <__main__.Case object at 0x7f35f40f7b80>, <__main__.Case object at 0x7f35f40f60e0>, <__main__.Case object at 0x7f35f40f40d0>, <__main__.Case object at 0x7f35f40f43d0>, <__main__.Case object at 0x7f35f40f61a0>, <__main__.Case object at 0x7f35f40f74c0>, <__main__.Case object at 0x7f35f40f4730>, <__main__.Case object at 0x7f35f40eca90>, <__main__.Case object at 0x7f35f40bf0a0>, <__main__.Case object at 0x7f35f40f4af0>, <__main__.Case object at 0x7f35f40bec50>, <__main__.Case object at 0x7f35f40bda80>, <__main__.Case object at 0x7f35f40bfbe0>, <__main__.Case object at 0x7f35f40f7af0>, <__main__.Case object at 0x7f35f40be2c0>, <__main__.Case object at 0x7f35f40bd570>, <__main__.Case object at 0x7f35f40bd960>, <__main__.Case object at 0x7f35f40f6d70>, <__main__.Case object at 0x7f35f40bcf70>, <__main__.Case object at 0x7f35f40bfd30>, <__main__.Case object at 0x7f35f40bef20>, <__main__.Case object at 0x7f35f40f6980>, <__main__.Case object at 0x7f35f40bdae0>, <__main__.Case object at 0x7f35f40bd510>, <__main__.Case object at 0x7f35f40bd4b0>, <__main__.Case object at 0x7f35f40bf8e0>, <__main__.Case object at 0x7f35f40be3b0>, <__main__.Case object at 0x7f35f40bf3d0>, <__main__.Case object at 0x7f35f40bec80>, <__main__.Case object at 0x7f35f40bfa00>, <__main__.Case object at 0x7f35f40bd180>, <__main__.Case object at 0x7f35f40bca00>, <__main__.Case object at 0x7f35f40be170>, <__main__.Case object at 0x7f35f40bc6a0>, <__main__.Case object at 0x7f35f40bead0>, <__main__.Case object at 0x7f35f40bd8a0>, <__main__.Case object at 0x7f35f40bfd90>, <__main__.Case object at 0x7f35f40be470>, <__main__.Case object at 0x7f35f40bd810>, <__main__.Case object at 0x7f35f40bf160>, <__main__.Case object at 0x7f35f40bece0>, <__main__.Case object at 0x7f35f40bee30>, <__main__.Case object at 0x7f35f40be1a0>, <__main__.Case object at 0x7f35f40bcdf0>, <__main__.Case object at 0x7f35f40bd660>, <__main__.Case object at 0x7f35f40bfee0>, <__main__.Case object at 0x7f35f40bcee0>, <__main__.Case object at 0x7f35f40bca60>, <__main__.Case object at 0x7f35f40bdfc0>, <__main__.Case object at 0x7f35f40bf250>, <__main__.Case object at 0x7f35f40bfcd0>, <__main__.Case object at 0x7f35f40bfc70>, <__main__.Case object at 0x7f35f40bd6c0>, <__main__.Case object at 0x7f35f40bf400>, <__main__.Case object at 0x7f35f40be3e0>, <__main__.Case object at 0x7f35f40bf220>, <__main__.Case object at 0x7f35f40bcb20>, <__main__.Case object at 0x7f35f40bd930>, <__main__.Case object at 0x7f35f40bf1c0>, <__main__.Case object at 0x7f35f40db520>, <__main__.Case object at 0x7f35f40d8280>, <__main__.Case object at 0x7f35f40bea10>, <__main__.Case object at 0x7f35f40d8ca0>, <__main__.Case object at 0x7f35f40d9090>, <__main__.Case object at 0x7f35f40dbe20>, <__main__.Case object at 0x7f35f40d9b10>, <__main__.Case object at 0x7f35f40d9e10>, <__main__.Case object at 0x7f35f40be050>, <__main__.Case object at 0x7f35f40da560>, <__main__.Case object at 0x7f35f40da890>, <__main__.Case object at 0x7f35f40dab90>, <__main__.Case object at 0x7f35f40bcc10>, <__main__.Case object at 0x7f35f40db550>, <__main__.Case object at 0x7f35f40db7f0>, <__main__.Case object at 0x7f35f40dbb50>, <__main__.Case object at 0x7f35f40be500>, <__main__.Case object at 0x7f35f40d85b0>, <__main__.Case object at 0x7f35f40d8880>, <__main__.Case object at 0x7f35f40d8be0>, <__main__.Case object at 0x7f35f40d9180>, <__main__.Case object at 0x7f35f40d9330>, <__main__.Case object at 0x7f35f40d9600>, <__main__.Case object at 0x7f35f40d9960>, <__main__.Case object at 0x7f35f40d9f00>, <__main__.Case object at 0x7f35f40da0b0>, <__main__.Case object at 0x7f35f40da380>, <__main__.Case object at 0x7f35f40da6e0>, <__main__.Case object at 0x7f35f40dac80>, <__main__.Case object at 0x7f35f40d8130>, <__main__.Case object at 0x7f35f40dafe0>, <__main__.Case object at 0x7f35f40db340>, <__main__.Case object at 0x7f35f40db400>, <__main__.Case object at 0x7f35f40d8970>, <__main__.Case object at 0x7f35f40d8100>, <__main__.Case object at 0x7f35f40d87c0>, <__main__.Case object at 0x7f35f40d8850>, <__main__.Case object at 0x7f35f40d8c40>, <__main__.Case object at 0x7f35f40d8fa0>, <__main__.Case object at 0x7f35f40d9540>, <__main__.Case object at 0x7f35f40d82b0>, <__main__.Case object at 0x7f35f40d99c0>, <__main__.Case object at 0x7f35f40d9d20>, <__main__.Case object at 0x7f35f40da2c0>, <__main__.Case object at 0x7f35f40da350>, <__main__.Case object at 0x7f35f40da740>, <__main__.Case object at 0x7f35f40daaa0>, <__main__.Case object at 0x7f35f40dbeb0>, <__main__.Case object at 0x7f35f40db280>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.9)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f409bdf0>, <__main__.Case object at 0x7f35f409bbb0>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f409b070>, <__main__.Case object at 0x7f35f409bee0>, <__main__.Case object at 0x7f35f409af80>, <__main__.Case object at 0x7f35f409b430>, <__main__.Case object at 0x7f35f409b2e0>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f40edbd0>, <__main__.Case object at 0x7f35f40edab0>, <__main__.Case object at 0x7f35f409ad70>, <__main__.Case object at 0x7f35f40eebf0>, <__main__.Case object at 0x7f35f409ae30>, <__main__.Case object at 0x7f35f40eee00>, <__main__.Case object at 0x7f35f40eceb0>, <__main__.Case object at 0x7f35f40ee6e0>, <__main__.Case object at 0x7f35f40ed720>, <__main__.Case object at 0x7f35f40ed750>, <__main__.Case object at 0x7f35f40eeb30>, <__main__.Case object at 0x7f35f40edf00>, <__main__.Case object at 0x7f35f40ee4d0>, <__main__.Case object at 0x7f35f40ec6a0>, <__main__.Case object at 0x7f35f40ec340>, <__main__.Case object at 0x7f35f40ee500>, <__main__.Case object at 0x7f35f40eeec0>, <__main__.Case object at 0x7f35f40ee410>, <__main__.Case object at 0x7f35f40eeb90>, <__main__.Case object at 0x7f35f40efe20>, <__main__.Case object at 0x7f35f40ed360>, <__main__.Case object at 0x7f35f40ec160>, <__main__.Case object at 0x7f35f40efd60>, <__main__.Case object at 0x7f35f40eee30>, <__main__.Case object at 0x7f35f40efc70>, <__main__.Case object at 0x7f35f40ee5f0>, <__main__.Case object at 0x7f35f40ed5d0>, <__main__.Case object at 0x7f35f40edd20>, <__main__.Case object at 0x7f35f40ed000>, <__main__.Case object at 0x7f35f40ecf10>, <__main__.Case object at 0x7f35f40ed4e0>, <__main__.Case object at 0x7f35f40ef010>, <__main__.Case object at 0x7f35f40ec0a0>, <__main__.Case object at 0x7f35f40eded0>, <__main__.Case object at 0x7f35f40edd50>, <__main__.Case object at 0x7f35f40eea70>, <__main__.Case object at 0x7f35f40ee710>, <__main__.Case object at 0x7f35f40efe80>, <__main__.Case object at 0x7f35f40ee890>, <__main__.Case object at 0x7f35f40ecaf0>, <__main__.Case object at 0x7f35f40ef970>, <__main__.Case object at 0x7f35f40eece0>, <__main__.Case object at 0x7f35f40efbe0>, <__main__.Case object at 0x7f35f40ec4f0>, <__main__.Case object at 0x7f35f40ed240>, <__main__.Case object at 0x7f35f40ee8f0>, <__main__.Case object at 0x7f35f40eff40>, <__main__.Case object at 0x7f35f40ef700>, <__main__.Case object at 0x7f35f40ed540>, <__main__.Case object at 0x7f35f40ee320>, <__main__.Case object at 0x7f35f40ec280>, <__main__.Case object at 0x7f35f40efca0>, <__main__.Case object at 0x7f35f40ed150>, <__main__.Case object at 0x7f35f40ec940>, <__main__.Case object at 0x7f35f40f4dc0>, <__main__.Case object at 0x7f35f40f7fd0>, <__main__.Case object at 0x7f35f40f76a0>, <__main__.Case object at 0x7f35f40f7610>, <__main__.Case object at 0x7f35f40f6da0>, <__main__.Case object at 0x7f35f40ee590>, <__main__.Case object at 0x7f35f40f7640>, <__main__.Case object at 0x7f35f40f7850>, <__main__.Case object at 0x7f35f40f7670>, <__main__.Case object at 0x7f35f40ed7b0>, <__main__.Case object at 0x7f35f40f4850>, <__main__.Case object at 0x7f35f40f6500>, <__main__.Case object at 0x7f35f40f7760>, <__main__.Case object at 0x7f35f40f7430>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40f4be0>, <__main__.Case object at 0x7f35f40f4820>, <__main__.Case object at 0x7f35f40f7910>, <__main__.Case object at 0x7f35f40f5e70>, <__main__.Case object at 0x7f35f40f5900>, <__main__.Case object at 0x7f35f40f6410>, <__main__.Case object at 0x7f35f40f6e00>, <__main__.Case object at 0x7f35f40f70a0>, <__main__.Case object at 0x7f35f40f7880>, <__main__.Case object at 0x7f35f40f6bc0>, <__main__.Case object at 0x7f35f40f6770>, <__main__.Case object at 0x7f35f40f4700>, <__main__.Case object at 0x7f35f40f6890>, <__main__.Case object at 0x7f35f40f7790>, <__main__.Case object at 0x7f35f40f4340>, <__main__.Case object at 0x7f35f40f5f60>, <__main__.Case object at 0x7f35f40f73d0>, <__main__.Case object at 0x7f35f40f7c10>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40f64d0>, <__main__.Case object at 0x7f35f40f51b0>, <__main__.Case object at 0x7f35f40f7310>, <__main__.Case object at 0x7f35f40f78e0>, <__main__.Case object at 0x7f35f40f64a0>, <__main__.Case object at 0x7f35f40f45e0>, <__main__.Case object at 0x7f35f40f4430>, <__main__.Case object at 0x7f35f40f6560>, <__main__.Case object at 0x7f35f40f6b60>, <__main__.Case object at 0x7f35f40f79d0>, <__main__.Case object at 0x7f35f40f6a10>, <__main__.Case object at 0x7f35f40bd870>, <__main__.Case object at 0x7f35f40bf7f0>, <__main__.Case object at 0x7f35f40bcbe0>, <__main__.Case object at 0x7f35f40f6950>, <__main__.Case object at 0x7f35f40bf370>, <__main__.Case object at 0x7f35f40bd630>, <__main__.Case object at 0x7f35f40bfca0>, <__main__.Case object at 0x7f35f40be860>, <__main__.Case object at 0x7f35f40f6320>, <__main__.Case object at 0x7f35f40bde70>, <__main__.Case object at 0x7f35f40f5f00>, <__main__.Case object at 0x7f35f40bc940>, <__main__.Case object at 0x7f35f40bdff0>, <__main__.Case object at 0x7f35f40bff10>, <__main__.Case object at 0x7f35f40beec0>, <__main__.Case object at 0x7f35f40be320>, <__main__.Case object at 0x7f35f40bddb0>, <__main__.Case object at 0x7f35f40bd060>, <__main__.Case object at 0x7f35f40be620>, <__main__.Case object at 0x7f35f40befe0>, <__main__.Case object at 0x7f35f40beda0>, <__main__.Case object at 0x7f35f40bd330>, <__main__.Case object at 0x7f35f40bfd60>, <__main__.Case object at 0x7f35f40bc730>, <__main__.Case object at 0x7f35f40bd840>, <__main__.Case object at 0x7f35f40bcca0>, <__main__.Case object at 0x7f35f40bd8d0>, <__main__.Case object at 0x7f35f40bef80>, <__main__.Case object at 0x7f35f40bed10>, <__main__.Case object at 0x7f35f40bfb80>, <__main__.Case object at 0x7f35f40bdf30>, <__main__.Case object at 0x7f35f40eff10>, <__main__.Case object at 0x7f35f40be680>, <__main__.Case object at 0x7f35f40bfc40>, <__main__.Case object at 0x7f35f40bca90>, <__main__.Case object at 0x7f35f40bf430>, <__main__.Case object at 0x7f35f40bf730>, <__main__.Case object at 0x7f35f40be4d0>, <__main__.Case object at 0x7f35f40be9b0>, <__main__.Case object at 0x7f35f40bfa60>, <__main__.Case object at 0x7f35f40bf550>, <__main__.Case object at 0x7f35f40be260>, <__main__.Case object at 0x7f35f40bcfa0>, <__main__.Case object at 0x7f35f40bdf00>, <__main__.Case object at 0x7f35f40bdcf0>, <__main__.Case object at 0x7f35f40bd5d0>, <__main__.Case object at 0x7f35f40bfaf0>, <__main__.Case object at 0x7f35f40be230>, <__main__.Case object at 0x7f35f40bc520>, <__main__.Case object at 0x7f35f40be4a0>, <__main__.Case object at 0x7f35f40bdb10>, <__main__.Case object at 0x7f35f40becb0>, <__main__.Case object at 0x7f35f40bfa90>, <__main__.Case object at 0x7f35f40bc250>, <__main__.Case object at 0x7f35f40bf6d0>, <__main__.Case object at 0x7f35f40bc310>, <__main__.Case object at 0x7f35f40dbd30>, <__main__.Case object at 0x7f35f40bf4f0>, <__main__.Case object at 0x7f35f40d83a0>, <__main__.Case object at 0x7f35f40d8790>, <__main__.Case object at 0x7f35f40bd4e0>, <__main__.Case object at 0x7f35f40d8910>, <__main__.Case object at 0x7f35f40ef820>, <__main__.Case object at 0x7f35f40d95a0>, <__main__.Case object at 0x7f35f40d9900>, <__main__.Case object at 0x7f35f40d9690>, <__main__.Case object at 0x7f35f40d9f90>, <__main__.Case object at 0x7f35f40d9270>, <__main__.Case object at 0x7f35f40da290>, <__main__.Case object at 0x7f35f40d8dc0>, <__main__.Case object at 0x7f35f40dae90>, <__main__.Case object at 0x7f35f40db250>, <__main__.Case object at 0x7f35f40bf040>, <__main__.Case object at 0x7f35f40db970>, <__main__.Case object at 0x7f35f40dbc70>, <__main__.Case object at 0x7f35f40d82e0>, <__main__.Case object at 0x7f35f40db370>, <__main__.Case object at 0x7f35f40d89d0>, <__main__.Case object at 0x7f35f40d8d00>, <__main__.Case object at 0x7f35f40d9060>, <__main__.Case object at 0x7f35f40d8400>, <__main__.Case object at 0x7f35f40d9750>, <__main__.Case object at 0x7f35f40d9a80>, <__main__.Case object at 0x7f35f40d8af0>, <__main__.Case object at 0x7f35f40d9210>, <__main__.Case object at 0x7f35f40da410>, <__main__.Case object at 0x7f35f40da4d0>, <__main__.Case object at 0x7f35f40da800>, <__main__.Case object at 0x7f35f40d97e0>, <__main__.Case object at 0x7f35f40db130>, <__main__.Case object at 0x7f35f40dab60>, <__main__.Case object at 0x7f35f40db460>, <__main__.Case object at 0x7f35f40da470>, <__main__.Case object at 0x7f35f40dbd60>, <__main__.Case object at 0x7f35f40d8190>, <__main__.Case object at 0x7f35f40db7c0>, <__main__.Case object at 0x7f35f40d86a0>, <__main__.Case object at 0x7f35f40d8d60>, <__main__.Case object at 0x7f35f40d90c0>, <__main__.Case object at 0x7f35f40d9de0>, <__main__.Case object at 0x7f35f40d8490>, <__main__.Case object at 0x7f35f40db6a0>, <__main__.Case object at 0x7f35f40d9420>, <__main__.Case object at 0x7f35f40da1a0>, <__main__.Case object at 0x7f35f40d9e40>, <__main__.Case object at 0x7f35f40d91b0>, <__main__.Case object at 0x7f35f40dabc0>, <__main__.Case object at 0x7f35f40dae60>, <__main__.Case object at 0x7f35f40da860>, <__main__.Case object at 0x7f35f4091e40>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.4, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 1, tv: 0.4, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 1, tv: 0.4, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 1, tv: 0.4, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.4, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.4, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4, time steps: 18\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "Episode: 20, Total Steps: 223, Total Rewards: [-322, 90], Status Episode: False\n",
      "------------------------------------------End of episode 20 loop--------------------\n",
      "----- starting point of Episode 21 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 2) with action 3 to next state (6, 2): pull reward: -0.1\n",
      "----- starting point of Episode 21 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 2) with action 4 to next state (7, 2): pull reward: 0.1\n",
      "----- starting point of Episode 21 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 0 to next state (6, 3): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 186 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 187 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 188 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 189 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 190 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 191 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 192 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 193 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 194 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 195 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 196 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 197 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 198 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 199 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 200 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 201 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 202 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 203 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 204 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 205 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 206 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 207 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 208 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 209 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 210 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 211 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 212 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 213 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 214 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 215 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 216 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 217 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 218 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 219 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 220 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 221 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 222 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 223 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 224 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 225 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 226 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 227 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 228 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 229 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 230 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 231 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 232 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 233 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 234 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 235 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 236 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 237 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 238 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 239 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 240 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 241 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 242 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 243 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 244 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 245 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 246 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 247 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 248 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 249 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 250 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 251 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 252 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 253 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 254 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 255 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 256 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 257 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 258 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 259 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 260 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 261 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 262 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 263 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 264 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 265 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 266 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 267 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 268 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 269 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 270 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 271 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 272 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 273 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 274 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 275 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 276 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 277 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 278 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 279 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 280 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 281 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 282 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 283 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 284 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 21 in steps 285 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 286 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 287 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 288 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 289 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 290 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 291 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 292 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 293 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 294 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 295 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 296 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 297 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 298 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 4 to next state (2, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 299 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 2) with action 0 to next state (2, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 300 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1.0, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 2) with action 1 to next state (2, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f40f7a90>, <__main__.Case object at 0x7f35f40f63e0>, <__main__.Case object at 0x7f35f40f7160>, <__main__.Case object at 0x7f35f40f7010>, <__main__.Case object at 0x7f35f40f6620>, <__main__.Case object at 0x7f35f40f63b0>, <__main__.Case object at 0x7f35f40f6b00>, <__main__.Case object at 0x7f35f40f6080>, <__main__.Case object at 0x7f35f40f4790>, <__main__.Case object at 0x7f35f40f7730>, <__main__.Case object at 0x7f35f40f5390>, <__main__.Case object at 0x7f35f40f44c0>, <__main__.Case object at 0x7f35f40f61a0>, <__main__.Case object at 0x7f35f40f7af0>, <__main__.Case object at 0x7f35f40f4310>, <__main__.Case object at 0x7f35f40f6ad0>, <__main__.Case object at 0x7f35f40f7940>, <__main__.Case object at 0x7f35f40f5e10>, <__main__.Case object at 0x7f35f40f7fa0>, <__main__.Case object at 0x7f35f40f7220>, <__main__.Case object at 0x7f35f40f67d0>, <__main__.Case object at 0x7f35f40f69b0>, <__main__.Case object at 0x7f35f40f5fc0>, <__main__.Case object at 0x7f35f40f4ac0>, <__main__.Case object at 0x7f35f40f7040>, <__main__.Case object at 0x7f35f40f6f50>, <__main__.Case object at 0x7f35f40f41f0>, <__main__.Case object at 0x7f35f40f7d60>, <__main__.Case object at 0x7f35f40f4c70>, <__main__.Case object at 0x7f35f40f52a0>, <__main__.Case object at 0x7f35f40f7fd0>, <__main__.Case object at 0x7f35f40f76a0>, <__main__.Case object at 0x7f35f40f4f70>, <__main__.Case object at 0x7f35f40f6500>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40f4be0>, <__main__.Case object at 0x7f35f40f6410>, <__main__.Case object at 0x7f35f40f6e00>, <__main__.Case object at 0x7f35f40f7100>, <__main__.Case object at 0x7f35f40f7790>, <__main__.Case object at 0x7f35f40f62c0>, <__main__.Case object at 0x7f35f40f40a0>, <__main__.Case object at 0x7f35f40f7460>, <__main__.Case object at 0x7f35f40f46a0>, <__main__.Case object at 0x7f35f40f6b60>, <__main__.Case object at 0x7f35f40bc910>, <__main__.Case object at 0x7f35f40bec50>, <__main__.Case object at 0x7f35f40bda80>, <__main__.Case object at 0x7f35f40bc1f0>, <__main__.Case object at 0x7f35f40bef20>, <__main__.Case object at 0x7f35f40bf580>, <__main__.Case object at 0x7f35f40be7d0>, <__main__.Case object at 0x7f35f40bc280>, <__main__.Case object at 0x7f35f40be2f0>, <__main__.Case object at 0x7f35f40bcd30>, <__main__.Case object at 0x7f35f40be8c0>, <__main__.Case object at 0x7f35f40bece0>, <__main__.Case object at 0x7f35f40bf1f0>, <__main__.Case object at 0x7f35f40bc8b0>, <__main__.Case object at 0x7f35f40bf310>, <__main__.Case object at 0x7f35f40bd120>, <__main__.Case object at 0x7f35f40bfeb0>, <__main__.Case object at 0x7f35f40be050>, <__main__.Case object at 0x7f35f40be500>, <__main__.Case object at 0x7f35f40bd300>, <__main__.Case object at 0x7f35f40bce80>, <__main__.Case object at 0x7f35f40bd900>, <__main__.Case object at 0x7f35f40bded0>, <__main__.Case object at 0x7f35f40bf490>, <__main__.Case object at 0x7f35f40bc220>, <__main__.Case object at 0x7f35f40bf100>, <__main__.Case object at 0x7f35f40bcd00>, <__main__.Case object at 0x7f35f40bc610>, <__main__.Case object at 0x7f35f40bc5b0>, <__main__.Case object at 0x7f35f40bc880>, <__main__.Case object at 0x7f35f40bc460>, <__main__.Case object at 0x7f35f40bf820>, <__main__.Case object at 0x7f35f40be590>, <__main__.Case object at 0x7f35f40bdba0>, <__main__.Case object at 0x7f35f40bd480>, <__main__.Case object at 0x7f35f40bed40>, <__main__.Case object at 0x7f35f40beb60>, <__main__.Case object at 0x7f35f40bf4c0>, <__main__.Case object at 0x7f35f40bee00>, <__main__.Case object at 0x7f35f40bde10>, <__main__.Case object at 0x7f35f40be440>, <__main__.Case object at 0x7f35f40bfb50>, <__main__.Case object at 0x7f35f40bfdc0>, <__main__.Case object at 0x7f35f40bdc90>, <__main__.Case object at 0x7f35f40bcf40>, <__main__.Case object at 0x7f35f40bfe20>, <__main__.Case object at 0x7f35f40bd6f0>, <__main__.Case object at 0x7f35f40bde40>, <__main__.Case object at 0x7f35f40bc850>, <__main__.Case object at 0x7f35f40bfc40>, <__main__.Case object at 0x7f35f40bca90>, <__main__.Case object at 0x7f35f40bfa60>, <__main__.Case object at 0x7f35f40bcfa0>, <__main__.Case object at 0x7f35f40bd5d0>, <__main__.Case object at 0x7f35f40bfaf0>, <__main__.Case object at 0x7f35f40bc4c0>, <__main__.Case object at 0x7f35f40becb0>, <__main__.Case object at 0x7f35f40bd4e0>, <__main__.Case object at 0x7f35f40d8430>, <__main__.Case object at 0x7f35f40db670>, <__main__.Case object at 0x7f35f40d8160>, <__main__.Case object at 0x7f35f40d9090>, <__main__.Case object at 0x7f35f40dbe20>, <__main__.Case object at 0x7f35f40dab00>, <__main__.Case object at 0x7f35f40db7f0>, <__main__.Case object at 0x7f35f40d87f0>, <__main__.Case object at 0x7f35f40d8b50>, <__main__.Case object at 0x7f35f40d9c30>, <__main__.Case object at 0x7f35f40da650>, <__main__.Case object at 0x7f35f40daef0>, <__main__.Case object at 0x7f35f40db2b0>, <__main__.Case object at 0x7f35f40d88e0>, <__main__.Case object at 0x7f35f40d9150>, <__main__.Case object at 0x7f35f40d95d0>, <__main__.Case object at 0x7f35f40d9ed0>, <__main__.Case object at 0x7f35f40db100>, <__main__.Case object at 0x7f35f40d81f0>, <__main__.Case object at 0x7f35f40d9120>, <__main__.Case object at 0x7f35f40d9510>, <__main__.Case object at 0x7f35f40da710>, <__main__.Case object at 0x7f35f40d96c0>, <__main__.Case object at 0x7f35f40d80a0>, <__main__.Case object at 0x7f35f40d81c0>, <__main__.Case object at 0x7f35f40d94e0>, <__main__.Case object at 0x7f35f40d9cc0>, <__main__.Case object at 0x7f35f40da920>, <__main__.Case object at 0x7f35f40daa40>, <__main__.Case object at 0x7f35f40dba30>, <__main__.Case object at 0x7f35f40d8580>, <__main__.Case object at 0x7f35f40d91e0>, <__main__.Case object at 0x7f35f40d9300>, <__main__.Case object at 0x7f35f40da620>, <__main__.Case object at 0x7f35f40da980>, <__main__.Case object at 0x7f35f40db040>, <__main__.Case object at 0x7f35f40dbd30>, <__main__.Case object at 0x7f35f40d96f0>, <__main__.Case object at 0x7f35f40d9ff0>, <__main__.Case object at 0x7f35f40da320>, <__main__.Case object at 0x7f35f40da9e0>, <__main__.Case object at 0x7f35f40db490>, <__main__.Case object at 0x7f35f40db370>, <__main__.Case object at 0x7f35f40d9060>, <__main__.Case object at 0x7f35f40d8400>, <__main__.Case object at 0x7f35f40da410>, <__main__.Case object at 0x7f35f40d97e0>, <__main__.Case object at 0x7f35f40db460>, <__main__.Case object at 0x7f35f40da470>, <__main__.Case object at 0x7f35f40d8d60>, <__main__.Case object at 0x7f35f40d90c0>, <__main__.Case object at 0x7f35f40d8490>, <__main__.Case object at 0x7f35f40da1a0>, <__main__.Case object at 0x7f35f40da860>, <__main__.Case object at 0x7f35f40dbf40>, <__main__.Case object at 0x7f35f40dbc10>, <__main__.Case object at 0x7f35f40d9c90>, <__main__.Case object at 0x7f35f40d8790>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f4064070>, <__main__.Case object at 0x7f35f409bbe0>, <__main__.Case object at 0x7f35f409b250>, <__main__.Case object at 0x7f35f409ac80>, <__main__.Case object at 0x7f35f409b8e0>, <__main__.Case object at 0x7f35f409bc10>, <__main__.Case object at 0x7f35f409ac50>, <__main__.Case object at 0x7f35f409b100>, <__main__.Case object at 0x7f35f4099a20>, <__main__.Case object at 0x7f35f409b880>, <__main__.Case object at 0x7f35f409b9d0>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f409b9a0>, <__main__.Case object at 0x7f35f409b850>, <__main__.Case object at 0x7f35f409b190>, <__main__.Case object at 0x7f35f409b460>, <__main__.Case object at 0x7f35f40eca30>, <__main__.Case object at 0x7f35f40ef850>, <__main__.Case object at 0x7f35f40ec040>, <__main__.Case object at 0x7f35f40ee680>, <__main__.Case object at 0x7f35f40edc30>, <__main__.Case object at 0x7f35f40edb40>, <__main__.Case object at 0x7f35f40ef760>, <__main__.Case object at 0x7f35f40ee5c0>, <__main__.Case object at 0x7f35f40ef730>, <__main__.Case object at 0x7f35f40edcc0>, <__main__.Case object at 0x7f35f40ecd30>, <__main__.Case object at 0x7f35f40ee2f0>, <__main__.Case object at 0x7f35f40eef50>, <__main__.Case object at 0x7f35f40ee560>, <__main__.Case object at 0x7f35f40ed300>, <__main__.Case object at 0x7f35f40efd00>, <__main__.Case object at 0x7f35f40effd0>, <__main__.Case object at 0x7f35f40ef370>, <__main__.Case object at 0x7f35f40eecb0>, <__main__.Case object at 0x7f35f40efa00>, <__main__.Case object at 0x7f35f40eef20>, <__main__.Case object at 0x7f35f40ec7f0>, <__main__.Case object at 0x7f35f40eedd0>, <__main__.Case object at 0x7f35f40ed0c0>, <__main__.Case object at 0x7f35f40ef610>, <__main__.Case object at 0x7f35f40ec7c0>, <__main__.Case object at 0x7f35f40ef6d0>, <__main__.Case object at 0x7f35f40ecc40>, <__main__.Case object at 0x7f35f40ec970>, <__main__.Case object at 0x7f35f40ef310>, <__main__.Case object at 0x7f35f40eeaa0>, <__main__.Case object at 0x7f35f40ed210>, <__main__.Case object at 0x7f35f40edc60>, <__main__.Case object at 0x7f35f40efb20>, <__main__.Case object at 0x7f35f40ecfa0>, <__main__.Case object at 0x7f35f40ede40>, <__main__.Case object at 0x7f35f40edde0>, <__main__.Case object at 0x7f35f40ecac0>, <__main__.Case object at 0x7f35f40ed660>, <__main__.Case object at 0x7f35f40ec250>, <__main__.Case object at 0x7f35f40ed420>, <__main__.Case object at 0x7f35f40edc00>, <__main__.Case object at 0x7f35f40edc90>, <__main__.Case object at 0x7f35f40ed120>, <__main__.Case object at 0x7f35f40eec50>, <__main__.Case object at 0x7f35f40ec400>, <__main__.Case object at 0x7f35f40efa90>, <__main__.Case object at 0x7f35f40ee260>, <__main__.Case object at 0x7f35f40ee140>, <__main__.Case object at 0x7f35f40ec0d0>, <__main__.Case object at 0x7f35f40ee590>, <__main__.Case object at 0x7f35f40eff10>, <__main__.Case object at 0x7f35f40aeef0>, <__main__.Case object at 0x7f35f40ac1c0>, <__main__.Case object at 0x7f35f40adfc0>, <__main__.Case object at 0x7f35f40ad4b0>, <__main__.Case object at 0x7f35f40ad330>, <__main__.Case object at 0x7f35f40acc70>, <__main__.Case object at 0x7f35f40af2e0>, <__main__.Case object at 0x7f35f40ad4e0>, <__main__.Case object at 0x7f35f40ac8e0>, <__main__.Case object at 0x7f35f40ad3f0>, <__main__.Case object at 0x7f35f40ae470>, <__main__.Case object at 0x7f35f40ac340>, <__main__.Case object at 0x7f35f40afdc0>, <__main__.Case object at 0x7f35f40ad0c0>, <__main__.Case object at 0x7f35f40ac070>, <__main__.Case object at 0x7f35f40afee0>, <__main__.Case object at 0x7f35f40aeb00>, <__main__.Case object at 0x7f35f40ae9e0>, <__main__.Case object at 0x7f35f40af5b0>, <__main__.Case object at 0x7f35f40ac610>, <__main__.Case object at 0x7f35f40aece0>, <__main__.Case object at 0x7f35f40af160>, <__main__.Case object at 0x7f35f40ae650>, <__main__.Case object at 0x7f35f40ace80>, <__main__.Case object at 0x7f35f40af7c0>, <__main__.Case object at 0x7f35f40af9d0>, <__main__.Case object at 0x7f35f40afcd0>, <__main__.Case object at 0x7f35f40ae230>, <__main__.Case object at 0x7f35f40aded0>, <__main__.Case object at 0x7f35f40affd0>, <__main__.Case object at 0x7f35f40ad900>, <__main__.Case object at 0x7f35f40ae890>, <__main__.Case object at 0x7f35f40af250>, <__main__.Case object at 0x7f35f40ad930>, <__main__.Case object at 0x7f35f40ac730>, <__main__.Case object at 0x7f35f40aeb60>, <__main__.Case object at 0x7f35f40af760>, <__main__.Case object at 0x7f35f40d0070>, <__main__.Case object at 0x7f35f40d0190>, <__main__.Case object at 0x7f35f40d02b0>, <__main__.Case object at 0x7f35f40d03d0>, <__main__.Case object at 0x7f35f40d0430>, <__main__.Case object at 0x7f35f40d0610>, <__main__.Case object at 0x7f35f40d0730>, <__main__.Case object at 0x7f35f40d0850>, <__main__.Case object at 0x7f35f40d08b0>, <__main__.Case object at 0x7f35f40d0a00>, <__main__.Case object at 0x7f35f40d0a90>, <__main__.Case object at 0x7f35f40d0bb0>, <__main__.Case object at 0x7f35f40d0b50>, <__main__.Case object at 0x7f35f40d0e50>, <__main__.Case object at 0x7f35f40d0f70>, <__main__.Case object at 0x7f35f40d1090>, <__main__.Case object at 0x7f35f40ac9d0>, <__main__.Case object at 0x7f35f40d10f0>, <__main__.Case object at 0x7f35f40d1390>, <__main__.Case object at 0x7f35f40d14b0>, <__main__.Case object at 0x7f35f40d1510>, <__main__.Case object at 0x7f35f40d16f0>, <__main__.Case object at 0x7f35f40d1810>, <__main__.Case object at 0x7f35f40d1930>, <__main__.Case object at 0x7f35f40d1b40>, <__main__.Case object at 0x7f35f40d1ba0>, <__main__.Case object at 0x7f35f40d1cc0>, <__main__.Case object at 0x7f35f40d1de0>, <__main__.Case object at 0x7f35f40d1fc0>, <__main__.Case object at 0x7f35f40d2020>, <__main__.Case object at 0x7f35f40d2140>, <__main__.Case object at 0x7f35f40d2260>, <__main__.Case object at 0x7f35f40d2440>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f40f65f0>, <__main__.Case object at 0x7f35f40f7c40>, <__main__.Case object at 0x7f35f40f7bb0>, <__main__.Case object at 0x7f35f40f4a60>, <__main__.Case object at 0x7f35f40f4490>, <__main__.Case object at 0x7f35f40f49d0>, <__main__.Case object at 0x7f35f40f4f40>, <__main__.Case object at 0x7f35f40f60e0>, <__main__.Case object at 0x7f35f40f6a40>, <__main__.Case object at 0x7f35f40f74c0>, <__main__.Case object at 0x7f35f40f6980>, <__main__.Case object at 0x7f35f40f7340>, <__main__.Case object at 0x7f35f40f66b0>, <__main__.Case object at 0x7f35f40f6ec0>, <__main__.Case object at 0x7f35f40f7070>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f40f59c0>, <__main__.Case object at 0x7f35f40f4940>, <__main__.Case object at 0x7f35f40f7490>, <__main__.Case object at 0x7f35f40f7ac0>, <__main__.Case object at 0x7f35f40f6290>, <__main__.Case object at 0x7f35f40f7580>, <__main__.Case object at 0x7f35f40f7df0>, <__main__.Case object at 0x7f35f40f4d60>, <__main__.Case object at 0x7f35f40f5ea0>, <__main__.Case object at 0x7f35f40f48e0>, <__main__.Case object at 0x7f35f40f6da0>, <__main__.Case object at 0x7f35f40f7640>, <__main__.Case object at 0x7f35f40f4b80>, <__main__.Case object at 0x7f35f40f7760>, <__main__.Case object at 0x7f35f40f7910>, <__main__.Case object at 0x7f35f40f4c40>, <__main__.Case object at 0x7f35f40f6590>, <__main__.Case object at 0x7f35f40f4340>, <__main__.Case object at 0x7f35f40f4190>, <__main__.Case object at 0x7f35f40f7550>, <__main__.Case object at 0x7f35f40f4370>, <__main__.Case object at 0x7f35f40f6170>, <__main__.Case object at 0x7f35f40f7be0>, <__main__.Case object at 0x7f35f40f4f10>, <__main__.Case object at 0x7f35f40be020>, <__main__.Case object at 0x7f35f40bcf70>, <__main__.Case object at 0x7f35f40bf850>, <__main__.Case object at 0x7f35f40f6f20>, <__main__.Case object at 0x7f35f40bec80>, <__main__.Case object at 0x7f35f40bc970>, <__main__.Case object at 0x7f35f40bd9c0>, <__main__.Case object at 0x7f35f40f7ee0>, <__main__.Case object at 0x7f35f40f6a10>, <__main__.Case object at 0x7f35f40bf700>, <__main__.Case object at 0x7f35f40f5210>, <__main__.Case object at 0x7f35f40bfc70>, <__main__.Case object at 0x7f35f40bfc10>, <__main__.Case object at 0x7f35f40bdc60>, <__main__.Case object at 0x7f35f40f6560>, <__main__.Case object at 0x7f35f40bf2e0>, <__main__.Case object at 0x7f35f40bcf10>, <__main__.Case object at 0x7f35f40bf130>, <__main__.Case object at 0x7f35f40bc5e0>, <__main__.Case object at 0x7f35f40bd150>, <__main__.Case object at 0x7f35f40be6b0>, <__main__.Case object at 0x7f35f40bf3a0>, <__main__.Case object at 0x7f35f40bc6d0>, <__main__.Case object at 0x7f35f40bd720>, <__main__.Case object at 0x7f35f40bf760>, <__main__.Case object at 0x7f35f40bd540>, <__main__.Case object at 0x7f35f40bf880>, <__main__.Case object at 0x7f35f40bcdc0>, <__main__.Case object at 0x7f35f40bf970>, <__main__.Case object at 0x7f35f40bed70>, <__main__.Case object at 0x7f35f40bd7e0>, <__main__.Case object at 0x7f35f40bf7f0>, <__main__.Case object at 0x7f35f40be530>, <__main__.Case object at 0x7f35f40bc0a0>, <__main__.Case object at 0x7f35f40bffa0>, <__main__.Case object at 0x7f35f40be320>, <__main__.Case object at 0x7f35f40be800>, <__main__.Case object at 0x7f35f40bcca0>, <__main__.Case object at 0x7f35f40bfd00>, <__main__.Case object at 0x7f35f40bd7b0>, <__main__.Case object at 0x7f35f40bf730>, <__main__.Case object at 0x7f35f40bfac0>, <__main__.Case object at 0x7f35f40bf550>, <__main__.Case object at 0x7f35f40bdf00>, <__main__.Case object at 0x7f35f40bc520>, <__main__.Case object at 0x7f35f40bcb50>, <__main__.Case object at 0x7f35f40bf6d0>, <__main__.Case object at 0x7f35f40be5c0>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f40da4a0>, <__main__.Case object at 0x7f35f40d85e0>, <__main__.Case object at 0x7f35f40beec0>, <__main__.Case object at 0x7f35f40da560>, <__main__.Case object at 0x7f35f40db0d0>, <__main__.Case object at 0x7f35f40dbb50>, <__main__.Case object at 0x7f35f40bd840>, <__main__.Case object at 0x7f35f40d9600>, <__main__.Case object at 0x7f35f40da050>, <__main__.Case object at 0x7f35f40da9b0>, <__main__.Case object at 0x7f35f40be890>, <__main__.Case object at 0x7f35f40d8100>, <__main__.Case object at 0x7f35f40d8a90>, <__main__.Case object at 0x7f35f40d9660>, <__main__.Case object at 0x7f35f40bf9d0>, <__main__.Case object at 0x7f35f40daaa0>, <__main__.Case object at 0x7f35f40daf50>, <__main__.Case object at 0x7f35f40d8a60>, <__main__.Case object at 0x7f35f40d9cf0>, <__main__.Case object at 0x7f35f40da200>, <__main__.Case object at 0x7f35f40daa70>, <__main__.Case object at 0x7f35f40db6d0>, <__main__.Case object at 0x7f35f40d8ac0>, <__main__.Case object at 0x7f35f40d8fd0>, <__main__.Case object at 0x7f35f40d9840>, <__main__.Case object at 0x7f35f40da260>, <__main__.Case object at 0x7f35f40db220>, <__main__.Case object at 0x7f35f40db730>, <__main__.Case object at 0x7f35f40dbc40>, <__main__.Case object at 0x7f35f40d8b20>, <__main__.Case object at 0x7f35f40d9c00>, <__main__.Case object at 0x7f35f40da020>, <__main__.Case object at 0x7f35f40bc250>, <__main__.Case object at 0x7f35f40d8820>, <__main__.Case object at 0x7f35f40d8ee0>, <__main__.Case object at 0x7f35f40d9990>, <__main__.Case object at 0x7f35f40db2e0>, <__main__.Case object at 0x7f35f40dbc70>, <__main__.Case object at 0x7f35f40d89d0>, <__main__.Case object at 0x7f35f40d9a80>, <__main__.Case object at 0x7f35f40da1d0>, <__main__.Case object at 0x7f35f40da4d0>, <__main__.Case object at 0x7f35f40db130>, <__main__.Case object at 0x7f35f40d8190>, <__main__.Case object at 0x7f35f40d89a0>, <__main__.Case object at 0x7f35f40da740>, <__main__.Case object at 0x7f35f40d91b0>, <__main__.Case object at 0x7f35f40dada0>, <__main__.Case object at 0x7f35f40dbfa0>, <__main__.Case object at 0x7f35f40dad40>, <__main__.Case object at 0x7f35f40db790>, <__main__.Case object at 0x7f35f40d9420>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f4067a30>, <__main__.Case object at 0x7f35f4070b80>, <__main__.Case object at 0x7f35f409be20>, <__main__.Case object at 0x7f35f409b670>, <__main__.Case object at 0x7f35f40d9e70>, <__main__.Case object at 0x7f35f409ad40>, <__main__.Case object at 0x7f35f409b580>, <__main__.Case object at 0x7f35f409b550>, <__main__.Case object at 0x7f35f40d8610>, <__main__.Case object at 0x7f35f40dab30>, <__main__.Case object at 0x7f35f409baf0>, <__main__.Case object at 0x7f35f40999c0>, <__main__.Case object at 0x7f35f409b7c0>, <__main__.Case object at 0x7f35f40db820>, <__main__.Case object at 0x7f35f40ef640>, <__main__.Case object at 0x7f35f40ee020>, <__main__.Case object at 0x7f35f40ed9c0>, <__main__.Case object at 0x7f35f40db5e0>, <__main__.Case object at 0x7f35f40ed6f0>, <__main__.Case object at 0x7f35f40ec1f0>, <__main__.Case object at 0x7f35f40eefe0>, <__main__.Case object at 0x7f35f409b520>, <__main__.Case object at 0x7f35f40eed40>, <__main__.Case object at 0x7f35f40ed570>, <__main__.Case object at 0x7f35f40ef3d0>, <__main__.Case object at 0x7f35f409bf10>, <__main__.Case object at 0x7f35f409b4c0>, <__main__.Case object at 0x7f35f40eed10>, <__main__.Case object at 0x7f35f409bfa0>, <__main__.Case object at 0x7f35f40edae0>, <__main__.Case object at 0x7f35f40efee0>, <__main__.Case object at 0x7f35f40ecbe0>, <__main__.Case object at 0x7f35f40ed930>, <__main__.Case object at 0x7f35f40efe50>, <__main__.Case object at 0x7f35f40ed1b0>, <__main__.Case object at 0x7f35f40ee2c0>, <__main__.Case object at 0x7f35f40ec640>, <__main__.Case object at 0x7f35f40ee740>, <__main__.Case object at 0x7f35f40eee60>, <__main__.Case object at 0x7f35f40ecca0>, <__main__.Case object at 0x7f35f40ed900>, <__main__.Case object at 0x7f35f40ee620>, <__main__.Case object at 0x7f35f40ef2e0>, <__main__.Case object at 0x7f35f40ed330>, <__main__.Case object at 0x7f35f40ee0b0>, <__main__.Case object at 0x7f35f40ee1a0>, <__main__.Case object at 0x7f35f40ecc70>, <__main__.Case object at 0x7f35f40ec100>, <__main__.Case object at 0x7f35f40efb80>, <__main__.Case object at 0x7f35f40ee500>, <__main__.Case object at 0x7f35f40ee380>, <__main__.Case object at 0x7f35f40efeb0>, <__main__.Case object at 0x7f35f40ec2e0>, <__main__.Case object at 0x7f35f40edd20>, <__main__.Case object at 0x7f35f40ee170>, <__main__.Case object at 0x7f35f40ed2a0>, <__main__.Case object at 0x7f35f40efdc0>, <__main__.Case object at 0x7f35f40ecaf0>, <__main__.Case object at 0x7f35f40ed990>, <__main__.Case object at 0x7f35f40ef5e0>, <__main__.Case object at 0x7f35f40ee6b0>, <__main__.Case object at 0x7f35f40efca0>, <__main__.Case object at 0x7f35f40ed030>, <__main__.Case object at 0x7f35f40ee9e0>, <__main__.Case object at 0x7f35f40db250>, <__main__.Case object at 0x7f35f40af370>, <__main__.Case object at 0x7f35f40acd60>, <__main__.Case object at 0x7f35f40ec340>, <__main__.Case object at 0x7f35f40aff70>, <__main__.Case object at 0x7f35f40adc30>, <__main__.Case object at 0x7f35f40ade10>, <__main__.Case object at 0x7f35f40ed5d0>, <__main__.Case object at 0x7f35f40ad750>, <__main__.Case object at 0x7f35f40acdf0>, <__main__.Case object at 0x7f35f40ae590>, <__main__.Case object at 0x7f35f40ee890>, <__main__.Case object at 0x7f35f40ac880>, <__main__.Case object at 0x7f35f40af670>, <__main__.Case object at 0x7f35f40ec280>, <__main__.Case object at 0x7f35f40af700>, <__main__.Case object at 0x7f35f40aebc0>, <__main__.Case object at 0x7f35f40ac670>, <__main__.Case object at 0x7f35f40adcc0>, <__main__.Case object at 0x7f35f40ae1a0>, <__main__.Case object at 0x7f35f40afb50>, <__main__.Case object at 0x7f35f40af7f0>, <__main__.Case object at 0x7f35f40ad660>, <__main__.Case object at 0x7f35f40ac5b0>, <__main__.Case object at 0x7f35f40ac250>, <__main__.Case object at 0x7f35f40ae680>, <__main__.Case object at 0x7f35f40ac1f0>, <__main__.Case object at 0x7f35f40af790>, <__main__.Case object at 0x7f35f40af970>, <__main__.Case object at 0x7f35f40ac280>, <__main__.Case object at 0x7f35f40aff40>, <__main__.Case object at 0x7f35f40acf10>, <__main__.Case object at 0x7f35f40ad450>, <__main__.Case object at 0x7f35f40afd30>, <__main__.Case object at 0x7f35f40ae560>, <__main__.Case object at 0x7f35f40edab0>, <__main__.Case object at 0x7f35f40d01f0>, <__main__.Case object at 0x7f35f40d0310>, <__main__.Case object at 0x7f35f40ad180>, <__main__.Case object at 0x7f35f40d0580>, <__main__.Case object at 0x7f35f40d0670>, <__main__.Case object at 0x7f35f40d0790>, <__main__.Case object at 0x7f35f40ada50>, <__main__.Case object at 0x7f35f40ad7e0>, <__main__.Case object at 0x7f35f40afbe0>, <__main__.Case object at 0x7f35f40d0dc0>, <__main__.Case object at 0x7f35f40d0eb0>, <__main__.Case object at 0x7f35f40d0fd0>, <__main__.Case object at 0x7f35f40d0d30>, <__main__.Case object at 0x7f35f40d12d0>, <__main__.Case object at 0x7f35f40d13f0>, <__main__.Case object at 0x7f35f40ae1d0>, <__main__.Case object at 0x7f35f40d1660>, <__main__.Case object at 0x7f35f40d1750>, <__main__.Case object at 0x7f35f40d1870>, <__main__.Case object at 0x7f35f40d1990>, <__main__.Case object at 0x7f35f40d0520>, <__main__.Case object at 0x7f35f40d1c00>, <__main__.Case object at 0x7f35f40d1d20>, <__main__.Case object at 0x7f35f40d1b10>, <__main__.Case object at 0x7f35f40d09a0>, <__main__.Case object at 0x7f35f40d2080>, <__main__.Case object at 0x7f35f40d21a0>, <__main__.Case object at 0x7f35f40d1f90>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1.0)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40f6140>, <__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f40f4040>, <__main__.Case object at 0x7f35f40f5d80>, <__main__.Case object at 0x7f35f40f6ef0>, <__main__.Case object at 0x7f35f40f6380>, <__main__.Case object at 0x7f35f40f6d10>, <__main__.Case object at 0x7f35f40f7cd0>, <__main__.Case object at 0x7f35f40f66e0>, <__main__.Case object at 0x7f35f40f49a0>, <__main__.Case object at 0x7f35f40f7520>, <__main__.Case object at 0x7f35f40f5d50>, <__main__.Case object at 0x7f35f40f7a30>, <__main__.Case object at 0x7f35f40f43d0>, <__main__.Case object at 0x7f35f40f4730>, <__main__.Case object at 0x7f35f40f6680>, <__main__.Case object at 0x7f35f40f5f30>, <__main__.Case object at 0x7f35f40f5360>, <__main__.Case object at 0x7f35f40f7190>, <__main__.Case object at 0x7f35f40f7d00>, <__main__.Case object at 0x7f35f40f6c80>, <__main__.Case object at 0x7f35f40f77f0>, <__main__.Case object at 0x7f35f40f4910>, <__main__.Case object at 0x7f35f40f5de0>, <__main__.Case object at 0x7f35f40f6830>, <__main__.Case object at 0x7f35f40f43a0>, <__main__.Case object at 0x7f35f40f4cd0>, <__main__.Case object at 0x7f35f40f5f90>, <__main__.Case object at 0x7f35f40f5180>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f40f48b0>, <__main__.Case object at 0x7f35f40f4dc0>, <__main__.Case object at 0x7f35f40f6110>, <__main__.Case object at 0x7f35f40f7f10>, <__main__.Case object at 0x7f35f40f4850>, <__main__.Case object at 0x7f35f40f7430>, <__main__.Case object at 0x7f35f40f4d90>, <__main__.Case object at 0x7f35f40f6260>, <__main__.Case object at 0x7f35f40f7880>, <__main__.Case object at 0x7f35f40f71c0>, <__main__.Case object at 0x7f35f40f7820>, <__main__.Case object at 0x7f35f40f7c10>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40f5db0>, <__main__.Case object at 0x7f35f40f4520>, <__main__.Case object at 0x7f35f40f4eb0>, <__main__.Case object at 0x7f35f40bd780>, <__main__.Case object at 0x7f35f40bf0a0>, <__main__.Case object at 0x7f35f40bfbe0>, <__main__.Case object at 0x7f35f40bef50>, <__main__.Case object at 0x7f35f40bfd30>, <__main__.Case object at 0x7f35f40bcd90>, <__main__.Case object at 0x7f35f40bf6a0>, <__main__.Case object at 0x7f35f40bfa30>, <__main__.Case object at 0x7f35f40be9e0>, <__main__.Case object at 0x7f35f40f6320>, <__main__.Case object at 0x7f35f40bd3c0>, <__main__.Case object at 0x7f35f40bc1c0>, <__main__.Case object at 0x7f35f40bc100>, <__main__.Case object at 0x7f35f40bc190>, <__main__.Case object at 0x7f35f40bc160>, <__main__.Case object at 0x7f35f40bf010>, <__main__.Case object at 0x7f35f40bc430>, <__main__.Case object at 0x7f35f40bf1c0>, <__main__.Case object at 0x7f35f40bf5e0>, <__main__.Case object at 0x7f35f40bc9a0>, <__main__.Case object at 0x7f35f40f7610>, <__main__.Case object at 0x7f35f40bf9a0>, <__main__.Case object at 0x7f35f40bf070>, <__main__.Case object at 0x7f35f40bfe80>, <__main__.Case object at 0x7f35f40bf340>, <__main__.Case object at 0x7f35f40be0e0>, <__main__.Case object at 0x7f35f40bc3d0>, <__main__.Case object at 0x7f35f40be2c0>, <__main__.Case object at 0x7f35f40be560>, <__main__.Case object at 0x7f35f40bd1e0>, <__main__.Case object at 0x7f35f40be110>, <__main__.Case object at 0x7f35f40bf3d0>, <__main__.Case object at 0x7f35f40bf8b0>, <__main__.Case object at 0x7f35f40bdd80>, <__main__.Case object at 0x7f35f40bf910>, <__main__.Case object at 0x7f35f40bf160>, <__main__.Case object at 0x7f35f40bc040>, <__main__.Case object at 0x7f35f40bc820>, <__main__.Case object at 0x7f35f40bd870>, <__main__.Case object at 0x7f35f40bfcd0>, <__main__.Case object at 0x7f35f40bebc0>, <__main__.Case object at 0x7f35f40be740>, <__main__.Case object at 0x7f35f40bd0c0>, <__main__.Case object at 0x7f35f40be410>, <__main__.Case object at 0x7f35f40be140>, <__main__.Case object at 0x7f35f40bce50>, <__main__.Case object at 0x7f35f40bde70>, <__main__.Case object at 0x7f35f40be920>, <__main__.Case object at 0x7f35f40bd3f0>, <__main__.Case object at 0x7f35f40be680>, <__main__.Case object at 0x7f35f40bf430>, <__main__.Case object at 0x7f35f40bc0d0>, <__main__.Case object at 0x7f35f40be260>, <__main__.Case object at 0x7f35f40bdcf0>, <__main__.Case object at 0x7f35f40f6bc0>, <__main__.Case object at 0x7f35f40be230>, <__main__.Case object at 0x7f35f40bfa90>, <__main__.Case object at 0x7f35f40f4820>, <__main__.Case object at 0x7f35f40db3a0>, <__main__.Case object at 0x7f35f40d8730>, <__main__.Case object at 0x7f35f40f70a0>, <__main__.Case object at 0x7f35f40bdea0>, <__main__.Case object at 0x7f35f40f4970>, <__main__.Case object at 0x7f35f40d9ae0>, <__main__.Case object at 0x7f35f40db550>, <__main__.Case object at 0x7f35f40d8550>, <__main__.Case object at 0x7f35f40d8eb0>, <__main__.Case object at 0x7f35f40d9e10>, <__main__.Case object at 0x7f35f40da2f0>, <__main__.Case object at 0x7f35f40bf2b0>, <__main__.Case object at 0x7f35f40db610>, <__main__.Case object at 0x7f35f40d92d0>, <__main__.Case object at 0x7f35f40d8e50>, <__main__.Case object at 0x7f35f40bc310>, <__main__.Case object at 0x7f35f40db430>, <__main__.Case object at 0x7f35f40d8ca0>, <__main__.Case object at 0x7f35f40dbe50>, <__main__.Case object at 0x7f35f40d9b10>, <__main__.Case object at 0x7f35f40dad70>, <__main__.Case object at 0x7f35f40d8070>, <__main__.Case object at 0x7f35f40db070>, <__main__.Case object at 0x7f35f40d9fc0>, <__main__.Case object at 0x7f35f40d8760>, <__main__.Case object at 0x7f35f40da590>, <__main__.Case object at 0x7f35f40d9bd0>, <__main__.Case object at 0x7f35f40da5c0>, <__main__.Case object at 0x7f35f40d9480>, <__main__.Case object at 0x7f35f40daec0>, <__main__.Case object at 0x7f35f40d9810>, <__main__.Case object at 0x7f35f40d8e80>, <__main__.Case object at 0x7f35f40d98a0>, <__main__.Case object at 0x7f35f40d9330>, <__main__.Case object at 0x7f35f40dace0>, <__main__.Case object at 0x7f35f40db1f0>, <__main__.Case object at 0x7f35f40d84f0>, <__main__.Case object at 0x7f35f40d8970>, <__main__.Case object at 0x7f35f40da3e0>, <__main__.Case object at 0x7f35f40da680>, <__main__.Case object at 0x7f35f40daf80>, <__main__.Case object at 0x7f35f40db9d0>, <__main__.Case object at 0x7f35f40db910>, <__main__.Case object at 0x7f35f40d8df0>, <__main__.Case object at 0x7f35f40da0e0>, <__main__.Case object at 0x7f35f40d8460>, <__main__.Case object at 0x7f35f40da800>, <__main__.Case object at 0x7f35f40dab60>, <__main__.Case object at 0x7f35f40dbd60>, <__main__.Case object at 0x7f35f40d8c70>, <__main__.Case object at 0x7f35f40d9de0>, <__main__.Case object at 0x7f35f40db6a0>, <__main__.Case object at 0x7f35f40d9e40>, <__main__.Case object at 0x7f35f40d9c60>, <__main__.Case object at 0x7f35f40dbf10>, <__main__.Case object at 0x7f35f40dbdc0>, <__main__.Case object at 0x7f35f40db8b0>, <__main__.Case object at 0x7f35f40dbb80>, <__main__.Case object at 0x7f35f407eaa0>, <__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f40d82e0>, <__main__.Case object at 0x7f35f40d8d00>, <__main__.Case object at 0x7f35f40d9750>, <__main__.Case object at 0x7f35f40d9ea0>, <__main__.Case object at 0x7f35f40db3d0>, <__main__.Case object at 0x7f35f409bc70>, <__main__.Case object at 0x7f35f409bca0>, <__main__.Case object at 0x7f35f409bf70>, <__main__.Case object at 0x7f35f409ac20>, <__main__.Case object at 0x7f35f409b040>, <__main__.Case object at 0x7f35f409b1f0>, <__main__.Case object at 0x7f35f409bfd0>, <__main__.Case object at 0x7f35f4070d60>, <__main__.Case object at 0x7f35f409ad10>, <__main__.Case object at 0x7f35f409b280>, <__main__.Case object at 0x7f35f409b370>, <__main__.Case object at 0x7f35f40ef790>, <__main__.Case object at 0x7f35f40ec460>, <__main__.Case object at 0x7f35f409afe0>, <__main__.Case object at 0x7f35f40ec070>, <__main__.Case object at 0x7f35f40ece20>, <__main__.Case object at 0x7f35f40ecd90>, <__main__.Case object at 0x7f35f40ef0d0>, <__main__.Case object at 0x7f35f40ec550>, <__main__.Case object at 0x7f35f40ef9a0>, <__main__.Case object at 0x7f35f40bebf0>, <__main__.Case object at 0x7f35f40ef100>, <__main__.Case object at 0x7f35f40ecc10>, <__main__.Case object at 0x7f35f40ec670>, <__main__.Case object at 0x7f35f40ee860>, <__main__.Case object at 0x7f35f40ee440>, <__main__.Case object at 0x7f35f40efdf0>, <__main__.Case object at 0x7f35f40eead0>, <__main__.Case object at 0x7f35f40ed870>, <__main__.Case object at 0x7f35f40ec130>, <__main__.Case object at 0x7f35f40ef7f0>, <__main__.Case object at 0x7f35f40edea0>, <__main__.Case object at 0x7f35f40ef6a0>, <__main__.Case object at 0x7f35f40ee650>, <__main__.Case object at 0x7f35f40ecd60>, <__main__.Case object at 0x7f35f40ee4a0>, <__main__.Case object at 0x7f35f40efa60>, <__main__.Case object at 0x7f35f40eec80>, <__main__.Case object at 0x7f35f40ef4f0>, <__main__.Case object at 0x7f35f40ef130>, <__main__.Case object at 0x7f35f40eda20>, <__main__.Case object at 0x7f35f40edd80>, <__main__.Case object at 0x7f35f40ee050>, <__main__.Case object at 0x7f35f40ec700>, <__main__.Case object at 0x7f35f40eec20>, <__main__.Case object at 0x7f35f40ed7e0>, <__main__.Case object at 0x7f35f40ed960>, <__main__.Case object at 0x7f35f40ee770>, <__main__.Case object at 0x7f35f40ef3a0>, <__main__.Case object at 0x7f35f40ef400>, <__main__.Case object at 0x7f35f40eddb0>, <__main__.Case object at 0x7f35f40ed060>, <__main__.Case object at 0x7f35f40eca00>, <__main__.Case object at 0x7f35f40ed390>, <__main__.Case object at 0x7f35f40ef4c0>, <__main__.Case object at 0x7f35f40ec310>, <__main__.Case object at 0x7f35f40ecee0>, <__main__.Case object at 0x7f35f40ef430>, <__main__.Case object at 0x7f35f40ec760>, <__main__.Case object at 0x7f35f40ef0a0>, <__main__.Case object at 0x7f35f40ef910>, <__main__.Case object at 0x7f35f40ee350>, <__main__.Case object at 0x7f35f40ad690>, <__main__.Case object at 0x7f35f40eff70>, <__main__.Case object at 0x7f35f409b910>, <__main__.Case object at 0x7f35f409b3a0>, <__main__.Case object at 0x7f35f409b010>, <__main__.Case object at 0x7f35f40ac430>, <__main__.Case object at 0x7f35f40edf30>, <__main__.Case object at 0x7f35f40ad540>, <__main__.Case object at 0x7f35f40afd90>, <__main__.Case object at 0x7f35f40adab0>, <__main__.Case object at 0x7f35f40ae9b0>, <__main__.Case object at 0x7f35f40ac5e0>, <__main__.Case object at 0x7f35f40aeb30>, <__main__.Case object at 0x7f35f40adb70>, <__main__.Case object at 0x7f35f40ae830>, <__main__.Case object at 0x7f35f40ac940>, <__main__.Case object at 0x7f35f40acb80>, <__main__.Case object at 0x7f35f40adc60>, <__main__.Case object at 0x7f35f40ad6f0>, <__main__.Case object at 0x7f35f40ae3e0>, <__main__.Case object at 0x7f35f40afc40>, <__main__.Case object at 0x7f35f40acbb0>, <__main__.Case object at 0x7f35f40af3a0>, <__main__.Case object at 0x7f35f40ad7b0>, <__main__.Case object at 0x7f35f40ae290>, <__main__.Case object at 0x7f35f40af6a0>, <__main__.Case object at 0x7f35f40ae710>, <__main__.Case object at 0x7f35f40aea10>, <__main__.Case object at 0x7f35f40ac2e0>, <__main__.Case object at 0x7f35f40af340>, <__main__.Case object at 0x7f35f40aead0>, <__main__.Case object at 0x7f35f40ada20>, <__main__.Case object at 0x7f35f40ae320>, <__main__.Case object at 0x7f35f40affa0>, <__main__.Case object at 0x7f35f40ae080>, <__main__.Case object at 0x7f35f40aee00>, <__main__.Case object at 0x7f35f40af910>, <__main__.Case object at 0x7f35f40ac8b0>, <__main__.Case object at 0x7f35f40d0130>, <__main__.Case object at 0x7f35f40aec20>, <__main__.Case object at 0x7f35f40d0370>, <__main__.Case object at 0x7f35f40aed70>, <__main__.Case object at 0x7f35f40ace20>, <__main__.Case object at 0x7f35f40ad9c0>, <__main__.Case object at 0x7f35f40ac2b0>, <__main__.Case object at 0x7f35f40d0910>, <__main__.Case object at 0x7f35f40d04f0>, <__main__.Case object at 0x7f35f40d0af0>, <__main__.Case object at 0x7f35f40d0c10>, <__main__.Case object at 0x7f35f40d0cd0>, <__main__.Case object at 0x7f35f40d0970>, <__main__.Case object at 0x7f35f40d0250>, <__main__.Case object at 0x7f35f40d1030>, <__main__.Case object at 0x7f35f40d1150>, <__main__.Case object at 0x7f35f40d1240>, <__main__.Case object at 0x7f35f40d1330>, <__main__.Case object at 0x7f35f40d0490>, <__main__.Case object at 0x7f35f40d1450>, <__main__.Case object at 0x7f35f40d11b0>, <__main__.Case object at 0x7f35f40d17b0>, <__main__.Case object at 0x7f35f40d1570>, <__main__.Case object at 0x7f35f40d19f0>, <__main__.Case object at 0x7f35f40d00d0>, <__main__.Case object at 0x7f35f40d06d0>, <__main__.Case object at 0x7f35f40d07f0>, <__main__.Case object at 0x7f35f40d0f10>, <__main__.Case object at 0x7f35f40d1a80>, <__main__.Case object at 0x7f35f40aceb0>, <__main__.Case object at 0x7f35f40d2200>, <__main__.Case object at 0x7f35f40db700>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 5\n",
      "Episode: 21, Total Steps: 301, Total Rewards: [-400, 87], Status Episode: False\n",
      "------------------------------------------End of episode 21 loop--------------------\n",
      "----- starting point of Episode 22 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 4 to next state (9, 0): pull reward: -0.1\n",
      "----- starting point of Episode 22 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 3) with action 0 to next state (7, 3): pull reward: 0.0\n",
      "----- starting point of Episode 22 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 3) with action 1 to next state (7, 2): pull reward: 0.1\n",
      "----- starting point of Episode 22 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 22 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 2 to next state (0, 5): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 3 to next state (0, 5): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 0 to next state (0, 5): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 3 to next state (0, 5): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 2 to next state (0, 6): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 6) with action 1 to next state (0, 5): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 1 to next state (0, 4): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 4 to next state (1, 4): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 3 to next state (0, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 2 to next state (0, 5): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 4 to next state (1, 5): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f40f6350>, <__main__.Case object at 0x7f35f40f5ff0>, <__main__.Case object at 0x7f35f40f40d0>, <__main__.Case object at 0x7f35f40f74f0>, <__main__.Case object at 0x7f35f40f4940>, <__main__.Case object at 0x7f35f40f7490>, <__main__.Case object at 0x7f35f40f4d60>, <__main__.Case object at 0x7f35f40f6290>, <__main__.Case object at 0x7f35f40f7b20>, <__main__.Case object at 0x7f35f40f70d0>, <__main__.Case object at 0x7f35f40f4190>, <__main__.Case object at 0x7f35f40f6f20>, <__main__.Case object at 0x7f35f40f7a90>, <__main__.Case object at 0x7f35f40f7010>, <__main__.Case object at 0x7f35f40f6620>, <__main__.Case object at 0x7f35f40f6800>, <__main__.Case object at 0x7f35f40f7d90>, <__main__.Case object at 0x7f35f40f52d0>, <__main__.Case object at 0x7f35f40f6530>, <__main__.Case object at 0x7f35f40f75b0>, <__main__.Case object at 0x7f35f40f6f50>, <__main__.Case object at 0x7f35f40f4c70>, <__main__.Case object at 0x7f35f40f52a0>, <__main__.Case object at 0x7f35f40f45b0>, <__main__.Case object at 0x7f35f40f72e0>, <__main__.Case object at 0x7f35f40f73d0>, <__main__.Case object at 0x7f35f40f6950>, <__main__.Case object at 0x7f35f40f5960>, <__main__.Case object at 0x7f35f40f6ef0>, <__main__.Case object at 0x7f35f40f66e0>, <__main__.Case object at 0x7f35f40f49a0>, <__main__.Case object at 0x7f35f40f4730>, <__main__.Case object at 0x7f35f40f5360>, <__main__.Case object at 0x7f35f40f68c0>, <__main__.Case object at 0x7f35f40f5300>, <__main__.Case object at 0x7f35f40f4880>, <__main__.Case object at 0x7f35f40f4280>, <__main__.Case object at 0x7f35f40f7850>, <__main__.Case object at 0x7f35f40f4580>, <__main__.Case object at 0x7f35f40f71c0>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40f4eb0>, <__main__.Case object at 0x7f35f40f4fd0>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f408a6e0>, <__main__.Case object at 0x7f35f40da4a0>, <__main__.Case object at 0x7f35f40d85e0>, <__main__.Case object at 0x7f35f40da050>, <__main__.Case object at 0x7f35f40d8850>, <__main__.Case object at 0x7f35f40daaa0>, <__main__.Case object at 0x7f35f40d9360>, <__main__.Case object at 0x7f35f40d8250>, <__main__.Case object at 0x7f35f40da140>, <__main__.Case object at 0x7f35f40dbcd0>, <__main__.Case object at 0x7f35f40d92a0>, <__main__.Case object at 0x7f35f40d8ee0>, <__main__.Case object at 0x7f35f40dbc70>, <__main__.Case object at 0x7f35f40da1d0>, <__main__.Case object at 0x7f35f40db130>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f40920b0>, <__main__.Case object at 0x7f35f40f44f0>, <__main__.Case object at 0x7f35f40f6980>, <__main__.Case object at 0x7f35f40f79a0>, <__main__.Case object at 0x7f35f40f7eb0>, <__main__.Case object at 0x7f35f40f6860>, <__main__.Case object at 0x7f35f40f7670>, <__main__.Case object at 0x7f35f40f4a30>, <__main__.Case object at 0x7f35f40f64a0>, <__main__.Case object at 0x7f35f40f6170>, <__main__.Case object at 0x7f35f40f6a10>, <__main__.Case object at 0x7f35f40f63e0>, <__main__.Case object at 0x7f35f40f6650>, <__main__.Case object at 0x7f35f40f4790>, <__main__.Case object at 0x7f35f40f6470>, <__main__.Case object at 0x7f35f40f4550>, <__main__.Case object at 0x7f35f40931c0>, <__main__.Case object at 0x7f35f40f67d0>, <__main__.Case object at 0x7f35f40f7040>, <__main__.Case object at 0x7f35f40f41f0>, <__main__.Case object at 0x7f35f40f76a0>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40f5ed0>, <__main__.Case object at 0x7f35f40f45e0>, <__main__.Case object at 0x7f35f40f6b60>, <__main__.Case object at 0x7f35f40f4040>, <__main__.Case object at 0x7f35f40f6d10>, <__main__.Case object at 0x7f35f40f5d50>, <__main__.Case object at 0x7f35f40f6230>, <__main__.Case object at 0x7f35f40f6680>, <__main__.Case object at 0x7f35f40f4250>, <__main__.Case object at 0x7f35f40f6c20>, <__main__.Case object at 0x7f35f40f43a0>, <__main__.Case object at 0x7f35f40f67a0>, <__main__.Case object at 0x7f35f40f6f80>, <__main__.Case object at 0x7f35f40f7250>, <__main__.Case object at 0x7f35f40f7880>, <__main__.Case object at 0x7f35f40f7820>, <__main__.Case object at 0x7f35f40f5db0>, <__main__.Case object at 0x7f35f402e800>, <__main__.Case object at 0x7f35f407eaa0>, <__main__.Case object at 0x7f35f40f70a0>, <__main__.Case object at 0x7f35f408a680>, <__main__.Case object at 0x7f35f40f46a0>, <__main__.Case object at 0x7f35f40d9600>, <__main__.Case object at 0x7f35f40d8040>, <__main__.Case object at 0x7f35f40d9d20>, <__main__.Case object at 0x7f35f40f7b80>, <__main__.Case object at 0x7f35f40dab90>, <__main__.Case object at 0x7f35f40d9630>, <__main__.Case object at 0x7f35f40db580>, <__main__.Case object at 0x7f35f40f6830>, <__main__.Case object at 0x7f35f40d84c0>, <__main__.Case object at 0x7f35f40db2e0>, <__main__.Case object at 0x7f35f40d9a80>, <__main__.Case object at 0x7f35f40f4d90>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f40f73a0>, <__main__.Case object at 0x7f35f40f65f0>, <__main__.Case object at 0x7f35f40f53c0>, <__main__.Case object at 0x7f35f40f6c50>, <__main__.Case object at 0x7f35f40f4d30>, <__main__.Case object at 0x7f35f40f66b0>, <__main__.Case object at 0x7f35f40f59c0>, <__main__.Case object at 0x7f35f40f7ac0>, <__main__.Case object at 0x7f35f40f7580>, <__main__.Case object at 0x7f35f40f6da0>, <__main__.Case object at 0x7f35f40f4100>, <__main__.Case object at 0x7f35f40f4700>, <__main__.Case object at 0x7f35f40f5f60>, <__main__.Case object at 0x7f35f40f7be0>, <__main__.Case object at 0x7f35f40f6560>, <__main__.Case object at 0x7f35f40f7160>, <__main__.Case object at 0x7f35f40f63b0>, <__main__.Case object at 0x7f35f40f61d0>, <__main__.Case object at 0x7f35f40f5e40>, <__main__.Case object at 0x7f35f40f7dc0>, <__main__.Case object at 0x7f35f40f6e60>, <__main__.Case object at 0x7f35f40f6e90>, <__main__.Case object at 0x7f35f40f6020>, <__main__.Case object at 0x7f35f40f7d60>, <__main__.Case object at 0x7f35f40f7fd0>, <__main__.Case object at 0x7f35f40f6fe0>, <__main__.Case object at 0x7f35f40f4be0>, <__main__.Case object at 0x7f35f40f6b00>, <__main__.Case object at 0x7f35f40f7310>, <__main__.Case object at 0x7f35f40f72b0>, <__main__.Case object at 0x7f35f40f5d80>, <__main__.Case object at 0x7f35f40f7cd0>, <__main__.Case object at 0x7f35f40f7520>, <__main__.Case object at 0x7f35f40f7a00>, <__main__.Case object at 0x7f35f40f7190>, <__main__.Case object at 0x7f35f40f7d00>, <__main__.Case object at 0x7f35f40f4df0>, <__main__.Case object at 0x7f35f40f4370>, <__main__.Case object at 0x7f35f40f6d40>, <__main__.Case object at 0x7f35f40f41c0>, <__main__.Case object at 0x7f35f40f6260>, <__main__.Case object at 0x7f35f40f6080>, <__main__.Case object at 0x7f35f40f7c10>, <__main__.Case object at 0x7f35f40f4520>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f407ece0>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f40da230>, <__main__.Case object at 0x7f35f40d9570>, <__main__.Case object at 0x7f35f40d8940>, <__main__.Case object at 0x7f35f40f6bc0>, <__main__.Case object at 0x7f35f40d8a60>, <__main__.Case object at 0x7f35f40d9540>, <__main__.Case object at 0x7f35f40daa70>, <__main__.Case object at 0x7f35f40d8e20>, <__main__.Case object at 0x7f35f40daad0>, <__main__.Case object at 0x7f35f40d8a00>, <__main__.Case object at 0x7f35f40db9a0>, <__main__.Case object at 0x7f35f40d9990>, <__main__.Case object at 0x7f35f40da9b0>, <__main__.Case object at 0x7f35f40d9f60>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 4, tv: 0.4, time steps: 5\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "Episode: 22, Total Steps: 61, Total Rewards: [-160, 85], Status Episode: False\n",
      "------------------------------------------End of episode 22 loop--------------------\n",
      "----- starting point of Episode 23 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 4 to next state (1, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 3 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 4 to next state (1, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 3 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 23 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 1 to next state (0, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 4 to next state (1, 4): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 0 to next state (1, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 1 to next state (1, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 2 to next state (1, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 2 to next state (1, 5): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f35f4092080>, <__main__.Case object at 0x7f35f408a5c0>, <__main__.Case object at 0x7f35f40679a0>, <__main__.Case object at 0x7f35f40f4490>, <__main__.Case object at 0x7f35f40f60e0>, <__main__.Case object at 0x7f35f40f4a30>, <__main__.Case object at 0x7f35f40f6a10>, <__main__.Case object at 0x7f35f40f4790>, <__main__.Case object at 0x7f35f40f6470>, <__main__.Case object at 0x7f35f40f76a0>, <__main__.Case object at 0x7f35f40f45e0>, <__main__.Case object at 0x7f35f40f6d10>, <__main__.Case object at 0x7f35f40f5d50>, <__main__.Case object at 0x7f35f40f5f90>, <__main__.Case object at 0x7f35f40f7880>, <__main__.Case object at 0x7f35f40f7220>, <__main__.Case object at 0x7f35f40f46a0>, <__main__.Case object at 0x7f35f40f74f0>, <__main__.Case object at 0x7f35f40f4d60>, <__main__.Case object at 0x7f35f40f70d0>, <__main__.Case object at 0x7f35f40f4190>, <__main__.Case object at 0x7f35f40f6800>, <__main__.Case object at 0x7f35f40f6530>, <__main__.Case object at 0x7f35f40f4c70>, <__main__.Case object at 0x7f35f40f52a0>, <__main__.Case object at 0x7f35f40f5960>, <__main__.Case object at 0x7f35f40f49a0>, <__main__.Case object at 0x7f35f40f68c0>, <__main__.Case object at 0x7f35f40f5300>, <__main__.Case object at 0x7f35f40f71c0>, <__main__.Case object at 0x7f35f40f4fd0>, <__main__.Case object at 0x7f35f40f4f40>, <__main__.Case object at 0x7f35f40f74c0>, <__main__.Case object at 0x7f35f40f7ac0>, <__main__.Case object at 0x7f35f40f6d70>, <__main__.Case object at 0x7f35f40f62f0>, <__main__.Case object at 0x7f35f40f5e40>, <__main__.Case object at 0x7f35f40f7fa0>, <__main__.Case object at 0x7f35f40f7280>, <__main__.Case object at 0x7f35f40f5900>, <__main__.Case object at 0x7f35f40f7f70>, <__main__.Case object at 0x7f35f40f5270>, <__main__.Case object at 0x7f35f40f6c80>, <__main__.Case object at 0x7f35f40f4ca0>, <__main__.Case object at 0x7f35f40f6110>, <__main__.Case object at 0x7f35f40f4820>, <__main__.Case object at 0x7f35f40da020>, <__main__.Case object at 0x7f35f40d8280>, <__main__.Case object at 0x7f35f40f4dc0>, <__main__.Case object at 0x7f35f40d9600>, <__main__.Case object at 0x7f35f40da260>, <__main__.Case object at 0x7f35f40db0d0>, <__main__.Case object at 0x7f35f40d9a80>, <__main__.Case object at 0x7f35f40d9660>, <__main__.Case object at 0x7f35f40d8fd0>, <__main__.Case object at 0x7f35f40db220>, <__main__.Case object at 0x7f35f40d93f0>, <__main__.Case object at 0x7f35f40db130>, <__main__.Case object at 0x7f35f40da230>, <__main__.Case object at 0x7f35f40db6d0>, <__main__.Case object at 0x7f35f40d9840>, <__main__.Case object at 0x7f35f40d8af0>, <__main__.Case object at 0x7f35f40d8670>, <__main__.Case object at 0x7f35f40d88e0>, <__main__.Case object at 0x7f35f40db2b0>, <__main__.Case object at 0x7f35f40d8be0>, <__main__.Case object at 0x7f35f40da7a0>, <__main__.Case object at 0x7f35f40d8160>, <__main__.Case object at 0x7f35f40db670>, <__main__.Case object at 0x7f35f40daf20>, <__main__.Case object at 0x7f35f40db310>, <__main__.Case object at 0x7f35f40d9870>, <__main__.Case object at 0x7f35f40da170>, <__main__.Case object at 0x7f35f40d99f0>, <__main__.Case object at 0x7f35f40dba90>, <__main__.Case object at 0x7f35f40d9780>, <__main__.Case object at 0x7f35f40da080>, <__main__.Case object at 0x7f35f40d9900>, <__main__.Case object at 0x7f35f40db640>, <__main__.Case object at 0x7f35f40d97b0>, <__main__.Case object at 0x7f35f40d9210>, <__main__.Case object at 0x7f35f40d9390>, <__main__.Case object at 0x7f35f40dae60>, <__main__.Case object at 0x7f35f40dba60>, <__main__.Case object at 0x7f35f40d8790>, <__main__.Case object at 0x7f35f40db760>, <__main__.Case object at 0x7f35f40d9960>, <__main__.Case object at 0x7f35f40da380>, <__main__.Case object at 0x7f35f40d8940>, <__main__.Case object at 0x7f35f40d8310>, <__main__.Case object at 0x7f35f40da5f0>, <__main__.Case object at 0x7f35f40d88b0>, <__main__.Case object at 0x7f35f40d93c0>, <__main__.Case object at 0x7f35f40d8520>, <__main__.Case object at 0x7f35f40da500>, <__main__.Case object at 0x7f35f40d83a0>, <__main__.Case object at 0x7f35f40d8d90>, <__main__.Case object at 0x7f35f40d86d0>, <__main__.Case object at 0x7f35f40da530>, <__main__.Case object at 0x7f35f40d9240>, <__main__.Case object at 0x7f35f40da7d0>, <__main__.Case object at 0x7f35f40db160>, <__main__.Case object at 0x7f35f40db940>, <__main__.Case object at 0x7f35f40d9ea0>, <__main__.Case object at 0x7f35f40bea40>, <__main__.Case object at 0x7f35f40bcf70>, <__main__.Case object at 0x7f35f40bd180>, <__main__.Case object at 0x7f35f40bd660>, <__main__.Case object at 0x7f35f40bca60>, <__main__.Case object at 0x7f35f40bd210>, <__main__.Case object at 0x7f35f40bd750>, <__main__.Case object at 0x7f35f40bc490>, <__main__.Case object at 0x7f35f40be290>, <__main__.Case object at 0x7f35f40bff40>, <__main__.Case object at 0x7f35f40bd600>, <__main__.Case object at 0x7f35f40bc940>, <__main__.Case object at 0x7f35f40bfdf0>, <__main__.Case object at 0x7f35f40bc2b0>, <__main__.Case object at 0x7f35f40bea70>, <__main__.Case object at 0x7f35f40be5f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f35f4091e40>, <__main__.Case object at 0x7f35f407ed10>, <__main__.Case object at 0x7f35f408a4a0>, <__main__.Case object at 0x7f35f4092050>, <__main__.Case object at 0x7f35f40f7eb0>, <__main__.Case object at 0x7f35f40f7700>, <__main__.Case object at 0x7f35f40f64a0>, <__main__.Case object at 0x7f35f40f63e0>, <__main__.Case object at 0x7f35f40f67d0>, <__main__.Case object at 0x7f35f40f46d0>, <__main__.Case object at 0x7f35f40f4670>, <__main__.Case object at 0x7f35f40f6b60>, <__main__.Case object at 0x7f35f40f6a70>, <__main__.Case object at 0x7f35f40f6c20>, <__main__.Case object at 0x7f35f40f7250>, <__main__.Case object at 0x7f35f40f7820>, <__main__.Case object at 0x7f35f40f6350>, <__main__.Case object at 0x7f35f40f6a40>, <__main__.Case object at 0x7f35f40f4940>, <__main__.Case object at 0x7f35f40f6290>, <__main__.Case object at 0x7f35f40920b0>, <__main__.Case object at 0x7f35f40f6ce0>, <__main__.Case object at 0x7f35f40f7d90>, <__main__.Case object at 0x7f35f40f75b0>, <__main__.Case object at 0x7f35f40f72e0>, <__main__.Case object at 0x7f35f40f40a0>, <__main__.Case object at 0x7f35f40f6ef0>, <__main__.Case object at 0x7f35f40f4730>, <__main__.Case object at 0x7f35f40f4280>, <__main__.Case object at 0x7f35f40f4850>, <__main__.Case object at 0x7f35f40f42e0>, <__main__.Case object at 0x7f35f40f7c40>, <__main__.Case object at 0x7f35f40f4460>, <__main__.Case object at 0x7f35f40f6410>, <__main__.Case object at 0x7f35f40f51b0>, <__main__.Case object at 0x7f35f40f7730>, <__main__.Case object at 0x7f35f40f4640>, <__main__.Case object at 0x7f35f40f69b0>, <__main__.Case object at 0x7f35f40f6050>, <__main__.Case object at 0x7f35f40f6140>, <__main__.Case object at 0x7f35f40f5d80>, <__main__.Case object at 0x7f35f40f4af0>, <__main__.Case object at 0x7f35f40f5de0>, <__main__.Case object at 0x7f35f40f6770>, <__main__.Case object at 0x7f35f40f7c10>, <__main__.Case object at 0x7f35f40f4970>, <__main__.Case object at 0x7f35f40f7100>, <__main__.Case object at 0x7f35f40f4c40>, <__main__.Case object at 0x7f35f40f59c0>, <__main__.Case object at 0x7f35f40da050>, <__main__.Case object at 0x7f35f40d8700>, <__main__.Case object at 0x7f35f40f61d0>, <__main__.Case object at 0x7f35f40d9450>, <__main__.Case object at 0x7f35f40da830>, <__main__.Case object at 0x7f35f40f72b0>, <__main__.Case object at 0x7f35f40db9a0>, <__main__.Case object at 0x7f35f40dbbb0>, <__main__.Case object at 0x7f35f40d95d0>, <__main__.Case object at 0x7f35f40f6080>, <__main__.Case object at 0x7f35f40d8b50>, <__main__.Case object at 0x7f35f40d80d0>, <__main__.Case object at 0x7f35f40d9a20>, <__main__.Case object at 0x7f35f40db5e0>, <__main__.Case object at 0x7f35f40d9e70>, <__main__.Case object at 0x7f35f40db790>, <__main__.Case object at 0x7f35f40dada0>, <__main__.Case object at 0x7f35f40db880>, <__main__.Case object at 0x7f35f40d81c0>, <__main__.Case object at 0x7f35f40da3b0>, <__main__.Case object at 0x7f35f40d8220>, <__main__.Case object at 0x7f35f40dadd0>, <__main__.Case object at 0x7f35f40dbd30>, <__main__.Case object at 0x7f35f40d9270>, <__main__.Case object at 0x7f35f40d8370>, <__main__.Case object at 0x7f35f40db5b0>, <__main__.Case object at 0x7f35f40da470>, <__main__.Case object at 0x7f35f40d9a50>, <__main__.Case object at 0x7f35f40db8e0>, <__main__.Case object at 0x7f35f40d8730>, <__main__.Case object at 0x7f35f40d9d80>, <__main__.Case object at 0x7f35f40d85b0>, <__main__.Case object at 0x7f35f40db610>, <__main__.Case object at 0x7f35f40db430>, <__main__.Case object at 0x7f35f40d8f70>, <__main__.Case object at 0x7f35f40db1c0>, <__main__.Case object at 0x7f35f40da770>, <__main__.Case object at 0x7f35f40d9480>, <__main__.Case object at 0x7f35f40d8f10>, <__main__.Case object at 0x7f35f40dacb0>, <__main__.Case object at 0x7f35f40da290>, <__main__.Case object at 0x7f35f40daf80>, <__main__.Case object at 0x7f35f40d90f0>, <__main__.Case object at 0x7f35f40d8340>, <__main__.Case object at 0x7f35f40db6a0>, <__main__.Case object at 0x7f35f40da440>, <__main__.Case object at 0x7f35f40dbb80>, <__main__.Case object at 0x7f35f40db460>, <__main__.Case object at 0x7f35f4073340>, <__main__.Case object at 0x7f35f40bf850>, <__main__.Case object at 0x7f35f40bc6a0>, <__main__.Case object at 0x7f35f40d99c0>, <__main__.Case object at 0x7f35f40bcc10>, <__main__.Case object at 0x7f35f40beb30>, <__main__.Case object at 0x7f35f40bc4f0>, <__main__.Case object at 0x7f35f40da5c0>, <__main__.Case object at 0x7f35f40bf880>, <__main__.Case object at 0x7f35f40bf640>, <__main__.Case object at 0x7f35f40bcbe0>, <__main__.Case object at 0x7f35f40da680>, <__main__.Case object at 0x7f35f40bfd00>, <__main__.Case object at 0x7f35f40be4d0>, <__main__.Case object at 0x7f35f40bd270>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 1)\n",
      "Integrated case process. comm case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (5, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 3), 3, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 1, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 3, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6000000000000001, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6000000000000001, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f35f407ed40>, <__main__.Case object at 0x7f35f4092020>, <__main__.Case object at 0x7f35f408a6b0>, <__main__.Case object at 0x7f35f4067970>, <__main__.Case object at 0x7f35f40f7340>, <__main__.Case object at 0x7f35f40f7670>, <__main__.Case object at 0x7f35f40f6170>, <__main__.Case object at 0x7f35f40f6650>, <__main__.Case object at 0x7f35f40f4550>, <__main__.Case object at 0x7f35f40f41f0>, <__main__.Case object at 0x7f35f40f5ed0>, <__main__.Case object at 0x7f35f40f5e70>, <__main__.Case object at 0x7f35f40f7640>, <__main__.Case object at 0x7f35f40f48b0>, <__main__.Case object at 0x7f35f40f7f10>, <__main__.Case object at 0x7f35f40f5db0>, <__main__.Case object at 0x7f35f40f6830>, <__main__.Case object at 0x7f35f40f40d0>, <__main__.Case object at 0x7f35f40f7490>, <__main__.Case object at 0x7f35f40f4040>, <__main__.Case object at 0x7f35f40f6f20>, <__main__.Case object at 0x7f35f40f7bb0>, <__main__.Case object at 0x7f35f40f52d0>, <__main__.Case object at 0x7f35f40f6f50>, <__main__.Case object at 0x7f35f40f45b0>, <__main__.Case object at 0x7f35f40f7a90>, <__main__.Case object at 0x7f35f40f66e0>, <__main__.Case object at 0x7f35f40f5360>, <__main__.Case object at 0x7f35f40f4880>, <__main__.Case object at 0x7f35f40f6230>, <__main__.Case object at 0x7f35f40f4eb0>, <__main__.Case object at 0x7f35f40f4a60>, <__main__.Case object at 0x7f35f40f6ec0>, <__main__.Case object at 0x7f35f40f5fc0>, <__main__.Case object at 0x7f35f40f6200>, <__main__.Case object at 0x7f35f40f4f10>, <__main__.Case object at 0x7f35f40f6740>, <__main__.Case object at 0x7f35f40f4250>, <__main__.Case object at 0x7f35f40f7e80>, <__main__.Case object at 0x7f35f40f75e0>, <__main__.Case object at 0x7f35f40f7b20>, <__main__.Case object at 0x7f35f40f49d0>, <__main__.Case object at 0x7f35f40f60b0>, <__main__.Case object at 0x7f35f40f4cd0>, <__main__.Case object at 0x7f35f40f7e20>, <__main__.Case object at 0x7f35f40f4130>, <__main__.Case object at 0x7f35f40f68f0>, <__main__.Case object at 0x7f35f40dabc0>, <__main__.Case object at 0x7f35f40d8040>, <__main__.Case object at 0x7f35f40d8ac0>, <__main__.Case object at 0x7f35f40dbc40>, <__main__.Case object at 0x7f35f40db2e0>, <__main__.Case object at 0x7f35f40da8c0>, <__main__.Case object at 0x7f35f40d9d20>, <__main__.Case object at 0x7f35f40d9b40>, <__main__.Case object at 0x7f35f40f4b80>, <__main__.Case object at 0x7f35f40d9f90>, <__main__.Case object at 0x7f35f40d84c0>, <__main__.Case object at 0x7f35f40f5f30>, <__main__.Case object at 0x7f35f40f47f0>, <__main__.Case object at 0x7f35f40db730>, <__main__.Case object at 0x7f35f40d8820>, <__main__.Case object at 0x7f35f40da2c0>, <__main__.Case object at 0x7f35f40d82b0>, <__main__.Case object at 0x7f35f40db340>, <__main__.Case object at 0x7f35f40d9c00>, <__main__.Case object at 0x7f35f40db4f0>, <__main__.Case object at 0x7f35f40d8c10>, <__main__.Case object at 0x7f35f40dafb0>, <__main__.Case object at 0x7f35f40d9cf0>, <__main__.Case object at 0x7f35f40da0b0>, <__main__.Case object at 0x7f35f40d81f0>, <__main__.Case object at 0x7f35f40dac20>, <__main__.Case object at 0x7f35f40dbaf0>, <__main__.Case object at 0x7f35f40dae00>, <__main__.Case object at 0x7f35f40d8bb0>, <__main__.Case object at 0x7f35f40daa10>, <__main__.Case object at 0x7f35f40dbf70>, <__main__.Case object at 0x7f35f40d85e0>, <__main__.Case object at 0x7f35f40d8d30>, <__main__.Case object at 0x7f35f40dabf0>, <__main__.Case object at 0x7f35f40da1d0>, <__main__.Case object at 0x7f35f40d9ba0>, <__main__.Case object at 0x7f35f40dbe80>, <__main__.Case object at 0x7f35f40dbfd0>, <__main__.Case object at 0x7f35f40d8a00>, <__main__.Case object at 0x7f35f40db3a0>, <__main__.Case object at 0x7f35f40d9180>, <__main__.Case object at 0x7f35f40d92d0>, <__main__.Case object at 0x7f35f40d8dc0>, <__main__.Case object at 0x7f35f40dac80>, <__main__.Case object at 0x7f35f40dbbe0>, <__main__.Case object at 0x7f35f40d9d50>, <__main__.Case object at 0x7f35f40db820>, <__main__.Case object at 0x7f35f40d9930>, <__main__.Case object at 0x7f35f40d9b70>, <__main__.Case object at 0x7f35f40db4c0>, <__main__.Case object at 0x7f35f40d80a0>, <__main__.Case object at 0x7f35f40d9ab0>, <__main__.Case object at 0x7f35f40d89d0>, <__main__.Case object at 0x7f35f40d8cd0>, <__main__.Case object at 0x7f35f40db040>, <__main__.Case object at 0x7f35f40db850>, <__main__.Case object at 0x7f35f40d8d00>, <__main__.Case object at 0x7f35f4073370>, <__main__.Case object at 0x7f35f40be020>, <__main__.Case object at 0x7f35f40bffd0>, <__main__.Case object at 0x7f35f40d9690>, <__main__.Case object at 0x7f35f40bfd90>, <__main__.Case object at 0x7f35f40dbca0>, <__main__.Case object at 0x7f35f40be200>, <__main__.Case object at 0x7f35f40bf790>, <__main__.Case object at 0x7f35f40bc370>, <__main__.Case object at 0x7f35f40bfc10>, <__main__.Case object at 0x7f35f40bf7c0>, <__main__.Case object at 0x7f35f40daf50>, <__main__.Case object at 0x7f35f40bd060>, <__main__.Case object at 0x7f35f40d8130>, <__main__.Case object at 0x7f35f40bc2e0>, <__main__.Case object at 0x7f35f40da110>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 3), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 2\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 0\n",
      "Episode: 23, Total Steps: 120, Total Rewards: [-219, 90], Status Episode: False\n",
      "------------------------------------------End of episode 23 loop--------------------\n",
      "----- starting point of Episode 24 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 1 to next state (8, 0): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.0573424842234217\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 0 to next state (8, 0): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 2)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 2) with action 0 to next state (7, 2): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 4), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 3), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 3), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.05145037113226166\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 2 to next state (0, 5): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 1 to next state (0, 4): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 1 to next state (0, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 4 to next state (1, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 0 to next state (1, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 1 to next state (1, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.08511468768694855\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.06378380280654468\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 1 to next state (0, 3): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 24 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.1\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 0, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import psutil\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment_ma_reward_distance_dynamic_notrandom import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env, alpha, gamma, epsilon, lambda_trace=0.8):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_trace = lambda_trace\n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]\n",
    "        self.e_trace_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]  # Eligibility traces\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def choose_action(self, agent_idx, state):\n",
    "        state = tuple(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_action = self.q_tables[agent_idx][state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    def learn(self, agent_idx, state, action, reward, next_state, case_base=None):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "        current_q = self.q_tables[agent_idx][state][action]\n",
    "        max_next_q = max(self.q_tables[agent_idx][next_state])\n",
    "\n",
    "        # Calculate the Q-update with weighted distance-based pull reward if case_base exists\n",
    "        if case_base:\n",
    "            print(f\"Calculating weighted distance-based pull reward for agent {agent_idx}\")\n",
    "            pull_reward = 0  # Initialize pull reward\n",
    "            \n",
    "            for case in case_base:\n",
    "                problems = case.problem if isinstance(case.problem, list) else [case.problem]\n",
    "                \n",
    "                weight = case.trust_value\n",
    "                \n",
    "                # Calculate the pull reward based on the weighted difference in distances\n",
    "                for p in problems:\n",
    "                    distance_current = np.linalg.norm(np.array(state) - np.array(p))\n",
    "                    distance_next = np.linalg.norm(np.array(next_state) - np.array(p))\n",
    "\n",
    "                    distance_diff = distance_current - distance_next\n",
    "                    distance_diff = weight * distance_diff\n",
    "                    pull_reward += np.log1p(abs(distance_diff)) * np.sign(distance_diff)\n",
    "                    pull_reward = np.clip(pull_reward, -1 * self.epsilon, self.epsilon)\n",
    "            \n",
    "            pull_reward = 0\n",
    "            \n",
    "            print(f\"Calculating pull reward agent {agent_idx}: from state {state} with action {action} to next state {next_state}: pull reward: {pull_reward}\")\n",
    "            delta = reward + pull_reward + self.discount_factor * max_next_q - current_q\n",
    "        else:\n",
    "            # Standard Q-learning update without pull reward\n",
    "            print(f\"No communication. Standard Q-learning update for agent {agent_idx}\")\n",
    "            delta = reward + self.discount_factor * max_next_q - current_q\n",
    "\n",
    "        # Update eligibility traces for the current state-action pair\n",
    "        self.e_trace_tables[agent_idx][state][action] += 1\n",
    "\n",
    "        # Update Q-values and eligibility traces for all state-action pairs\n",
    "        for s, actions in self.q_tables[agent_idx].items():\n",
    "            for a in range(self.num_actions):\n",
    "                self.q_tables[agent_idx][s][a] += self.learning_rate * delta * self.e_trace_tables[agent_idx][s][a]\n",
    "                self.e_trace_tables[agent_idx][s][a] *= self.discount_factor * self.lambda_trace\n",
    "        \n",
    "        # Reset the trace for the next state-action pair after the update\n",
    "        self.e_trace_tables[agent_idx][next_state] = [0.0] * self.num_actions\n",
    "\n",
    "\n",
    "class Case:\n",
    "    def __init__(self, problem, solution, trust_value=0.5, total_time_steps=0):\n",
    "        self.problem = ast.literal_eval(problem) if isinstance(problem, str) else problem\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "        self.total_time_steps = total_time_steps\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6\n",
    "        v = state1.size\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.1):\n",
    "        state = ast.literal_eval(state) if isinstance(state, str) else state\n",
    "        for case in case_base:\n",
    "            if state == case.problem: \n",
    "                return case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(agent_idx, c, own_temp_case_base, comm_temp_case_base, source='own'):\n",
    "        \"\"\"Reuse step for adding cases to temporary case bases.\"\"\"\n",
    "        if source == 'own':\n",
    "            own_temp_case_base.append(c)\n",
    "        elif source == 'comm':\n",
    "            comm_temp_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(agent_idx, case_base, temporary_case_base, successful_episodes, total_steps):\n",
    "        for case in case_base:\n",
    "            if any((case.problem, case.solution) == (temp_case.problem, temp_case.solution) for temp_case in temporary_case_base):\n",
    "                if successful_episodes:\n",
    "                    case.trust_value += 0.1\n",
    "                else:\n",
    "                    case.trust_value -= 0.2\n",
    "            else:\n",
    "                if successful_episodes:\n",
    "                    case.trust_value -= 0.1\n",
    "            \n",
    "            case.trust_value = max(0, min(case.trust_value, 1))\n",
    "            print(f\"case content after REVISE for agent {agent_idx}, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(agent_idx, case_base, own_temp_case_base, comm_temp_case_base, successful_episodes, total_steps, threshold=0.49):\n",
    "        if successful_episodes:\n",
    "            for temp_case in reversed(own_temp_case_base):\n",
    "                state = tuple(np.atleast_1d(temp_case.problem))\n",
    "\n",
    "                existing_case = next((case for case in case_base if tuple(np.atleast_1d(case.problem)) == state), None)\n",
    "                \n",
    "                if existing_case is None:\n",
    "                    case_base.append(temp_case)\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} is empty. Temporary case base stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")\n",
    "                else:\n",
    "                    if total_steps < existing_case.total_time_steps:\n",
    "                        # Update the case in the case base if the new case has fewer total steps\n",
    "                        existing_case.solution = temp_case.solution\n",
    "                        existing_case.trust_value = max(0, temp_case.trust_value)\n",
    "                        existing_case.total_time_steps = total_steps\n",
    "                        print(f\"Episode succeeded, updated case base with fewer steps: {temp_case.problem, temp_case.solution, temp_case.trust_value, total_steps}\")\n",
    "                    else:\n",
    "                        print(f\"Episode succeeded, case {temp_case.problem} for agent {agent_idx} is not updated as it has more or equal steps.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Episode not succeeded, temporary case base from own experience is not stored to the case base\")\n",
    "\n",
    "        case_base_dict = {tuple(np.atleast_1d(case.problem)): case for case in case_base}\n",
    "\n",
    "        for temp_comm_case in reversed(comm_temp_case_base):\n",
    "            state_comm = tuple(np.atleast_1d(temp_comm_case.problem))\n",
    "            existing_case = case_base_dict.get(state_comm)\n",
    "\n",
    "            if existing_case is None:\n",
    "                case_base.append(temp_comm_case)\n",
    "                case_base_dict[state_comm] = temp_comm_case\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} is empty. Temporary case base stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "            else:\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} for agent {agent_idx} is not empty. Temporary case base that not stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "\n",
    "        # Remove cases with trust values below the threshold\n",
    "        case_base[:] = [case for case in case_base if case.trust_value >= threshold]\n",
    "\n",
    "        for case in case_base:\n",
    "            print(f\"cases content after RETAIN, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "        return case_base\n",
    "\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.render = render\n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.epsilon_min = epsilon_min  \n",
    "\n",
    "        self.problem_solvers = [ProblemSolver(num_actions, self.env, alpha, gamma, epsilon) for _ in range(self.env.num_agents)]\n",
    "        self.case_bases = [[] for _ in range(self.env.num_agents)]  # Individual case bases for each agent\n",
    "        self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for own experiences\n",
    "        self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for communication experiences\n",
    "        self.successful_episodes = [0] * self.env.num_agents\n",
    "        self.rewards_per_episode = [[] for _ in range(self.env.num_agents)]  \n",
    "        self.total_successful_episodes = 0 \n",
    "        self.action_type = [0] * self.env.num_agents\n",
    "\n",
    "    def run(self):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "        success_steps = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            states = self.env.reset()\n",
    "            episode_reward = [0] * self.env.num_agents\n",
    "            total_steps = 0 \n",
    "            self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            success_count = [0] * self.env.num_agents\n",
    "            dones = [False] * self.env.num_agents\n",
    "            win_states = [False] * self.env.num_agents\n",
    "            successful_episodes = False\n",
    "\n",
    "            while not all(dones):\n",
    "                print(f\"----- starting point of Episode {episode} in steps {total_steps} loop -----\")\n",
    "                \n",
    "                actions = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = self.take_action(agent_idx, state)\n",
    "                    actions.append(action)\n",
    "\n",
    "                next_states, rewards, dones = self.env.step(actions)\n",
    "\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = actions[agent_idx]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "\n",
    "                    physical_state = tuple(state[0])\n",
    "                    win_state = state[1]\n",
    "                    comm_state = state[2]  # Communication state containing messages from other agents\n",
    "\n",
    "                    physical_next_state = tuple(next_state[0])\n",
    "                    win_next_state = next_state[1]\n",
    "                    comm_next_state = tuple(next_state[2]) if next_state[2] != 0 else next_state[2]\n",
    "\n",
    "                    physical_action = action[0]\n",
    "                    comm_action = action[1]\n",
    "\n",
    "                    # Process messages received from other agents\n",
    "                    print(f\"comm next state for agent {agent_idx}: {comm_next_state}\")\n",
    "                    \n",
    "                    if comm_next_state == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        comm_case = Case(problem=comm_next_state[0], solution=comm_next_state[1], trust_value=comm_next_state[2], total_time_steps=comm_next_state[3])\n",
    "                        Case.reuse(agent_idx, comm_case, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='comm')\n",
    "\n",
    "                    c = Case(physical_state, physical_action, total_time_steps=total_steps)\n",
    "                    Case.reuse(agent_idx, c, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='own')\n",
    "\n",
    "                    if self.action_type[agent_idx] == 0:\n",
    "                        print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                        self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                    else:\n",
    "                        print(f\"action type of agent: {agent_idx}: using solution from case base, no learning\")\n",
    "\n",
    "                    if win_next_state: \n",
    "                        success_count[agent_idx] += 1\n",
    "\n",
    "                    episode_reward[agent_idx] += reward\n",
    "                    win_states.append(win_next_state)  \n",
    "\n",
    "                states = next_states\n",
    "                total_steps += 1\n",
    "\n",
    "                self.env.render()\n",
    "                \n",
    "            if self.env.win_flag:\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(total_steps)\n",
    "                successful_episodes = True\n",
    "            \n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                print(f\"win status of agent {agent_idx}  before update the case base: {win_states[agent_idx]}\")\n",
    "                self.rewards_per_episode[agent_idx].append(episode_reward[agent_idx])\n",
    "\n",
    "                print(f\"agent{agent_idx} own temp case base: {self.own_temp_case_bases[agent_idx]}\")\n",
    "                print(f\"agent{agent_idx} comm temp case base: {self.comm_temp_case_bases[agent_idx]}\")\n",
    "                \n",
    "                Case.revise(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], win_states[agent_idx], total_steps=total_steps)\n",
    "                self.case_bases[agent_idx] = Case.retain(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], win_states[agent_idx], total_steps=total_steps)\n",
    "               \n",
    "                \n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "            print(f\"Episode: {episode}, Total Steps: {total_steps}, Total Rewards: {episode_reward}, Status Episode: {successful_episodes}\")\n",
    "            print(f\"------------------------------------------End of episode {episode} loop--------------------\")\n",
    "\n",
    "        success_rate = self.total_successful_episodes / self.episodes * 100\n",
    "\n",
    "        return self.rewards_per_episode, success_rate, memory_usage, gpu_memory_usage, success_steps\n",
    "\n",
    "    def take_action(self, agent_idx, state):\n",
    "        physical_state = tuple(state[0])\n",
    "        win_state = state[1]\n",
    "        comm_state = state[2]\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "            comm_action = 0  # No communication action if using problem solver action\n",
    "            self.action_type[agent_idx] = 0\n",
    "            print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "        else:\n",
    "            similar_solution = Case.retrieve(physical_state, self.case_bases[agent_idx])\n",
    "            if similar_solution is not None:\n",
    "                physical_action = similar_solution.solution\n",
    "                comm_action = (similar_solution.problem, similar_solution.solution, similar_solution.trust_value, similar_solution.total_time_steps)\n",
    "                self.action_type[agent_idx] = 1\n",
    "                print(f\"Physical Action for Agent {agent_idx} from case base: {physical_action}\")\n",
    "            else:\n",
    "                physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "                comm_action = 0  # No communication action if using problem solver action\n",
    "                self.action_type[agent_idx] = 0\n",
    "                print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        return (physical_action, comm_action)\n",
    "\n",
    "    def save_case_base_temporary(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_temporary_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.own_temp_case_bases[agent_idx] + self.comm_temp_case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Temporary case base for Agent {agent_idx} saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Case base for Agent {agent_idx} saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            try:\n",
    "                with open(filename, 'r') as file:\n",
    "                    case_base_data = json.load(file)\n",
    "                    self.case_bases[agent_idx] = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"], case[\"total_time_steps\"]) for case in case_base_data]\n",
    "                    print(f\"Case base for Agent {agent_idx} loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Case base file for Agent {agent_idx} not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "    def plot_rewards(self, rewards, window=1):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            moving_avg_rewards = [np.mean(rewards[agent_idx][i:i + window]) for i in range(0, len(rewards[agent_idx]), window)]\n",
    "            \n",
    "            plt.plot(moving_avg_rewards, label=f'Agent {agent_idx}')\n",
    "        \n",
    "        plt.xlabel(f'Episode (Averaged over every {window} episodes)')\n",
    "        plt.ylabel('Average Total Reward')\n",
    "        plt.title('Average Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 2\n",
    "    num_obstacles = 5\n",
    "    obstacles_random_steps = 20\n",
    "    is_agent_silent = False\n",
    "    episodes = 59\n",
    "    max_steps = 1000\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.1\n",
    "    epsilon_decay = 0.995  \n",
    "    epsilon_min = 0.01  \n",
    "    render = True\n",
    "\n",
    "    env = Env(num_agents=num_agents, num_obstacles=num_obstacles, obstacles_random_steps=obstacles_random_steps, is_agent_silent=is_agent_silent)\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    agent = QCBRL(num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run()\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
