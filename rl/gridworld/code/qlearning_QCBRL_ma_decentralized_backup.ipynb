{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- starting point of Episode 0 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b970>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476bfd0>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb9447702b0>, <__main__.Case object at 0x7bb9447703a0>, <__main__.Case object at 0x7bb944770490>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb944770640>, <__main__.Case object at 0x7bb944770730>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb944770970>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94475ddb0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b0d0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b4c0>, <__main__.Case object at 0x7bb94476b5e0>, <__main__.Case object at 0x7bb94476b6d0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b6a0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb944770040>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb9447701f0>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb9447704c0>, <__main__.Case object at 0x7bb944770580>, <__main__.Case object at 0x7bb944770220>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944770880>, <__main__.Case object at 0x7bb9447705b0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 0, Total Steps: 42, Total Rewards: [-117, -141], Status Episode: False\n",
      "------------------------------------------End of episode 0 loop--------------------\n",
      "----- starting point of Episode 1 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b5e0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb944770610>, <__main__.Case object at 0x7bb944770730>, <__main__.Case object at 0x7bb944770940>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb944770460>, <__main__.Case object at 0x7bb944770670>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb9447713c0>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944771330>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb9447711b0>, <__main__.Case object at 0x7bb944771120>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944771210>, <__main__.Case object at 0x7bb944770d60>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb944770a60>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944771690>, <__main__.Case object at 0x7bb944771630>, <__main__.Case object at 0x7bb9447717b0>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb9447719c0>, <__main__.Case object at 0x7bb944771a50>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944771ba0>, <__main__.Case object at 0x7bb944771cc0>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771ea0>, <__main__.Case object at 0x7bb944771fc0>, <__main__.Case object at 0x7bb944772050>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944772470>, <__main__.Case object at 0x7bb944772500>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb9447727a0>, <__main__.Case object at 0x7bb944772830>, <__main__.Case object at 0x7bb944772950>, <__main__.Case object at 0x7bb9447728f0>, <__main__.Case object at 0x7bb944772a70>, <__main__.Case object at 0x7bb944772b30>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb9447719f0>, <__main__.Case object at 0x7bb944772e00>, <__main__.Case object at 0x7bb944772e90>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944773040>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb9447731c0>, <__main__.Case object at 0x7bb9447732e0>, <__main__.Case object at 0x7bb944773280>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb9447737c0>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944773970>, <__main__.Case object at 0x7bb944773a00>, <__main__.Case object at 0x7bb944773af0>, <__main__.Case object at 0x7bb944773b80>, <__main__.Case object at 0x7bb944773ca0>, <__main__.Case object at 0x7bb944773d30>, <__main__.Case object at 0x7bb944773dc0>, <__main__.Case object at 0x7bb944773e80>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944773fa0>, <__main__.Case object at 0x7bb94477c130>, <__main__.Case object at 0x7bb94477c250>, <__main__.Case object at 0x7bb944773310>, <__main__.Case object at 0x7bb94477c3d0>, <__main__.Case object at 0x7bb94477c4c0>, <__main__.Case object at 0x7bb94477c550>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb94477c730>, <__main__.Case object at 0x7bb94477c820>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476bfd0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476b4c0>, <__main__.Case object at 0x7bb94476b6d0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b6a0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb944770250>, <__main__.Case object at 0x7bb944770160>, <__main__.Case object at 0x7bb9447703a0>, <__main__.Case object at 0x7bb944770910>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb9447709d0>, <__main__.Case object at 0x7bb9447700d0>, <__main__.Case object at 0x7bb944770400>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb944770700>, <__main__.Case object at 0x7bb9447707f0>, <__main__.Case object at 0x7bb9447701c0>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb944771300>, <__main__.Case object at 0x7bb9447712a0>, <__main__.Case object at 0x7bb9447708b0>, <__main__.Case object at 0x7bb944771090>, <__main__.Case object at 0x7bb944770fd0>, <__main__.Case object at 0x7bb944770f70>, <__main__.Case object at 0x7bb944770e50>, <__main__.Case object at 0x7bb944770d30>, <__main__.Case object at 0x7bb944770c40>, <__main__.Case object at 0x7bb944770b80>, <__main__.Case object at 0x7bb944770f10>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb944771750>, <__main__.Case object at 0x7bb944770490>, <__main__.Case object at 0x7bb944771810>, <__main__.Case object at 0x7bb9447718d0>, <__main__.Case object at 0x7bb944771990>, <__main__.Case object at 0x7bb9447701f0>, <__main__.Case object at 0x7bb944771b40>, <__main__.Case object at 0x7bb944771c00>, <__main__.Case object at 0x7bb944771d80>, <__main__.Case object at 0x7bb944770880>, <__main__.Case object at 0x7bb944771e40>, <__main__.Case object at 0x7bb944771f00>, <__main__.Case object at 0x7bb944771f60>, <__main__.Case object at 0x7bb944771180>, <__main__.Case object at 0x7bb944772140>, <__main__.Case object at 0x7bb944772200>, <__main__.Case object at 0x7bb944772260>, <__main__.Case object at 0x7bb9447723b0>, <__main__.Case object at 0x7bb9447724a0>, <__main__.Case object at 0x7bb944772560>, <__main__.Case object at 0x7bb9447726e0>, <__main__.Case object at 0x7bb944770b20>, <__main__.Case object at 0x7bb9447727d0>, <__main__.Case object at 0x7bb944772890>, <__main__.Case object at 0x7bb944772a10>, <__main__.Case object at 0x7bb9447716c0>, <__main__.Case object at 0x7bb944772ad0>, <__main__.Case object at 0x7bb944772b90>, <__main__.Case object at 0x7bb944772bf0>, <__main__.Case object at 0x7bb944772d40>, <__main__.Case object at 0x7bb944772e30>, <__main__.Case object at 0x7bb944772ef0>, <__main__.Case object at 0x7bb944772f50>, <__main__.Case object at 0x7bb944771cf0>, <__main__.Case object at 0x7bb944773160>, <__main__.Case object at 0x7bb944773220>, <__main__.Case object at 0x7bb9447733a0>, <__main__.Case object at 0x7bb944771ff0>, <__main__.Case object at 0x7bb944773460>, <__main__.Case object at 0x7bb944773550>, <__main__.Case object at 0x7bb944773610>, <__main__.Case object at 0x7bb9447722f0>, <__main__.Case object at 0x7bb9447737f0>, <__main__.Case object at 0x7bb9447738b0>, <__main__.Case object at 0x7bb944773910>, <__main__.Case object at 0x7bb944772650>, <__main__.Case object at 0x7bb944773b20>, <__main__.Case object at 0x7bb944773be0>, <__main__.Case object at 0x7bb944773c40>, <__main__.Case object at 0x7bb944772980>, <__main__.Case object at 0x7bb944773e20>, <__main__.Case object at 0x7bb944773ee0>, <__main__.Case object at 0x7bb944772fe0>, <__main__.Case object at 0x7bb94477c0d0>, <__main__.Case object at 0x7bb94477c190>, <__main__.Case object at 0x7bb94477c280>, <__main__.Case object at 0x7bb94477c2e0>, <__main__.Case object at 0x7bb94477c0a0>, <__main__.Case object at 0x7bb94477c4f0>, <__main__.Case object at 0x7bb94477c5b0>, <__main__.Case object at 0x7bb94477c670>, <__main__.Case object at 0x7bb94477c340>, <__main__.Case object at 0x7bb94477c850>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 1, Total Steps: 119, Total Rewards: [-114, -218], Status Episode: False\n",
      "------------------------------------------End of episode 1 loop--------------------\n",
      "----- starting point of Episode 2 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bbb0>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb944770580>, <__main__.Case object at 0x7bb9447713c0>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb944771120>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944770d60>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb944771540>, <__main__.Case object at 0x7bb944771690>, <__main__.Case object at 0x7bb9447717b0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb944771d20>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94475ddb0>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b0d0>, <__main__.Case object at 0x7bb9447705b0>, <__main__.Case object at 0x7bb944770610>, <__main__.Case object at 0x7bb944770730>, <__main__.Case object at 0x7bb944773cd0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944771330>, <__main__.Case object at 0x7bb944770940>, <__main__.Case object at 0x7bb944771210>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb9447711e0>, <__main__.Case object at 0x7bb944771630>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb9447719c0>, <__main__.Case object at 0x7bb944771c30>, <__main__.Case object at 0x7bb944771db0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 2, Total Steps: 47, Total Rewards: [-114, -146], Status Episode: False\n",
      "------------------------------------------End of episode 2 loop--------------------\n",
      "----- starting point of Episode 3 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944770040>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944770be0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb944771cc0>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944770610>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb944771270>, <__main__.Case object at 0x7bb944770940>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944771a20>, <__main__.Case object at 0x7bb944771c30>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944773040>, <__main__.Case object at 0x7bb944772f20>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb9447728c0>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb9447724d0>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb944773730>, <__main__.Case object at 0x7bb9447738e0>, <__main__.Case object at 0x7bb944773b50>, <__main__.Case object at 0x7bb944773ca0>, <__main__.Case object at 0x7bb944773d90>, <__main__.Case object at 0x7bb944773f10>, <__main__.Case object at 0x7bb944770250>, <__main__.Case object at 0x7bb944770310>, <__main__.Case object at 0x7bb944770910>, <__main__.Case object at 0x7bb9447709d0>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb9447701c0>, <__main__.Case object at 0x7bb944771300>, <__main__.Case object at 0x7bb944771090>, <__main__.Case object at 0x7bb944770f40>, <__main__.Case object at 0x7bb944770e50>, <__main__.Case object at 0x7bb944770c40>, <__main__.Case object at 0x7bb9447715a0>, <__main__.Case object at 0x7bb944771510>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476ba00>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944770ac0>, <__main__.Case object at 0x7bb944771ea0>, <__main__.Case object at 0x7bb9447706a0>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb944771ab0>, <__main__.Case object at 0x7bb944770df0>, <__main__.Case object at 0x7bb944771540>, <__main__.Case object at 0x7bb944771690>, <__main__.Case object at 0x7bb944770580>, <__main__.Case object at 0x7bb9447705b0>, <__main__.Case object at 0x7bb944770730>, <__main__.Case object at 0x7bb944773cd0>, <__main__.Case object at 0x7bb9447717b0>, <__main__.Case object at 0x7bb944771210>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb9447711e0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb9447732e0>, <__main__.Case object at 0x7bb944773250>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944771630>, <__main__.Case object at 0x7bb944772e90>, <__main__.Case object at 0x7bb9447719f0>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb9447727a0>, <__main__.Case object at 0x7bb944772680>, <__main__.Case object at 0x7bb944772590>, <__main__.Case object at 0x7bb9447709a0>, <__main__.Case object at 0x7bb944772170>, <__main__.Case object at 0x7bb944771f30>, <__main__.Case object at 0x7bb944771a50>, <__main__.Case object at 0x7bb944771900>, <__main__.Case object at 0x7bb944773820>, <__main__.Case object at 0x7bb9447739d0>, <__main__.Case object at 0x7bb944773a60>, <__main__.Case object at 0x7bb9447714e0>, <__main__.Case object at 0x7bb944773e50>, <__main__.Case object at 0x7bb944773fa0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944771840>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb9447700d0>, <__main__.Case object at 0x7bb944770400>, <__main__.Case object at 0x7bb9447730a0>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb9447712a0>, <__main__.Case object at 0x7bb9447708b0>, <__main__.Case object at 0x7bb944772a70>, <__main__.Case object at 0x7bb944770d30>, <__main__.Case object at 0x7bb944770b80>, <__main__.Case object at 0x7bb944771720>, <__main__.Case object at 0x7bb944772470>]\n",
      "agent1 comm temp case base: []\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 0, 0.5)\n",
      "Episode succeeded, case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 0, 0.5)\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 3, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 1, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 3, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 4, 0.5)\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 1, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 1, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 0, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 1, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 4, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 1, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 1, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 0, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 1, 0.5)\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 0, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 4, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 0, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 0, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 4, 0.5)\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 4, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 4, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 1, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 0, 0.5)\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 0, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 1, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 1, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 1, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 4, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 4, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 0, 0.5)\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 1, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 0, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 4, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 0, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 4, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 0, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 0, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 0, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 4, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 4, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 0, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 0, 0.5)\n",
      "Episode succeeded, case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 4, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 4, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 1, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 2), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 3), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 1, tv: 0.5\n",
      "Episode: 3, Total Steps: 82, Total Rewards: [-128, 19], Status Episode: False\n",
      "------------------------------------------End of episode 3 loop--------------------\n",
      "----- starting point of Episode 4 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.5, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 77)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 77)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 77)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476ba60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476ac20>]\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476b040>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.6, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 4, tv: 0.09999999999999998, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 3, tv: 0.09999999999999998, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 1, tv: 0.09999999999999998, time steps: 53\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.09999999999999998, time steps: 52\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 1, tv: 0.09999999999999998, time steps: 48\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 4, tv: 0.09999999999999998, time steps: 45\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 1, tv: 0.09999999999999998, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 4, tv: 0.09999999999999998, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 1, tv: 0.09999999999999998, time steps: 25\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "Episode: 4, Total Steps: 9, Total Rewards: [-105, 92], Status Episode: False\n",
      "------------------------------------------End of episode 4 loop--------------------\n",
      "----- starting point of Episode 5 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.6, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.6, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.6, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.6, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.6, 78)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.6, 79)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.6, 81)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94472c100>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94475dff0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 3, 0.6)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.6)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.6)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476bfd0>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bbb0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944771000>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.7, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.7, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.7, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.7, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7, time steps: 36\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5\n",
      "Episode: 5, Total Steps: 22, Total Rewards: [-121, 92], Status Episode: False\n",
      "------------------------------------------End of episode 5 loop--------------------\n",
      "----- starting point of Episode 6 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.7, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b340>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bb80>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b370>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.7999999999999999, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7999999999999999, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.09999999999999998, time steps: 21\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999\n",
      "Episode: 6, Total Steps: 9, Total Rewards: [-105, 92], Status Episode: False\n",
      "------------------------------------------End of episode 6 loop--------------------\n",
      "----- starting point of Episode 7 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999999, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999999, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7999999999999999, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.7999999999999999, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999999, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7999999999999999, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7999999999999999, 78)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7999999999999999, 78)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7999999999999999, 78)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476bd90>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94472c130>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b550>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.8999999999999999, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.8999999999999999, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.8999999999999999, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.8999999999999999, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.8999999999999999, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999999\n",
      "Episode: 7, Total Steps: 9, Total Rewards: [-106, 92], Status Episode: False\n",
      "------------------------------------------End of episode 7 loop--------------------\n",
      "----- starting point of Episode 8 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8999999999999999, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.8999999999999999, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.8999999999999999, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.8999999999999999, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.8999999999999999, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.8999999999999999, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.8999999999999999, 78)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8999999999999999, 79)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.8999999999999999, 81)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 8 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 8 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 8 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b0a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94472c100>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476ba00>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476bb50>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.9999999999999999, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.9999999999999999, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.9999999999999999, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.9999999999999999, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.9999999999999999, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 36\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "Episode: 8, Total Steps: 13, Total Rewards: [-112, 92], Status Episode: False\n",
      "------------------------------------------End of episode 8 loop--------------------\n",
      "----- starting point of Episode 9 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9999999999999999, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.9999999999999999, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.9999999999999999, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.9999999999999999, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.9999999999999999, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.9999999999999999, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.9999999999999999, 78)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.9999999999999999, 79)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.9999999999999999, 81)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944770a30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944773040>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb9447713c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476bbb0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb944770940>, <__main__.Case object at 0x7bb944771270>, <__main__.Case object at 0x7bb944771c30>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944770220>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "Episode: 9, Total Steps: 23, Total Rewards: [-122, 92], Status Episode: False\n",
      "------------------------------------------End of episode 9 loop--------------------\n",
      "----- starting point of Episode 10 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b6a0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb944770220>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944771360>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bbb0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944771570>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 12\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944770d60>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944770550>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.7, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.7\n",
      "Episode: 10, Total Steps: 23, Total Rewards: [-122, 92], Status Episode: False\n",
      "------------------------------------------End of episode 10 loop--------------------\n",
      "----- starting point of Episode 11 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b2b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b790>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 12\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476ba00>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476ad10>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.7999999999999999, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.7999999999999999\n",
      "Episode: 11, Total Steps: 13, Total Rewards: [-112, 92], Status Episode: False\n",
      "------------------------------------------End of episode 11 loop--------------------\n",
      "----- starting point of Episode 12 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bbb0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771000>, <__main__.Case object at 0x7bb944772f20>, <__main__.Case object at 0x7bb944771450>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944770760>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.09999999999999998, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.09999999999999998, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.09999999999999998, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.7, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.09999999999999998, time steps: 12\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 1, 0.5)\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 1, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 0, 0.5)\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 1, 0.5)\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 1, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944771870>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.8999999999999999, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.8999999999999999\n",
      "Episode: 12, Total Steps: 19, Total Rewards: [82, 92], Status Episode: True\n",
      "------------------------------------------End of episode 12 loop--------------------\n",
      "----- starting point of Episode 13 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.6, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944772e60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94472c130>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.7, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.7, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.7999999999999999, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944771de0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944773130>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.9999999999999999, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5\n",
      "Episode: 13, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 13 loop--------------------\n",
      "----- starting point of Episode 14 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.7, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.7, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb944770c70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb944770550>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.8999999999999999, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.4999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944771270>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb9447707c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb9447702e0>, <__main__.Case object at 0x7bb944772b00>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.09999999999999998, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.09999999999999998, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.09999999999999998, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.09999999999999998, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.09999999999999998, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.09999999999999998, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 14, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 14 loop--------------------\n",
      "----- starting point of Episode 15 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.7999999999999999, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.7999999999999999, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944771030>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb944770130>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.8999999999999999, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.8999999999999999, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.9999999999999999, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.09999999999999987, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7999999999999999\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944770820>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944772fb0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.7)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.7)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.7)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.7)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "Episode: 15, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 15 loop--------------------\n",
      "----- starting point of Episode 16 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7999999999999999, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7999999999999999, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7999999999999999, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.8999999999999999, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.8999999999999999, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.8999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.8999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.8999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.8999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944771de0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb9447713f0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.9999999999999999, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.9999999999999999, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.8999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.8999999999999999, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944773010>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944772a40>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.29999999999999993, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.29999999999999993, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.29999999999999993, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.29999999999999993, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.29999999999999993, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.29999999999999993, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 16, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 16 loop--------------------\n",
      "----- starting point of Episode 17 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8999999999999999, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.8999999999999999, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.8999999999999999, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.8999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.9999999999999999, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.9999999999999999, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.9999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.9999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.9999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.9999999999999999, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb944772950>, <__main__.Case object at 0x7bb9447702e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b850>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.9999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.9999999999999999, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb944771de0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944770760>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "Episode: 17, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 17 loop--------------------\n",
      "----- starting point of Episode 18 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.9999999999999999, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.9999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.9999999999999999, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.9999999999999999, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb944772830>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb9447729b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb944772620>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944772800>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4999999999999999, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4999999999999999, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4999999999999999, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4999999999999999, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.4999999999999999\n",
      "Episode: 18, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 18 loop--------------------\n",
      "----- starting point of Episode 19 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb944772fb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944771090>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb9447702e0>, <__main__.Case object at 0x7bb944772830>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944770550>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.09999999999999987, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.09999999999999987, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.09999999999999987, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.09999999999999987, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.09999999999999987, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.09999999999999987, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 19, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 19 loop--------------------\n",
      "----- starting point of Episode 20 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944772a40>, <__main__.Case object at 0x7bb944770b50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476abc0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944772fb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb9447729b0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 20, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 20 loop--------------------\n",
      "----- starting point of Episode 21 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476ad10>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb944772800>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb9447728c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb9447720e0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb944771240>, <__main__.Case object at 0x7bb944771ae0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944770820>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 21, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 21 loop--------------------\n",
      "----- starting point of Episode 22 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944772800>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944770130>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb9447728c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb944771360>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 22, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 22 loop--------------------\n",
      "----- starting point of Episode 23 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb9447723e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476aec0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944772800>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb9447720e0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 23, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 23 loop--------------------\n",
      "----- starting point of Episode 24 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944771870>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb9447736d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944772b00>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944772620>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 24, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 24 loop--------------------\n",
      "----- starting point of Episode 25 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb944770820>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb9447715d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944771870>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944773340>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 25, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 25 loop--------------------\n",
      "----- starting point of Episode 26 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944772c50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb944773010>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944770820>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb944772830>, <__main__.Case object at 0x7bb9447736d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 26, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 26 loop--------------------\n",
      "----- starting point of Episode 27 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944770a30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb944773730>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944770760>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944771ae0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 27, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 27 loop--------------------\n",
      "----- starting point of Episode 28 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944772620>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb9447703d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb944772830>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944770a30>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb944773130>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 28, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 28 loop--------------------\n",
      "----- starting point of Episode 29 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944770760>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb9447733d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944772620>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944771d20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 29, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 29 loop--------------------\n",
      "----- starting point of Episode 30 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944773340>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447724d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb9447736d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944772b00>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 30, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 30 loop--------------------\n",
      "----- starting point of Episode 31 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb9447712d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944772020>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944773340>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944771ae0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 31, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 31 loop--------------------\n",
      "----- starting point of Episode 32 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb9447736d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb944773520>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb9447712d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb9447716f0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 32, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 32 loop--------------------\n",
      "----- starting point of Episode 33 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb9447728f0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb9447723e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb944773400>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944772950>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447728c0>, <__main__.Case object at 0x7bb944770820>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 33, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 33 loop--------------------\n",
      "----- starting point of Episode 34 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb944772950>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb9447728f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb944772800>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb9447723e0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944770070>, <__main__.Case object at 0x7bb944772b00>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 34, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 34 loop--------------------\n",
      "----- starting point of Episode 35 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476aec0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944772950>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb944770070>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb9447728f0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944771d20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 35, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 35 loop--------------------\n",
      "----- starting point of Episode 36 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb9447729b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944771240>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb9447716f0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944772620>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 36, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 36 loop--------------------\n",
      "----- starting point of Episode 37 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944772fb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb944772020>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944772950>, <__main__.Case object at 0x7bb9447729b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944770820>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 37, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 37 loop--------------------\n",
      "----- starting point of Episode 38 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb9447716f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb9447721a0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb944772fb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944770c70>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 38, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 38 loop--------------------\n",
      "----- starting point of Episode 39 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb944773010>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb9447738e0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944773520>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944771d20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 39, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 39 loop--------------------\n",
      "----- starting point of Episode 40 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb944773190>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb944771c60>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944773010>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944772620>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 40, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 40 loop--------------------\n",
      "----- starting point of Episode 41 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944771ba0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb944771e70>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb944772cb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944770ee0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 41, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 41 loop--------------------\n",
      "----- starting point of Episode 42 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb944773130>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb9447711b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb944773400>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944771030>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 42, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 42 loop--------------------\n",
      "----- starting point of Episode 43 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb9447724d0>, <__main__.Case object at 0x7bb944773010>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb944772020>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944771ba0>, <__main__.Case object at 0x7bb944773130>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb944773640>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 43, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 43 loop--------------------\n",
      "----- starting point of Episode 44 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944772cb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb944772b00>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb9447733d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944772c50>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 44, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 44 loop--------------------\n",
      "----- starting point of Episode 45 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb944771e70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb9447722c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944771de0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb9447711b0>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944771d20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 45, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 45 loop--------------------\n",
      "----- starting point of Episode 46 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944772950>, <__main__.Case object at 0x7bb944773130>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb944770130>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944771e70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944771090>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944773190>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 46, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 46 loop--------------------\n",
      "----- starting point of Episode 47 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb9447733d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb9447725c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb944772800>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb9447711b0>, <__main__.Case object at 0x7bb944770550>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 47, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 47 loop--------------------\n",
      "----- starting point of Episode 48 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb944771600>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb9447723e0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771090>, <__main__.Case object at 0x7bb9447738e0>, <__main__.Case object at 0x7bb944771360>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb9447707c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 48, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 48 loop--------------------\n",
      "----- starting point of Episode 49 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb944771c30>, <__main__.Case object at 0x7bb9447728c0>, <__main__.Case object at 0x7bb944771e70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb944772020>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb9447711b0>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb944771600>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944773010>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 49, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 49 loop--------------------\n",
      "----- starting point of Episode 50 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944772800>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb9447712d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944772b00>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944771030>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 50, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 50 loop--------------------\n",
      "----- starting point of Episode 51 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb9447725c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944772fb0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944771090>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944773190>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 51, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 51 loop--------------------\n",
      "----- starting point of Episode 52 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771600>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944772830>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944772800>, <__main__.Case object at 0x7bb9447725c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb944773130>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 52, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 52 loop--------------------\n",
      "----- starting point of Episode 53 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb9447738e0>, <__main__.Case object at 0x7bb944772b00>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb944770760>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944771c60>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944771d20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 53, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 53 loop--------------------\n",
      "----- starting point of Episode 54 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb9447721a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944772c50>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb944771090>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944773010>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944771360>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 54, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 54 loop--------------------\n",
      "----- starting point of Episode 55 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb9447724d0>, <__main__.Case object at 0x7bb9447725c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb944772020>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb9447721a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb944771e70>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 55, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 55 loop--------------------\n",
      "----- starting point of Episode 56 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb944771ae0>, <__main__.Case object at 0x7bb944771c60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb9447716f0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944770be0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476ae30>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb9447707c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 56, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 56 loop--------------------\n",
      "----- starting point of Episode 57 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944770760>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb9447703d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447711b0>, <__main__.Case object at 0x7bb944771090>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb9447715d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 57, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 57 loop--------------------\n",
      "----- starting point of Episode 58 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb9447721a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb944771c30>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944770760>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944771240>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944771600>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 58, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 58 loop--------------------\n",
      "----- starting point of Episode 59 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb9447707c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb944770550>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944773850>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944773190>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 59, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 59 loop--------------------\n",
      "----- starting point of Episode 60 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb9447720b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944771030>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb944771090>, <__main__.Case object at 0x7bb944771240>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944773010>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb944770ee0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 60, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 60 loop--------------------\n",
      "----- starting point of Episode 61 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb9447728c0>, <__main__.Case object at 0x7bb944770760>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb944772020>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb9447720b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb9447725c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 61, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 61 loop--------------------\n",
      "----- starting point of Episode 62 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944773190>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb944770a30>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944773520>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944771360>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 62, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 62 loop--------------------\n",
      "----- starting point of Episode 63 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944770550>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb9447713f0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944771600>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476bd00>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944771090>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 63, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 63 loop--------------------\n",
      "----- starting point of Episode 64 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb9447720b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb944773640>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944773190>, <__main__.Case object at 0x7bb944770550>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb944772800>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447721a0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 64, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 64 loop--------------------\n",
      "----- starting point of Episode 65 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476bdc0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476ab90>, <__main__.Case object at 0x7bb944771240>, <__main__.Case object at 0x7bb944773520>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb944771d20>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb9447723e0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb9447715d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 65, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 65 loop--------------------\n",
      "----- starting point of Episode 66 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944770550>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944770a30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb9447738e0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944772800>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944773010>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944772fb0>, <__main__.Case object at 0x7bb944770c70>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 66, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 66 loop--------------------\n",
      "----- starting point of Episode 67 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944770550>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476b9a0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb944772020>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476af80>, <__main__.Case object at 0x7bb94476b820>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944770a30>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b430>, <__main__.Case object at 0x7bb94476aaa0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb944772a40>, <__main__.Case object at 0x7bb944771090>, <__main__.Case object at 0x7bb944770760>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 67, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 67 loop--------------------\n",
      "----- starting point of Episode 68 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 67)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 78)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 68)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 79)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 81)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 77)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b2b0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476ab90>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb9447723e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944772fb0>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode succeeded, case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b0a0>, <__main__.Case object at 0x7bb94476bdf0>, <__main__.Case object at 0x7bb94476b250>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b550>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb9447712d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb944770ee0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 68, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 68 loop--------------------\n",
      "----- starting point of Episode 69 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bf40>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476b490>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476b820>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476afb0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 69, Total Steps: 4, Total Rewards: [-101, -103], Status Episode: False\n",
      "------------------------------------------End of episode 69 loop--------------------\n",
      "----- starting point of Episode 70 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 36)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 61)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476ada0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476bf40>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 36\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b520>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bc10>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476b880>, <__main__.Case object at 0x7bb94476b640>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.19999999999999996, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 70, Total Steps: 4, Total Rewards: [-101, -103], Status Episode: False\n",
      "------------------------------------------End of episode 70 loop--------------------\n",
      "----- starting point of Episode 71 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 71 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 71 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 76)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476bbe0>, <__main__.Case object at 0x7bb94476ada0>, <__main__.Case object at 0x7bb94476bee0>, <__main__.Case object at 0x7bb94476b130>, <__main__.Case object at 0x7bb94476b640>, <__main__.Case object at 0x7bb94476bc40>, <__main__.Case object at 0x7bb94476ace0>, <__main__.Case object at 0x7bb94476b820>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b610>]\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.19999999999999996, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.19999999999999996, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 36\n",
      "Episode succeeded, case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 1, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 1), 0, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 1), 3, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 1), solution: 1, tv: 0.5\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476ba60>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b370>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.6, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 1, time steps: 1\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 71, Total Steps: 13, Total Rewards: [88, -108], Status Episode: False\n",
      "------------------------------------------End of episode 71 loop--------------------\n",
      "----- starting point of Episode 72 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 5)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.5, 6)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 0.5, 7)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 0.5, 8)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.5, 10)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.5, 11)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.5, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 72 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb9447738e0>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb944771900>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb944773fd0>, <__main__.Case object at 0x7bb944770220>, <__main__.Case object at 0x7bb944773040>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944773d30>, <__main__.Case object at 0x7bb944773d90>]\n",
      "agent0 comm temp case base: []\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.19999999999999996, time steps: 77\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.19999999999999996, time steps: 76\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.19999999999999996, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.19999999999999996, time steps: 81\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.19999999999999996, time steps: 79\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.19999999999999996, time steps: 78\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 1, tv: 0.09999999999999998, time steps: 4\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b490>, <__main__.Case object at 0x7bb94476b220>, <__main__.Case object at 0x7bb94476b910>, <__main__.Case object at 0x7bb94476bc70>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944771600>, <__main__.Case object at 0x7bb944770070>, <__main__.Case object at 0x7bb944773010>, <__main__.Case object at 0x7bb944772a40>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944773640>, <__main__.Case object at 0x7bb9447703a0>, <__main__.Case object at 0x7bb944772620>, <__main__.Case object at 0x7bb944770760>, <__main__.Case object at 0x7bb944771690>, <__main__.Case object at 0x7bb944770610>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb944771270>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb944773730>, <__main__.Case object at 0x7bb9447713c0>, <__main__.Case object at 0x7bb944772950>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944772c50>, <__main__.Case object at 0x7bb944773400>, <__main__.Case object at 0x7bb944773ca0>, <__main__.Case object at 0x7bb944771de0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476bd30>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.19999999999999996, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) is empty. Temporary case base stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 0, 0.5)\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 7) is empty. Temporary case base stored to the case base: ((8, 7), 1, 0.5)\n",
      "Episode succeeded, case (8, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 7), 0, 0.5)\n",
      "Episode succeeded, case (8, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 7), 0, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 2, 0.5)\n",
      "Episode succeeded, case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 0, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 4, 0.5)\n",
      "Episode succeeded, case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 4, 0.5)\n",
      "Episode succeeded, case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 4, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 3, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 0, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 4, 0.5)\n",
      "Episode succeeded, case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 1, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 3, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 0, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 0, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 4, 0.5)\n",
      "Episode succeeded, case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 4, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 1, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 0, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.5)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.5)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 0.5)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 7), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.5\n",
      "Episode: 72, Total Steps: 34, Total Rewards: [93, 67], Status Episode: True\n",
      "------------------------------------------End of episode 72 loop--------------------\n",
      "----- starting point of Episode 73 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.5, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.5, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.6, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 0.5, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 0.6, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 0.5, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 0.6, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.6, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 0.5, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.6, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 0.5, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.6, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.5, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.5, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.5, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.5, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.5, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.5, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944772800>, <__main__.Case object at 0x7bb944773fa0>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb944771030>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb9447737c0>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944773580>, <__main__.Case object at 0x7bb944770820>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.7, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.7, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 0.7, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 0.7, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 0.7, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.7, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.6, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 0.5)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 0.5)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 0.5)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 0.5)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 0.5)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 0.5)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb944770160>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb944771000>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944771570>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944773e80>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.19999999999999996, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.19999999999999996, time steps: 78\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.19999999999999996, time steps: 77\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 0.6, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.6, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.6, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (8, 7), solution: 1, tv: 0.09999999999999998, time steps: 26\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 4, tv: 0.09999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 1, tv: 0.09999999999999998, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.09999999999999998, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.09999999999999998, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.09999999999999998, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.09999999999999998, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.09999999999999998, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.09999999999999998, time steps: 7\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "Episode: 73, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 73 loop--------------------\n",
      "----- starting point of Episode 74 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.6, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.6, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.7, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 0.6, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 0.7, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 0.6, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 0.7, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 0.6, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.7, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 0.6, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 0.6, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.7, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.6, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944771870>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944773580>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944770d00>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.7999999999999999, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.7999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 0.7999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 0.7999999999999999, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.7999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.09999999999999998, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.09999999999999998, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.09999999999999998, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.09999999999999998, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.09999999999999998, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.09999999999999998, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.09999999999999998, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.09999999999999998, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.7\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb9447721a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb9447737c0>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771d20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 0.7, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.7, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 0.7, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.7, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 0.7, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 0.7, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 0.7, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 0.7, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.6)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.7)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.7)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 0.7)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 0.7)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 0.7)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.7\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "Episode: 74, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 74 loop--------------------\n",
      "----- starting point of Episode 75 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.7, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7999999999999999, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.7, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.7999999999999999, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 0.7, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 0.7999999999999999, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 0.7, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 0.7999999999999999, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 0.7, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.7999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 0.7, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 0.7, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.7999999999999999, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944773fd0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb94472d420>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.8999999999999999, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.8999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 0.8999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 0.8999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 0.8999999999999999, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.8999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.7999999999999999, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 0.7)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 0.7)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 0.7)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 0.7)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 0.7)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 0.7)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.7)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb9447737c0>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944773040>, <__main__.Case object at 0x7bb944770be0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944773e80>, <__main__.Case object at 0x7bb9447720e0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 0.7999999999999999, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.7999999999999999, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 0.7999999999999999, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.7999999999999999, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 0.7999999999999999, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 0.7999999999999999, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 0.7999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 0.7999999999999999, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.7999999999999999, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999996, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.29999999999999993, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.29999999999999993, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.29999999999999993, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.29999999999999993, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.29999999999999993, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.29999999999999993, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.29999999999999993, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.29999999999999993, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7999999999999999\n",
      "Episode: 75, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 75 loop--------------------\n",
      "----- starting point of Episode 76 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.7999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8999999999999999, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.7999999999999999, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.8999999999999999, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 0.7999999999999999, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 0.8999999999999999, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 0.7999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 0.8999999999999999, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 0.7999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.8999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 0.7999999999999999, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.8999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 0.7999999999999999, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.8999999999999999, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.7999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.7999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944771c60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb94473b1c0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.9999999999999999, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.9999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 0.9999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 0.9999999999999999, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.8999999999999999, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.29999999999999993, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.29999999999999993, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.29999999999999993, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.29999999999999993, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.29999999999999993, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.29999999999999993, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.29999999999999993, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.29999999999999993, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.8999999999999999\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb9447705e0>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb9447729b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944773e80>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944773040>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 0.8999999999999999, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.8999999999999999, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 0.8999999999999999, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.8999999999999999, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 0.8999999999999999, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 0.8999999999999999, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 0.8999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 0.8999999999999999, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.8999999999999999, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.8999999999999999, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "Episode: 76, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 76 loop--------------------\n",
      "----- starting point of Episode 77 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.8999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.9999999999999999, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.8999999999999999, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.9999999999999999, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 0.8999999999999999, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 0.9999999999999999, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 0.8999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 0.9999999999999999, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 0.8999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.9999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 0.8999999999999999, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.9999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 0.8999999999999999, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.9999999999999999, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.8999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.9999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.8999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.8999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.8999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.8999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.8999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.8999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.8999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.8999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.8999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.8999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944770a30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb94476bd60>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 0.9999999999999999, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.8999999999999999\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb944773fa0>, <__main__.Case object at 0x7bb944773580>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb9447725c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb9447705e0>, <__main__.Case object at 0x7bb944772f20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 0.9999999999999999, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.9999999999999999, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 0.9999999999999999, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.9999999999999999, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 0.9999999999999999, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 0.9999999999999999, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 0.9999999999999999, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 0.9999999999999999, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.9999999999999999, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.9999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.3999999999999999, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.4999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.4999999999999999, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.4999999999999999, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.4999999999999999, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.4999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.4999999999999999, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.4999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4999999999999999, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.4999999999999999\n",
      "Episode: 77, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 77 loop--------------------\n",
      "----- starting point of Episode 78 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.9999999999999999, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 0.9999999999999999, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 0.9999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 0.9999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 0.9999999999999999, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 0.9999999999999999, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.9999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.9999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.9999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.9999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.9999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.9999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.9999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.9999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.9999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 0.9999999999999999, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 0.9999999999999999, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb944771ab0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb94476bb50>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.4999999999999999, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.4999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.4999999999999999, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.4999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.4999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.4999999999999999, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.4999999999999999, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.4999999999999999, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.4999999999999999\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944770ac0>, <__main__.Case object at 0x7bb944771420>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944772e60>, <__main__.Case object at 0x7bb944770d00>, <__main__.Case object at 0x7bb944773fd0>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb944771450>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.09999999999999987, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.09999999999999987, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.09999999999999987, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.09999999999999987, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.09999999999999987, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.09999999999999987, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.09999999999999987, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.09999999999999987, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.9999999999999999\n",
      "Episode: 78, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 78 loop--------------------\n",
      "----- starting point of Episode 79 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb9447720b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb944773fd0>, <__main__.Case object at 0x7bb944770a30>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.09999999999999987, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.09999999999999987, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.09999999999999987, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.09999999999999987, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.09999999999999987, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.09999999999999987, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.09999999999999987, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.09999999999999987, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944770d00>, <__main__.Case object at 0x7bb9447713f0>, <__main__.Case object at 0x7bb944770be0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944772e60>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944771510>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.5999999999999999, time steps: 33\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5999999999999999\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 79, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 79 loop--------------------\n",
      "----- starting point of Episode 80 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944772e60>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb944771360>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944747e50>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 1)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 1)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb9447721a0>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944771c60>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944770be0>, <__main__.Case object at 0x7bb944770a60>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb944771030>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999984, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 80, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 80 loop--------------------\n",
      "----- starting point of Episode 81 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944770a60>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447722c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944738dc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb944770be0>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb944772f20>, <__main__.Case object at 0x7bb944770460>, <__main__.Case object at 0x7bb9447725c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb9447705e0>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944771690>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.19999999999999996, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "Episode: 81, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 81 loop--------------------\n",
      "----- starting point of Episode 82 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944771d20>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944771870>, <__main__.Case object at 0x7bb944771360>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.19999999999999996, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.19999999999999996, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb944773fa0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb9447705e0>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944770ee0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 33\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 82, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 82 loop--------------------\n",
      "----- starting point of Episode 83 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb944771ab0>, <__main__.Case object at 0x7bb9447705e0>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944773130>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb94476b670>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 1)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 1)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944771420>, <__main__.Case object at 0x7bb944770a30>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944771150>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb944773fa0>, <__main__.Case object at 0x7bb944771000>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944771270>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999996, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 83, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 83 loop--------------------\n",
      "----- starting point of Episode 84 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb944771000>, <__main__.Case object at 0x7bb944770a60>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb944773850>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb94476aef0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb944773fa0>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944770220>, <__main__.Case object at 0x7bb944771c60>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944770160>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.19999999999999996, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "Episode: 84, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 84 loop--------------------\n",
      "----- starting point of Episode 85 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944772020>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944772b00>, <__main__.Case object at 0x7bb944773130>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.19999999999999996, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.19999999999999996, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944771510>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb944772bc0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 33\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 85, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 85 loop--------------------\n",
      "----- starting point of Episode 86 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb9447720b0>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944770280>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb94475caf0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 1)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 1)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944771360>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944770820>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb944770a60>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999996, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 86, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 86 loop--------------------\n",
      "----- starting point of Episode 87 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944771000>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb944772a70>, <__main__.Case object at 0x7bb9447715d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944744700>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb9447738e0>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb944770d00>, <__main__.Case object at 0x7bb9447737c0>, <__main__.Case object at 0x7bb944771270>, <__main__.Case object at 0x7bb944771ea0>, <__main__.Case object at 0x7bb944771150>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944773e80>, <__main__.Case object at 0x7bb944771ab0>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb9447724d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.19999999999999996, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "Episode: 87, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 87 loop--------------------\n",
      "----- starting point of Episode 88 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb944771060>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944771ab0>, <__main__.Case object at 0x7bb944770280>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.19999999999999996, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.19999999999999996, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944773e80>, <__main__.Case object at 0x7bb944770af0>, <__main__.Case object at 0x7bb944770ee0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944772a70>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 33\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 88, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 88 loop--------------------\n",
      "----- starting point of Episode 89 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb94476b670>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944771d20>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb944772cb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb94476b1c0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 1)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 1)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb944772a70>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb9447712d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944772f20>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb9447706a0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999996, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 89, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 89 loop--------------------\n",
      "----- starting point of Episode 90 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944772f20>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944770be0>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb9447720e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb94476b730>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb9447711b0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb944771000>, <__main__.Case object at 0x7bb944770a60>, <__main__.Case object at 0x7bb9447716f0>, <__main__.Case object at 0x7bb944770820>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb944771900>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944771db0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.19999999999999996, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "Episode: 90, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 90 loop--------------------\n",
      "----- starting point of Episode 91 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b790>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb9447738e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476aef0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944770ac0>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb944772cb0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.19999999999999996, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.19999999999999996, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb944771900>, <__main__.Case object at 0x7bb944771e70>, <__main__.Case object at 0x7bb944772bc0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b340>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944770130>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 33\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 91, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 91 loop--------------------\n",
      "----- starting point of Episode 92 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb944772020>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb9447729b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476afe0>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb94472c100>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 1)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 1)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94475f130>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb9447736d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb9447738e0>, <__main__.Case object at 0x7bb944771de0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999996, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 92, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 92 loop--------------------\n",
      "----- starting point of Episode 93 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476b940>, <__main__.Case object at 0x7bb94476b280>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944772f20>, <__main__.Case object at 0x7bb944773fa0>, <__main__.Case object at 0x7bb944773730>, <__main__.Case object at 0x7bb9447737c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944770ac0>, <__main__.Case object at 0x7bb944770be0>, <__main__.Case object at 0x7bb944746ce0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944773fd0>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb944773d30>, <__main__.Case object at 0x7bb9447706a0>, <__main__.Case object at 0x7bb944770c70>, <__main__.Case object at 0x7bb9447712d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944773850>, <__main__.Case object at 0x7bb944770d00>, <__main__.Case object at 0x7bb944771600>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.19999999999999996, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "Episode: 93, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 93 loop--------------------\n",
      "----- starting point of Episode 94 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944744700>, <__main__.Case object at 0x7bb94476b370>, <__main__.Case object at 0x7bb94476af50>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476bf10>, <__main__.Case object at 0x7bb944768490>, <__main__.Case object at 0x7bb9447738e0>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944771030>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476bb20>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb944772f20>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.19999999999999996, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.19999999999999996, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476b400>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476b7c0>, <__main__.Case object at 0x7bb94476be20>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476ab00>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb9447715d0>, <__main__.Case object at 0x7bb9447704f0>, <__main__.Case object at 0x7bb944771360>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb94476ba90>, <__main__.Case object at 0x7bb94476b9d0>, <__main__.Case object at 0x7bb94476b3d0>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944771270>, <__main__.Case object at 0x7bb944770820>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 33\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 94, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 94 loop--------------------\n",
      "----- starting point of Episode 95 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb94476ab60>, <__main__.Case object at 0x7bb94476b190>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb944769960>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb944770be0>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb944770280>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944752650>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94476b8b0>, <__main__.Case object at 0x7bb9447712d0>, <__main__.Case object at 0x7bb944770ac0>, <__main__.Case object at 0x7bb94476b340>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 1)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 1)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94472fa30>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944772f20>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944773730>, <__main__.Case object at 0x7bb944770b50>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb944773580>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999996, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 95, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 95 loop--------------------\n",
      "----- starting point of Episode 96 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476b010>, <__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb944771270>, <__main__.Case object at 0x7bb9447723e0>, <__main__.Case object at 0x7bb944773c10>, <__main__.Case object at 0x7bb944771000>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476bd30>, <__main__.Case object at 0x7bb94476bd60>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb944770d00>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb94476aef0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb944747e50>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb9447725c0>, <__main__.Case object at 0x7bb944773580>, <__main__.Case object at 0x7bb9447711b0>, <__main__.Case object at 0x7bb944770460>, <__main__.Case object at 0x7bb944772e60>, <__main__.Case object at 0x7bb9447703d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb9447720e0>, <__main__.Case object at 0x7bb944772a70>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944771de0>, <__main__.Case object at 0x7bb944770100>, <__main__.Case object at 0x7bb944772800>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.19999999999999996, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "Episode: 96, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 96 loop--------------------\n",
      "----- starting point of Episode 97 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb944738dc0>, <__main__.Case object at 0x7bb94476b850>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb9447736d0>, <__main__.Case object at 0x7bb9447733d0>, <__main__.Case object at 0x7bb944771c60>, <__main__.Case object at 0x7bb944771360>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b3a0>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb944746d10>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944770280>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.19999999999999996, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.19999999999999996, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.19999999999999996, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b5b0>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944771510>, <__main__.Case object at 0x7bb944770ac0>, <__main__.Case object at 0x7bb944773130>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94472f820>, <__main__.Case object at 0x7bb94476ac80>, <__main__.Case object at 0x7bb94476bb50>, <__main__.Case object at 0x7bb94476b610>, <__main__.Case object at 0x7bb94476ae90>, <__main__.Case object at 0x7bb94476beb0>, <__main__.Case object at 0x7bb94476b1c0>, <__main__.Case object at 0x7bb94476baf0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb944771060>, <__main__.Case object at 0x7bb944770820>, <__main__.Case object at 0x7bb944770a60>, <__main__.Case object at 0x7bb944773c10>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.6, time steps: 33\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 97, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 97 loop--------------------\n",
      "----- starting point of Episode 98 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476be80>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb94473b3a0>, <__main__.Case object at 0x7bb944770ee0>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb944772a70>, <__main__.Case object at 0x7bb944770fa0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94476b7f0>, <__main__.Case object at 0x7bb94476b2e0>, <__main__.Case object at 0x7bb94476bb80>, <__main__.Case object at 0x7bb94476b040>, <__main__.Case object at 0x7bb944769930>, <__main__.Case object at 0x7bb94476b8e0>, <__main__.Case object at 0x7bb94476bc10>, <__main__.Case object at 0x7bb944752680>, <__main__.Case object at 0x7bb9477fa8f0>, <__main__.Case object at 0x7bb94476bcd0>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944747e50>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 1)\n",
      "Integrated case process. comm case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 1)\n",
      "Integrated case process. comm case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 1)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb9447525f0>, <__main__.Case object at 0x7bb94472d420>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944771000>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476bac0>, <__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94475caf0>, <__main__.Case object at 0x7bb94472c100>, <__main__.Case object at 0x7bb944773520>, <__main__.Case object at 0x7bb944771030>, <__main__.Case object at 0x7bb944773670>, <__main__.Case object at 0x7bb944771270>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 0, tv: 0.19999999999999996, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 98, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 98 loop--------------------\n",
      "----- starting point of Episode 99 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 5)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 2, 1, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 2), 2, 1, 5)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 1, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 3), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 4), 2, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 5), 2, 1, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 6), 3, 1, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((8, 6), 3, 1, 27)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 0, 1, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7bb94476ae00>, <__main__.Case object at 0x7bb94476abc0>, <__main__.Case object at 0x7bb94476b730>, <__main__.Case object at 0x7bb94476afb0>, <__main__.Case object at 0x7bb944746b00>, <__main__.Case object at 0x7bb94475dfc0>, <__main__.Case object at 0x7bb94473b1c0>, <__main__.Case object at 0x7bb9447703d0>, <__main__.Case object at 0x7bb944773730>, <__main__.Case object at 0x7bb944772a70>, <__main__.Case object at 0x7bb944770130>, <__main__.Case object at 0x7bb9447722c0>, <__main__.Case object at 0x7bb9447720b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7bb94476b1f0>, <__main__.Case object at 0x7bb94476b160>, <__main__.Case object at 0x7bb94476b760>, <__main__.Case object at 0x7bb94476bd90>, <__main__.Case object at 0x7bb94476bca0>, <__main__.Case object at 0x7bb944750070>, <__main__.Case object at 0x7bb94472c130>, <__main__.Case object at 0x7bb94476a8c0>, <__main__.Case object at 0x7bb94476aad0>, <__main__.Case object at 0x7bb9447729b0>, <__main__.Case object at 0x7bb944770280>, <__main__.Case object at 0x7bb9447707c0>, <__main__.Case object at 0x7bb944738dc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 1, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 0, tv: 1, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (9, 3), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 2, tv: 0.6, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7bb94476ac50>, <__main__.Case object at 0x7bb94476be50>, <__main__.Case object at 0x7bb94476ac20>, <__main__.Case object at 0x7bb94476ba30>, <__main__.Case object at 0x7bb944752470>, <__main__.Case object at 0x7bb94472fa60>, <__main__.Case object at 0x7bb944771570>, <__main__.Case object at 0x7bb944773c10>, <__main__.Case object at 0x7bb944771900>, <__main__.Case object at 0x7bb944770b50>, <__main__.Case object at 0x7bb944773d30>, <__main__.Case object at 0x7bb944771150>, <__main__.Case object at 0x7bb9447736d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7bb94476ad70>, <__main__.Case object at 0x7bb94476b4f0>, <__main__.Case object at 0x7bb94476bfa0>, <__main__.Case object at 0x7bb94476bf40>, <__main__.Case object at 0x7bb944746ce0>, <__main__.Case object at 0x7bb94475dff0>, <__main__.Case object at 0x7bb944738be0>, <__main__.Case object at 0x7bb944772cb0>, <__main__.Case object at 0x7bb944773130>, <__main__.Case object at 0x7bb944770fa0>, <__main__.Case object at 0x7bb944773340>, <__main__.Case object at 0x7bb944772bc0>, <__main__.Case object at 0x7bb9447737c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 81\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 1, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 1, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 1, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 1, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 1, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 1, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 1, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.19999999999999996, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 5\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 1\n",
      "Episode: 99, Total Steps: 13, Total Rewards: [93, 88], Status Episode: True\n",
      "------------------------------------------End of episode 99 loop--------------------\n",
      "Success rate: 85.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZlUlEQVR4nO3dd1RU19oG8GeGMnQQkKaoCDbUqMGGWIMCttgSo6JijwYbdk2siRKN0Vyj0WgixKvGXiJJFKyosfcWo8Yu2BAQEBiY/f3h5XyOgDIwDHp8fmuxktlnzz77vAzycKpCCCFAREREJFPKkp4AERERUXFi2CEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZY9ghIiIiWWPYISIiSWRkJBQKBW7cuGHQ9SoUCkybNs2g66R3B8MOvTNy/hHP+TI2NkaZMmXQp08f3L17t6Sn99aoUKEC2rVrl+ey48ePQ6FQIDIy0rCTekft3btX6zP98teaNWtKeopEbwTjkp4AkaHNmDEDHh4eSE9Px+HDhxEZGYkDBw7g/PnzMDMzK+npEels+PDhqFevXq52X19fncfq1asXunXrBpVKpY+pEb0RGHbondO6dWvUrVsXADBgwAA4Ojpi9uzZ+O2339C1a9cSnl3haDQaZGZmMqzJUGpqKiwtLV/Zp0mTJvjoo4/0sj4jIyMYGRnpZSyiNwUPY9E7r0mTJgCAa9euabX//fff+Oijj2Bvbw8zMzPUrVsXv/32m1YftVqN6dOno1KlSjAzM4ODgwMaN26MmJgYrX67d+9GkyZNYGlpCTs7O3To0AGXLl3S6tOnTx9UqFAh1/ymTZsGhUKh1aZQKDB06FCsWrUK1atXh0qlwvbt2wEAd+/eRf/+/eHm5gaVSgUPDw8MGTIEmZmZ0vsTExMxcuRIuLu7Q6VSwcvLC7Nnz4ZGo9GteAUQHx+Pvn37omzZslCpVHB1dUWHDh20zgnZunUr2rZtK83Z09MTX375JbKzs3ONt2jRIlSsWBHm5uaoX78+9u/fj+bNm6N58+Za/TIyMjB16lR4eXlBpVLB3d0d48aNQ0ZGRoHmvX79evj4+MDc3ByOjo7o2bOn1uHOuXPnQqFQ4ObNm7neO3HiRJiamuLJkydS25EjRxAUFARbW1tYWFigWbNmOHjwoNb7cr7XFy9eRI8ePVCqVCk0bty4QPN9nRc/M1WqVIGZmRl8fHwQGxur1S+vc3aOHz+OwMBAODo6wtzcHB4eHujXr5/W+1JTUzF69GjpM1WlShXMnTsXQgitfhkZGQgLC0Pp0qVhbW2NDz/8EHfu3Mlzznfv3kW/fv3g7OwMlUqF6tWrY/ny5bn6ff/996hevTosLCxQqlQp1K1bF6tXry5kpUiOuGeH3nk5/6iXKlVKartw4QL8/PxQpkwZTJgwAZaWlli3bh06duyIjRs3olOnTgCe/3IKDw/HgAEDUL9+fSQnJ+P48eM4efIkWrVqBQDYuXMnWrdujYoVK2LatGl49uwZvv/+e/j5+eHkyZN5BpyC2L17N9atW4ehQ4fC0dERFSpUwL1791C/fn0kJiZi0KBBqFq1Ku7evYsNGzYgLS0NpqamSEtLQ7NmzXD37l18+umnKFeuHP766y9MnDgRcXFx+O6774pSzly6dOmCCxcuYNiwYahQoQIePHiAmJgY3Lp1S9r2yMhIWFlZYdSoUbCyssLu3bsxZcoUJCcn45tvvpHGWrx4MYYOHYomTZogLCwMN27cQMeOHVGqVCmULVtW6qfRaPDhhx/iwIEDGDRoEKpVq4Zz585h/vz5+Oeff7Bly5ZXzjkyMhJ9+/ZFvXr1EB4ejvv37+M///kPDh48iFOnTsHOzg5du3bFuHHjsG7dOowdO1br/evWrUNAQID0mdq9ezdat24NHx8fTJ06FUqlEhEREfjggw+wf/9+1K9fX+v9H3/8MSpVqoRZs2blCgt5efr0KR49epSr3cHBQSso79u3D2vXrsXw4cOhUqnwww8/ICgoCEePHkWNGjXyHPvBgwcICAhA6dKlMWHCBNjZ2eHGjRvYtGmT1EcIgQ8//BB79uxB//79Ubt2bezYsQNjx47F3bt3MX/+fKnvgAEDsHLlSvTo0QONGjXC7t270bZt21zrvX//Pho2bCiFtNKlS+PPP/9E//79kZycjJEjRwIAli1bhuHDh+Ojjz7CiBEjkJ6ejrNnz+LIkSPo0aPHa2tH7whB9I6IiIgQAMTOnTvFw4cPxe3bt8WGDRtE6dKlhUqlErdv35b6+vv7i5o1a4r09HSpTaPRiEaNGolKlSpJbbVq1RJt27Z95Xpr164tnJycxOPHj6W2M2fOCKVSKXr37i21hYSEiPLly+d6/9SpU8XLP6oAhFKpFBcuXNBq7927t1AqleLYsWO5xtFoNEIIIb788kthaWkp/vnnH63lEyZMEEZGRuLWrVuv3J7y5cvnu83Hjh0TAERERIQQQognT54IAOKbb7555ZhpaWm52j799FNhYWEhfQ8yMjKEg4ODqFevnlCr1VK/yMhIAUA0a9ZMavvvf/8rlEql2L9/v9aYS5YsEQDEwYMH851LZmamcHJyEjVq1BDPnj2T2qOiogQAMWXKFKnN19dX+Pj4aL3/6NGjAoBYsWKFEOJ53StVqiQCAwOl70HONnt4eIhWrVpJbTnf6+7du+c7vxft2bNHAMj3Ky4uTuqb03b8+HGp7ebNm8LMzEx06tRJasv5Obl+/boQQojNmzcLAHl+pnJs2bJFABBfffWVVvtHH30kFAqFuHr1qhBCiNOnTwsA4rPPPtPq16NHDwFATJ06VWrr37+/cHV1FY8ePdLq261bN2Frayt9Zjp06CCqV69egGrRu4yHseid07JlS5QuXRru7u746KOPYGlpid9++03aM5CQkIDdu3eja9eu0l/Mjx49wuPHjxEYGIgrV65IhzPs7Oxw4cIFXLlyJc91xcXF4fTp0+jTpw/s7e2l9vfeew+tWrXCH3/8UejtaNasGby9vaXXGo0GW7ZsQfv27aVzkl6U8xf++vXr0aRJE5QqVUratkePHqFly5bIzs7OdVijKMzNzWFqaoq9e/dqHdLJq1+OnJo3adIEaWlp+PvvvwE8P5Ty+PFjDBw4EMbG/79TOjg4WGuvXM42VqtWDVWrVtXaxg8++AAAsGfPnnzncvz4cTx48ACfffaZ1jlQbdu2RdWqVfH7779LbZ988glOnDihdQh07dq1UKlU6NChAwDg9OnTuHLlCnr06IHHjx9Lc0lNTYW/vz9iY2NzHT4cPHhwvvPLy5QpUxATE5Pr68XPHPD8hGUfHx/pdbly5dChQwfs2LEjz0OGwPPPOABERUVBrVbn2eePP/6AkZERhg8frtU+evRoCCHw559/Sv0A5OqXs5cmhxACGzduRPv27SGE0PoeBgYGIikpCSdPnpTmd+fOHRw7duwVFaJ3HQ9j0Ttn0aJFqFy5MpKSkrB8+XLExsZqXXly9epVCCEwefJkTJ48Oc8xHjx4gDJlymDGjBno0KEDKleujBo1aiAoKAi9evXCe++9BwDS+RxVqlTJNUa1atWwY8eOAp2AmhcPDw+t1w8fPkRycnK+hyNyXLlyBWfPnkXp0qXz3baiyglWKpUKs2fPxujRo+Hs7IyGDRuiXbt26N27N1xcXKT+Fy5cwBdffIHdu3cjOTlZa6ykpCQA/19LLy8vreXGxsa5DgVeuXIFly5dKtQ2vup7VrVqVRw4cEB6/fHHH2PUqFFYu3YtJk2aBCEE1q9fj9atW8PGxkaaCwCEhITku86kpCStwPby9/Z1atasiZYtW762X6VKlXK1Va5cGWlpaXj48KHW9yRHs2bN0KVLF0yfPh3z589H8+bN0bFjR/To0UP6ubl58ybc3NxgbW2t9d5q1apJy3P+q1Qq4enpqdXv5Vo/fPgQiYmJWLp0KZYuXZrntuR8D8ePH4+dO3eifv368PLyQkBAAHr06AE/P7/X1oPeHQw79M6pX7++tOejY8eOaNy4MXr06IHLly/DyspK+it7zJgxCAwMzHOMnF+4TZs2xbVr17B161ZER0fjp59+wvz587FkyRIMGDBAp3m9fBJyjvz+4n5xb4guNBoNWrVqhXHjxuW5vHLlyq98v5mZGZ49e5bnsrS0NKlPjpEjR6J9+/bYsmULduzYgcmTJyM8PBy7d+9GnTp1kJiYiGbNmsHGxgYzZsyAp6cnzMzMcPLkSYwfP75QJ01rNBrUrFkT8+bNy3O5u7u7zmPmxc3NDU2aNMG6deswadIkHD58GLdu3cLs2bO15gIA33zzDWrXrp3nOFZWVlqvC/u9LQ4KhQIbNmzA4cOHsW3bNuzYsQP9+vXDt99+i8OHD+eauz7k1Kxnz575hsScPyiqVauGy5cvIyoqCtu3b8fGjRvxww8/YMqUKZg+fbre50ZvJ4YdeqcZGRkhPDwcLVq0wMKFCzFhwgRUrFgRAGBiYlKgv5bt7e3Rt29f9O3bFykpKWjatCmmTZuGAQMGoHz58gCAy5cv53rf33//DUdHR2mvTqlSpZCYmJirX15X++SldOnSsLGxwfnz51/Zz9PTEykpKQXatryUL18eFy9ezHNZznbmbPeL6xw9ejRGjx6NK1euoHbt2vj222+xcuVK7N27F48fP8amTZvQtGlT6T3Xr1/PtV7g+Z63Fi1aSO1ZWVm4ceOG9MsvZ31nzpyBv79/viHyVduXsy05h71e3L6Xt+2TTz7BZ599hsuXL2Pt2rWwsLBA+/btteYCADY2NoWuub7kdbj1n3/+gYWFRb57wXI0bNgQDRs2xMyZM7F69WoEBwdjzZo10ud8586dePr0qdbenZxDkDk1K1++PDQaDa5du6a1N+fln4+cK7Wys7MLVDNLS0t88skn+OSTT5CZmYnOnTtj5syZmDhxIm/HQAB46TkRmjdvjvr16+O7775Deno6nJyc0Lx5c/z444+Ii4vL1f/hw4fS/z9+/FhrmZWVFby8vKTLm11dXVG7dm388ssvWkHm/PnziI6ORps2baQ2T09PJCUl4ezZs1JbXFwcNm/eXKDtUCqV6NixI7Zt24bjx4/nWi7+d1VP165dcejQIezYsSNXn8TERGRlZb1yPW3atMGdO3dyXdGUkZGBn376CU5OTnj//fcBPN/Tk56ertXP09MT1tbWUo1y7ukiXrjqKDMzEz/88IPW++rWrQsHBwcsW7ZMa46rVq3KdT5Q165dcffuXSxbtizX/J89e4bU1NR8t69u3bpwcnLCkiVLtC5T//PPP3Hp0qVcVw516dIFRkZG+PXXX7F+/Xq0a9dO67Ckj48PPD09MXfuXKSkpORa34ufp+J26NAh6VwXALh9+za2bt2KgICAfO+t8+TJk1xXhOXsocqpT5s2bZCdnY2FCxdq9Zs/fz4UCgVat24NANJ/FyxYoNXv5SsAjYyM0KVLF2zcuDHP8P6qn0FTU1N4e3tDCJHvOUb07uGeHSIAY8eOxccff4zIyEgMHjwYixYtQuPGjVGzZk0MHDgQFStWxP3793Ho0CHcuXMHZ86cAQB4e3ujefPm8PHxgb29PY4fP44NGzZg6NCh0tjffPMNWrduDV9fX/Tv31+69NzW1lbrWUDdunXD+PHj0alTJwwfPhxpaWlYvHgxKleurPUL6lVmzZqF6OhoNGvWTLrkOi4uDuvXr8eBAwdgZ2eHsWPH4rfffkO7du3Qp08f+Pj4IDU1FefOncOGDRtw48YNODo65ruOQYMGYfny5fj444/Rr18/1KlTB48fP8batWtx/vx5rFixAqampgCe7zXw9/dH165d4e3tDWNjY2zevBn3799Ht27dAACNGjVCqVKlEBISguHDh0OhUOC///1vrl+wpqammDZtGoYNG4YPPvgAXbt2xY0bNxAZGQlPT0+tPTi9evXCunXrMHjwYOzZswd+fn7Izs7G33//jXXr1mHHjh15nsQNPN+jN3v2bPTt2xfNmjVD9+7dpUvPK1SogLCwMK3+Tk5OaNGiBebNm4enT5/ik08+0VquVCrx008/oXXr1qhevTr69u2LMmXK4O7du9izZw9sbGywbdu2An1/87N///5coRJ4fqjnxT1eNWrUQGBgoNal5wBeebjnl19+wQ8//IBOnTrB09MTT58+xbJly2BjYyOF9fbt26NFixb4/PPPcePGDdSqVQvR0dHYunUrRo4cKe3dql27Nrp3744ffvgBSUlJaNSoEXbt2oWrV6/mWu/XX3+NPXv2oEGDBhg4cCC8vb2RkJCAkydPYufOnUhISAAABAQEwMXFBX5+fnB2dsalS5ewcOFCtG3bNtc5RPQOK6nLwIgMLeeS2rwuoc3Ozhaenp7C09NTZGVlCSGEuHbtmujdu7dwcXERJiYmokyZMqJdu3Ziw4YN0vu++uorUb9+fWFnZyfMzc1F1apVxcyZM0VmZqbW+Dt37hR+fn7C3Nxc2NjYiPbt24uLFy/mmkd0dLSoUaOGMDU1FVWqVBErV67M99Lz0NDQPLfz5s2bonfv3tIl9RUrVhShoaEiIyND6vP06VMxceJE4eXlJUxNTYWjo6No1KiRmDt3bq655+XJkyciLCxMeHh4CBMTE2FjYyNatGgh/vzzT61+jx49EqGhoaJq1arC0tJS2NraigYNGoh169Zp9Tt48KBo2LChMDc3F25ubmLcuHFix44dAoDYs2ePVt8FCxaI8uXLC5VKJerXry8OHjwofHx8RFBQkFa/zMxMMXv2bFG9enWhUqlEqVKlhI+Pj5g+fbpISkp67TauXbtW1KlTR6hUKmFvby+Cg4PFnTt38uy7bNkyAUBYW1trXa7+olOnTonOnTsLBwcHoVKpRPny5UXXrl3Frl27pD453+uHDx++dn5CvP7S8xcv5c75zKxcuVJUqlRJqFQqUadOnVz1ffnS85MnT4ru3buLcuXKCZVKJZycnES7du20LmEX4vlnKiwsTLi5uQkTExNRqVIl8c0332hdbi+EEM+ePRPDhw8XDg4OwtLSUrRv317cvn0713yFEOL+/fsiNDRUuLu7CxMTE+Hi4iL8/f3F0qVLpT4//vijaNq0qVRXT09PMXbs2AJ9j+ndoRCiAHesIiJ6Q2k0GpQuXRqdO3fO87AVPadQKBAaGprrUBPRu4Dn7BDRWyM9PT3X4a0VK1YgISEh1+MiiIhy8JwdInprHD58GGFhYfj444/h4OCAkydP4ueff0aNGjXw8ccfl/T0iOgNxbBDRG+NChUqwN3dHQsWLEBCQgLs7e3Ru3dvfP3119JJ0UREL+M5O0RERCRrPGeHiIiIZI1hh4iIiGSN5+zg+aWr9+7dg7W1tc63liciIqKSIYTA06dP4ebmBqUy//03DDsA7t27p7cHAxIREZFh3b59G2XLls13OcMOIN1S/Pbt27CxsdHbuGq1GtHR0QgICICJiYnexqXcWGvDYa0Nh7U2LNbbcPRV6+TkZLi7u7/20SAMO4B06MrGxkbvYcfCwgI2Njb8wSlmrLXhsNaGw1obFuttOPqu9etOQeEJykRERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGt8EGhxEQLITIVRdgaQmQoIPlSuWKnVrLWhsNaGw1obFutdvEwsgNc8sLO4lGjYCQ8Px6ZNm/D333/D3NwcjRo1wuzZs1GlShWpT/PmzbFv3z6t93366adYsmSJ9PrWrVsYMmQI9uzZAysrK4SEhCA8PBzGxiW4eeo0mHxTHu0A4GzJTeNdYQKw1gbCWhsOa21YrHcxm3QPMLUskVWXaNjZt28fQkNDUa9ePWRlZWHSpEkICAjAxYsXYWn5/wUZOHAgZsyYIb22sLCQ/j87Oxtt27aFi4sL/vrrL8TFxaF3794wMTHBrFmzDLo9RERE9OYp0bCzfft2rdeRkZFwcnLCiRMn0LRpU6ndwsICLi4ueY4RHR2NixcvYufOnXB2dkbt2rXx5ZdfYvz48Zg2bRpMTU2LdRvyZWIB9dib2LEjGoGBATAx4S7R4qRWq1lrA2GtDYe1NizWu5iZWLy+TzF5o87ZSUpKAgDY29trta9atQorV66Ei4sL2rdvj8mTJ0t7dw4dOoSaNWvC2dlZ6h8YGIghQ4bgwoULqFOnjuE24EUKBWBqiWwj1fPddvzBKV4KNWttKKy14bDWhsV6y9YbE3Y0Gg1GjhwJPz8/1KhRQ2rv0aMHypcvDzc3N5w9exbjx4/H5cuXsWnTJgBAfHy8VtABIL2Oj4/Pc10ZGRnIyMiQXicnJwN4nurVarXetilnLH2OSXljrQ2HtTYc1tqwWG/D0VetC/r+NybshIaG4vz58zhw4IBW+6BBg6T/r1mzJlxdXeHv749r167B09OzUOsKDw/H9OnTc7VHR0drnQ+kLzExMXofk/LGWhsOa204rLVhsd6GU9Rap6WlFajfGxF2hg4diqioKMTGxqJs2bKv7NugQQMAwNWrV+Hp6QkXFxccPXpUq8/9+/cBIN/zfCZOnIhRo0ZJr5OTk+Hu7o6AgADY2NgUZVO0qNVqxMTEoFWrVjz+W8xYa8NhrQ2HtTYs1ttw9FXrnCMzr1OiYUcIgWHDhmHz5s3Yu3cvPDw8Xvue06dPAwBcXV0BAL6+vpg5cyYePHgAJycnAM+Too2NDby9vfMcQ6VSQaVS5Wo3MTEplg94cY1LubHWhsNaGw5rbVist+EUtdYFfW+Jhp3Q0FCsXr0aW7duhbW1tXSOja2tLczNzXHt2jWsXr0abdq0gYODA86ePYuwsDA0bdoU7733HgAgICAA3t7e6NWrF+bMmYP4+Hh88cUXCA0NzTPQEBER0bulRB8XsXjxYiQlJaF58+ZwdXWVvtauXQsAMDU1xc6dOxEQEICqVati9OjR6NKlC7Zt2yaNYWRkhKioKBgZGcHX1xc9e/ZE7969te7LQ0RERO+uEj+M9Sru7u657p6cl/Lly+OPP/7Q17SIiIhIRvggUCIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKStRINO+Hh4ahXrx6sra3h5OSEjh074vLly1p90tPTERoaCgcHB1hZWaFLly64f/++Vp9bt26hbdu2sLCwgJOTE8aOHYusrCxDbgoRERG9oUo07Ozbtw+hoaE4fPgwYmJioFarERAQgNTUVKlPWFgYtm3bhvXr12Pfvn24d+8eOnfuLC3Pzs5G27ZtkZmZib/++gu//PILIiMjMWXKlJLYJCIiInrDGJfkyrdv3671OjIyEk5OTjhx4gSaNm2KpKQk/Pzzz1i9ejU++OADAEBERASqVauGw4cPo2HDhoiOjsbFixexc+dOODs7o3bt2vjyyy8xfvx4TJs2DaampiWxaURERPSGeKPO2UlKSgIA2NvbAwBOnDgBtVqNli1bSn2qVq2KcuXK4dChQwCAQ4cOoWbNmnB2dpb6BAYGIjk5GRcuXDDg7ImIiOhNVKJ7dl6k0WgwcuRI+Pn5oUaNGgCA+Ph4mJqaws7OTquvs7Mz4uPjpT4vBp2c5TnL8pKRkYGMjAzpdXJyMgBArVZDrVbrZXtyxnvxv1R8WGvDYa0Nh7U2LNbbcPRV64K+/40JO6GhoTh//jwOHDhQ7OsKDw/H9OnTc7VHR0fDwsJC7+uLiYnR+5iUN9bacFhrw2GtDYv1Npyi1jotLa1A/d6IsDN06FBERUUhNjYWZcuWldpdXFyQmZmJxMRErb079+/fh4uLi9Tn6NGjWuPlXK2V0+dlEydOxKhRo6TXycnJcHd3R0BAAGxsbPS1WVCr1YiJiUGrVq1gYmKit3EpN9bacFhrw2GtDYv1Nhx91TrnyMzrlGjYEUJg2LBh2Lx5M/bu3QsPDw+t5T4+PjAxMcGuXbvQpUsXAMDly5dx69Yt+Pr6AgB8fX0xc+ZMPHjwAE5OTgCeJ0UbGxt4e3vnuV6VSgWVSpWr3cTEpFg+4MU1LuXGWhsOa204rLVhsd6GU9RaF/S9JRp2QkNDsXr1amzduhXW1tbSOTa2trYwNzeHra0t+vfvj1GjRsHe3h42NjYYNmwYfH190bBhQwBAQEAAvL290atXL8yZMwfx8fH44osvEBoammegISIiondLiYadxYsXAwCaN2+u1R4REYE+ffoAAObPnw+lUokuXbogIyMDgYGB+OGHH6S+RkZGiIqKwpAhQ+Dr6wtLS0uEhIRgxowZhtoMIiIieoOV+GGs1zEzM8OiRYuwaNGifPuUL18ef/zxhz6nRkRERDLxRt1nh4iIiEjfGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWChV2srKysHPnTvz44494+vQpAODevXtISUnR6+SIiIiIispY1zfcvHkTQUFBuHXrFjIyMtCqVStYW1tj9uzZyMjIwJIlS4pjnkRERESFovOenREjRqBu3bp48uQJzM3NpfZOnTph165dep0cERERUVHpvGdn//79+Ouvv2BqaqrVXqFCBdy9e1dvEyMiIiLSB5337Gg0GmRnZ+dqv3PnDqytrfUyKSIiIiJ90TnsBAQE4LvvvpNeKxQKpKSkYOrUqWjTpo0+50ZERERUZDofxvr2228RGBgIb29vpKeno0ePHrhy5QocHR3x66+/FscciYiIiApN57BTtmxZnDlzBmvWrMHZs2eRkpKC/v37Izg4WOuEZSIiIqI3gc5hBwCMjY3Rs2dPfc+FiIiISO90Dju//fZbnu0KhQJmZmbw8vKCh4dHkSdGREREpA86h52OHTtCoVBACKHVntOmUCjQuHFjbNmyBaVKldLbRImIiIgKQ+ersWJiYlCvXj3ExMQgKSkJSUlJiImJQYMGDRAVFYXY2Fg8fvwYY8aMKY75EhEREelE5z07I0aMwNKlS9GoUSOpzd/fH2ZmZhg0aBAuXLiA7777Dv369dPrRImIiIgKQ+c9O9euXYONjU2udhsbG/z7778AgEqVKuHRo0dFnx0RERFREekcdnx8fDB27Fg8fPhQanv48CHGjRuHevXqAQCuXLkCd3d3/c2SiIiIqJB0Poz1888/o0OHDihbtqwUaG7fvo2KFSti69atAICUlBR88cUX+p0pERERUSHoHHaqVKmCixcvIjo6Gv/884/U1qpVKyiVz3cUdezYUa+TJCIiIiqsQt1UUKlUIigoCEFBQfqeDxEREZFeFSrspKamYt++fbh16xYyMzO1lg0fPlwvEyMiIiLSB53DzqlTp9CmTRukpaUhNTUV9vb2ePToESwsLODk5MSwQ0RERG8Una/GCgsLQ/v27fHkyROYm5vj8OHDuHnzJnx8fDB37tzimCMRERFRoekcdk6fPo3Ro0dDqVTCyMgIGRkZcHd3x5w5czBp0qTimCMRERFRoekcdkxMTKSrrpycnHDr1i0AgK2tLW7fvq3f2REREREVkc7n7NSpUwfHjh1DpUqV0KxZM0yZMgWPHj3Cf//7X9SoUaM45khERERUaDrv2Zk1axZcXV0BADNnzkSpUqUwZMgQPHz4EEuXLtX7BImIiIiKQuewU7duXbRo0QLA88NY27dvR3JyMk6cOIFatWrpNFZsbCzat28PNzc3KBQKbNmyRWt5nz59oFAotL5evrdPQkICgoODYWNjAzs7O/Tv3x8pKSm6bhYRERHJlM5h59mzZ0hLS5Ne37x5E9999x2io6N1Xnlqaipq1aqFRYsW5dsnKCgIcXFx0tevv/6qtTw4OBgXLlxATEwMoqKiEBsbi0GDBuk8FyIiIpInnc/Z6dChAzp37ozBgwcjMTER9evXh6mpKR49eoR58+ZhyJAhBR6rdevWaN269Sv7qFQquLi45Lns0qVL2L59O44dO4a6desCAL7//nu0adMGc+fOhZubW8E3jIiIiGRJ5z07J0+eRJMmTQAAGzZsgIuLC27evIkVK1ZgwYIFep/g3r174eTkhCpVqmDIkCF4/PixtOzQoUOws7OTgg4AtGzZEkqlEkeOHNH7XIiIiOjto/OenbS0NFhbWwMAoqOj0blzZyiVSjRs2BA3b97U6+SCgoLQuXNneHh44Nq1a5g0aRJat26NQ4cOwcjICPHx8XByctJ6j7GxMezt7REfH5/vuBkZGcjIyJBeJycnAwDUajXUarXe5p8zlj7HpLyx1obDWhsOa21YrLfh6KvWBX2/zmHHy8sLW7ZsQadOnbBjxw6EhYUBAB48eAAbGxtdh3ulbt26Sf9fs2ZNvPfee/D09MTevXvh7+9f6HHDw8Mxffr0XO3R0dGwsLAo9Lj5iYmJ0fuYlDfW2nBYa8PRV62NjQv1OMR3irGxMfbs2VPS03gnFKTW2dnZEELku/zFc4hfuS6dZgZgypQp6NGjB8LCwuDv7w9fX18Az4NCnTp1dB1OJxUrVoSjoyOuXr0Kf39/uLi44MGDB1p9srKykJCQkO95PgAwceJEjBo1SnqdnJwMd3d3BAQE6DWwqdVqxMTEoFWrVjAxMdHbuJQba204rLXh6KvWmZmZuH37NjQajR5nJz9CCKSnp8PMzAwKhaKkpyNrutTaxsYGTk5OefbLOTLzOjqHnY8++giNGzdGXFyc1qXm/v7+6NSpk67D6eTOnTt4/PixdJ8fX19fJCYm4sSJE/Dx8QEA7N69GxqNBg0aNMh3HJVKBZVKlavdxMSkWP7xLq5xKTfW2nBYa8MpSq2FELh37x6MjY3h5uYm3QGfctNoNEhJSYGVlRXrVMwKUmshBNLS0vDgwQMYGRlJv/tfVNCfi0Lt03Rxccm156R+/fo6j5OSkoKrV69Kr69fv47Tp0/D3t4e9vb2mD59Orp06QIXFxdcu3YN48aNg5eXFwIDAwEA1apVQ1BQEAYOHIglS5ZArVZj6NCh6NatG6/EIiLC873daWlpcHNzK5bD9HKi0WiQmZkJMzMzhp1iVtBam5ubA3h+qoyTkxOMjIwKtb4Ch506derkuQvJ1tYWlStXxsiRI1GtWjWdVn78+HHpBoUApENLISEhWLx4Mc6ePYtffvkFiYmJcHNzQ0BAAL788kutvTKrVq3C0KFD4e/vD6VSiS5duhTLVWFERG+j7OxsAICpqWkJz4SocHJCulqtLv6w07FjxzzbExMTcfLkSdSuXRu7d++Gn59fgVfevHnzV554tGPHjteOYW9vj9WrVxd4nURE7yKeg0JvK318dgscdqZOnfrK5Z9//jmmTJmCXbt2FXlSRERERPqit4OSPXr0wLlz5/Q1HBER0Tvn8uXLcHFxwdOnTws9xsWLF1G2bFmkpqbqcWZvN72FHSMjI17WSEREepHzIOjBgwfnWhYaGgqFQoE+ffoYfmLFbOLEiRg2bJh0894bN26gadOmsLS0RNOmTXHjxg2t/u3atcPGjRu12ry9vdGwYUPMmzfPUNN+4+kt7GzatAne3t76Go6IiN5x7u7uWLNmDZ49eya1paenY/Xq1ShXrlwJzix/QghkZWUV6r23bt1CVFSUVogbPXo0ypQpg9OnT8PV1RVjxoyRlq1du1a6MOdlffv2xeLFiws9F7kpcNhZsGBBnl9ffvklOnbsiKlTp2LKlCnFOVciInqHvP/++3B3d8emTZuktk2bNqFcuXK5bmKr0WgQHh4ODw8PmJubo1atWtiwYYO0fO/evVAoFNixYwfq1KkDc3NzfPDBB3jw4AH+/PNPVKtWDXZ2dhgwYIDWXXkzMjIwfPhwODk5wczMDI0bN8axY8dyjfvnn3/Cx8cHKpUKK1euhFKpxPHjx7Xm+N1336F8+fL5HgVZt24datWqhTJlykhtly5dQkhICCpVqoQ+ffrg0qVLAJ5fHPTFF19g0aJFeY7VqlUrJCQkYN++fa8r8zuhwCcoz58/P892GxsbVKlSBbGxsdLdlImI6M0khMAzdXaJrNvcxEjnK2v69euHiIgIBAcHAwCWL1+Ovn37Yu/evVr9wsPDsXLlSixZsgSVKlVCbGwsevbsidKlS6NZs2ZSv2nTpmHhwoWwsLBA165d0bVrV6hUKqxevRrJycno3LkzFi5ciAkTJgAAxo0bh40bN+KXX35B+fLlMWfOHAQGBuLq1auwt7eXxp0wYQLmzp2LihUrolSpUmjZsiUiIiK0HlQdERGBPn365Htfmf3792v1B4BatWph586dCAgIQHR0NN577z0AwNixYxEaGgp3d/c8xzI1NUXt2rWxf//+Ij1eSS4KHHauX79enPMgIiIDeKbOhveU19/WozhcnBEIC1Pd7mXbs2dPTJw4UXrQ9MGDB7FmzRqtsJORkYFZs2Zh586d0h/dFStWxIEDB/Djjz9qhZ2vvvpKukVK//79MXHiRFy7dg0VK1aERqPBhx9+iD179mDChAlITU3F4sWLERkZidatWwMAli1bhpiYGPz8888YO3asNO6MGTPQqlUr6fWAAQMwePBgzJs3DyqVCidPnsS5c+ewdevWfLf15s2bucLO3Llz8emnn6JChQp477338OOPPyI2NhanT5/G7Nmz0bVrVxw/fhwBAQFYsGCB1v2U3Nzc9P6A7rcVnwpHRERvrNKlS6Nt27aIjIyEEAJt27aFo6OjVp+rV68iLS1NK2wAz58J9vLhrpw9IwDg7OwMCwsLVKxYUWpzcnLCmTNnAADXrl2DWq3Wun+ciYkJ6tevLx1OyvFySOnYsSNCQ0OxefNmdOvWDZGRkWjRogUqVKiQ77Y+e/YMZmZmWm1lypRBVFSU9DojIwOBgYH45Zdf8NVXX8Ha2hqXL19GUFAQfvzxRwwbNkzqa25uXuAHZcodww4R0TvE3MQIF2cElti6C6Nfv34YOnQoAOR5jkpKSgoA4Pfff9c63wVArucgvvgsJYVCkevZSgqFolBXFltaWmq9NjU1Re/evREREYHOnTtj9erV+M9//vPKMRwdHfHkyZNX9pk1axYCAgLg4+ODgQMH4quvvoKJiQk6d+6M3bt3a4WdhIQEeHp66rwtcsSwQ0T0DlEoFDofSippQUFByMzMhEKhkJ6N+CJvb2+oVCrcunVL65BVUXl6esLU1BQHDx5E+fLlATx/ZMGxY8cwcuTI175/wIABqFGjBn744QdkZWWhc+fOr+xfp04dXLx4Md/lly5dwurVq3H69GkAzx8FolarpXnlPBokx/nz5/HRRx+9dp7vgrfrE09ERO8cIyMj6bBRXs9Gsra2xpgxYxAWFgaNRoPGjRsjKSkJBw8ehI2NDUJCQgq1XktLSwwZMgRjx46Fvb09ypUrhzlz5iAtLQ39+/d/7furVauGhg0bYvz48ejXr5/0UMv8BAYGYsCAAcjOzs61nUIIDBo0CPPnz5f2Ivn5+WHZsmWoXLkyVqxYge7du0v9b9y4gbt376Jly5aF2HL54WNdiYjojWdjYwMbG5t8l3/55ZeYPHkywsPDUa1aNQQFBeH333+Hh4dHkdb79ddfo0uXLujVqxfef/99XL16FTt27ECpUqUK9P7+/fsjMzMT/fr1e23f1q1bw9jYGDt37sy1bOnSpXB2dka7du2ktmnTpiE9PR0NGjSAl5cXQkNDpWW//vorAgICpD1S7zxRCLGxsSI4OFg0bNhQ3LlzRwghxIoVK8T+/fsLM1yJS0pKEgBEUlKSXsfNzMwUW7ZsEZmZmXodl3JjrQ2HtTYcfdT62bNn4uLFi+LZs2d6nJk8ZWdniydPnojs7Gy9jTljxgxRs2bNAvdfuHChCAgIKNI6MzIyRLly5cSBAweKNE5x0qXWr/oMF/T3t857djZu3IjAwECYm5vj1KlTyMjIAAAkJSVh1qxZeo5iREREb5+UlBScP38eCxcu1Dpp+HU+/fRTNG3atEjPxrp16xYmTZqkdRXZu07nsPPVV19hyZIlWLZsmdZZ7H5+fjh58qReJ0dERPQ2Gjp0KHx8fNC8efMCHcLKYWxsjM8//1x6NlZheHl54dNPPy30++VI5xOUL1++jKZNm+Zqt7W1RWJioj7mRERE9FaLjIxEZGRkSU+D/kfnPTsuLi64evVqrvYDBw5o3ZiJiIiI6E2gc9gZOHAgRowYgSNHjkChUODevXtYtWoVxowZgyFDhhTHHImIiIgKTefDWBMmTIBGo4G/vz/S0tLQtGlTqFQqjBkzRqeTsIiIiIgMQeewo1Ao8Pnnn2Ps2LG4evUqUlJS4O3tDSsrq+KYHxEREVGR6Bx2kpKSkJ2dDXt7e3h7e0vtCQkJMDY2fuVNn4iIiIgMTedzdrp164Y1a9bkal+3bh26deuml0kRERER6YvOYefIkSNo0aJFrvbmzZvjyJEjepkUERERlZzJkydj0KBBxbqOx48fw8XFBXfu3CnW9QCFCDsZGRnIysrK1a5Wq/Hs2TO9TIqIiCg+Ph4jRoyAl5cXzMzM4OzsDD8/PyxevBhpaWlSvwoVKkChUEChUMDS0hLvv/8+1q9fLy3v06cPOnbsmGv8vXv3QqFQvPIecTnjHj58WKs9IyMDDg4OUCgU2Lt3b1E39Y0SHx+P//znP/j888+ltj59+kChUGDw4MG5+oeGhkKhUKBPnz65+ud8OTg4ICgoCGfPnpX6ODg4oFevXpg6dWqxbg9QiLBTv359LF26NFf7kiVL4OPjo5dJERHRu+3ff/9FnTp1EB0djVmzZuHUqVM4dOgQxo0bh6ioqFwPy5wxYwbi4uJw6tQp1KtXD5988gn++usvvczF3d0dERERWm2bN29+oy/MyczMLPR7f/rpJzRq1CjXQ0Td3d2xZs0arR0b6enpWL16NcqVK5drnKCgIMTFxSEuLg67du2CsbGx1oNMgeehaNWqVUhISCj0fAuiUI+L+Omnn9C0aVNMnz4d06dPR9OmTbF8+XI+G4uIiPTis88+g7GxMY4fP46uXbuiWrVqqFixIjp06IDff/8d7du31+pvbW0NFxcXVK5cGYsWLYK5uTm2bduml7mEhITk+iW/fPlyhISE5Op7+/ZtdO3aFXZ2drC3t0eHDh1w48YNaXnOXqZZs2bB2dkZdnZ2mDFjBrKysjB27FjY29ujbNmyucLVuXPn8MEHH8Dc3BwODg4YNGgQUlJSco07c+ZMuLm5oUqVKpgxYwZq1KiRa461a9fG5MmT893eNWvW5KovALz//vtwd3fHpk2bpLZNmzahXLlyqFOnTq7+KpUKLi4ucHFxQe3atTFhwgTcvn0bDx8+lPpUr14dbm5u2Lx5c77z0Qedw46fnx8OHz4Md3d3rFu3Dtu2bYOXlxfOnj2LJk2aFMcciYhIX4QAMlNL5kuIAk3x8ePHiI6ORmhoKCwtLfPso1Ao8n2/sbExTExMirR340U+Pj6oUKECNm7cCOD5gzZjY2PRq1cvrX5qtRqBgYGwtrbG/v37cfDgQVhZWSEoKEhrLrt378a9e/cQGxuLefPmYerUqWjXrh1KlSqFI0eOYPDgwfj000+lc1lSU1MRGBiIUqVK4dixY1i/fj127tyJoUOHaq1/165duHz5MmJiYhAVFYV+/frh0qVLOHbsmNTn1KlTOHv2LPr27ZvntiYkJODixYuoW7dunsv79eunFcSWL1+e71gvSklJwcqVK+Hl5QUHBwetZfXr18f+/ftfO0ZR6HTpuVqtxqefforJkydj1apVxTUnIiIqLuo0YJZbyax70j3ANO/w8qKrV69CCIEqVapotTs6OiI9PR3A8/NEZs+eneu9mZmZ+Pbbb5GUlIQPPvhAP/PG81/yy5cvR8+ePREZGYk2bdqgdOnSWn3Wrl0LjUaDn376SQpjERERsLOzw969exEQEAAAsLe3x4IFC6BUKlGlShXMmTMHaWlpmDRpEgBg4sSJ+Prrr3HgwAF069YNq1evRnp6OlasWCGFv4ULF6J9+/aYPXs2nJ2dAQCWlpb46aefYGpqKs0pMDAQERERqFevnjSfZs2a5ft4p1u3bkEIATe3vD8jPXv2xMSJE3Hz5k0AwMGDB7FmzZo8z1uKioqSDvWlpqbC1dUVUVFRUCqV0Gg0Uj83NzecOnXqFdUvOp327JiYmEjJloiIyJCOHj2K06dPo3r16sjIyNBaNn78eFhZWcHCwgKzZ8/G119/jbZt2+pt3T179sShQ4fw77//IjIyMs8nmZ85cwZXr16FtbU1rKysYGVlBXt7e6Snp+PatWtSv+rVq0Op/P9fv87OzqhZs6b02sjICA4ODnjw4AEA4NKlS6hVq5bWXi4/Pz9oNBpcvnxZaqtZs6ZW0AGeP+Lp119/RXp6OjIzM7F69epXPoU951CdmZlZnstLly6Ntm3bIjIyEhEREWjbti0cHR3z7NuiRQucPn0ap0+fxtGjRxEYGIjWrVtLQSmHubm51gnnxUHnmwp27NgRW7ZsQVhYWHHMh4iIipOJxfM9LCW17gLw8vKCQqHQ+kUOQNobYW5unus9Y8eORZ8+fWBlZQVnZ2etw1w2Nja5fsECQGJiIoyMjPI9VPYiBwcHtGvXDv3790d6ejpat26Np0+favVJSUmBj49Pnkc+XtwLZGJiorVMoVDk2fbi3o+CyGs72rdvD5VKhc2bN8PU1BRqtRofffRRvmPkBJcnT57k2nOVo1+/ftIhtEWLFr1yPl5eXtLrn376Cba2tli2bBlmzJghtSckJOS7Ln3ROexUqlQJM2bMwMGDB+Hj45OruMOHD9fb5IiISM8UigIdSipJDg4OaNWqFRYuXIhhw4YVKIw4Ojpq/WJ9UZUqVbBmzRpkZGRApVJJ7SdPnoSHh0euoJGffv36oU2bNhg/fjyMjIxyLX///fexdu1aODk56fVpAtWqVUNkZCRSU1OlWhw8eFA6DPYqxsbGCAkJQUREBExNTdGtW7c8w2IOT09P2NjY4OLFi6hcuXKefXLOQVIoFAgMDCzwdigUCiiVyly3qTl//jyaN29e4HEKQ+ew8/PPP8POzg4nTpzAiRMntJYpFAqGHSIiKrIffvgBfn5+qFu3LqZNm4b33nsPSqUSx44dw99//63TrU6Cg4MxY8YM9O7dG+PGjYOtrS1iY2Px3XffYc6cOQUeJygoCA8fPsw3yAQHB+Obb75Bhw4dMGPGDJQtWxY3b97Epk2bMG7cOJQtW7bA63p53KlTpyIkJATTpk3Dw4cPMWzYMPTq1Us6X+dVBgwYgGrVqgF4HpJeRalUomXLljhw4ECe9yYCnh9mu3TpkvT/+cnIyEB8fDyA53uKFi5ciJSUFK0rvdLS0nDixIliv5pb57Bz/fr14pgHERGRxNPTE6dOncKsWbMwceJE3LlzByqVCt7e3hgzZgw+++yzAo9lZ2eH/fv3Y8KECfjwww+RlJQELy8vzJs3D/379y/wOAqFIt/zUwDAwsICsbGxGD9+PDp37oynT5+iTJky8Pf3L9KeHgsLC+zYsQMjRoxAvXr1YGFhgS5dumDevHkFen+lSpXQqFEjJCQkoEGDBq/tP2DAAAwcOBBz5szROrfoRQXZnu3bt8PV1RXA81sDVK1aFevXr0fz5s2lQ3Rbt25FuXLliv1qboUQBbwWUMaSk5Nha2uLpKQkve56VKvV+OOPP9CmTZsC7yalwmGtDYe1Nhx91Do9PR3Xr1+Hh4dHvied0nMajQbJycmwsbHJ95f820gIgUqVKuGzzz7DqFGjCtS/QYMGCAsLQ/fu3YtlTjm1DgoKwvDhw9GjR498+77qM1zQ398679l51VncwPNr7omIiKjkPXz4EGvWrEF8fHyB7ocDPN+DtXTpUpw7d65Y5/b48WN06tSp2ALVi3QOO0+ePNF6rVarcf78eSQmJur1ngZERERUNE5OTnB0dMTSpUtRqlSpAr+vdu3aqF27dvFNDM9PRB87duwrbxCpLzqHnbxu6azRaDBkyBB4enrqZVJERERUdDxT5Tm9HJRUKpUYNWoU5s+fr4/hiIiIiPRGb2dgXbt2DVlZWfoajoiI9Ih/4dPbSh+fXZ0PY718JrcQAnFxcfj999/zfAIsERGVnJz7oGRmZr7yZnJEb6qcR0kU5epPncPOyw/rUiqVKF26NL799tvXXqlFRESGZWxsDAsLCzx8+BAmJiayuqRa3zQaDTIzM5Gens46FbOC1FoIgbS0NDx48AB2dnavvIHh6+gcdvbs2VPolRERkWEpFAq4urri+vXreT4fiv6fEALPnj2Dubm5Qa4QepfpUms7Ozu4uLgUaX06h50cDx8+lB7SVqVKlWJ/iBcRERWOqakpKlWqhMzMzJKeyhtNrVYjNjYWTZs25Q0zi1lBa21iYlKkPTo5dA47qampGDZsGFasWCHd7tnIyAi9e/fG999/DwuLgj3VloiIDEepVPIOyq9hZGSErKwsmJmZMewUM0PXWueDkqNGjcK+ffuwbds2JCYmIjExEVu3bsW+ffswevTo4pgjERERUaHpvGdn48aN2LBhg9bj2Nu0aQNzc3N07doVixcv1uf8iIiIiIpE5z07aWlpeT5S3snJSbo8jIiIiOhNoXPY8fX1xdSpU5Geni61PXv2DNOnT4evr69eJ0dERERUVDofxvrPf/6DwMBAlC1bFrVq1QIAnDlzBmZmZtixY4feJ0hERERUFDqHnRo1auDKlStYtWoV/v77bwBA9+7dERwczLtzEhER0RunUPfZsbCwwMCBA/U9FyIiIiK90/mcnV9++QW///679HrcuHGws7NDo0aNeHdOIiIieuPoHHZmzZolHa46dOgQFi5ciDlz5sDR0RFhYWF6nyARERFRUeh8GOv27dvw8vICAGzZsgUfffQRBg0aBD8/P6177xARERG9CXTes2NlZYXHjx8DAKKjo9GqVSsAgJmZGZ49e6bf2REREREVkc57dlq1aoUBAwagTp06+Oeff9CmTRsAwIULF1ChQgV9z4+IiIioSHTes7No0SL4+vri4cOH2LhxIxwcHAAAJ06cQPfu3fU+QSIiIqKi0HnPjp2dHRYuXJirffr06XqZEBEREZE+6bxnBwD279+Pnj17olGjRrh79y4A4L///S8OHDig18kRERERFZXOYWfjxo0IDAyEubk5Tp48iYyMDABAUlISZs2apdNYsbGxaN++Pdzc3KBQKLBlyxat5UIITJkyBa6urjA3N0fLli1x5coVrT4JCQkIDg6GjY0N7Ozs0L9/f6SkpOi6WURERCRTOoedr776CkuWLMGyZctgYmIitfv5+eHkyZM6jZWamopatWph0aJFeS6fM2cOFixYgCVLluDIkSOwtLREYGCg1kNIg4ODceHCBcTExCAqKgqxsbEYNGiQrptFREREMqXzOTuXL19G06ZNc7Xb2toiMTFRp7Fat26N1q1b57lMCIHvvvsOX3zxBTp06AAAWLFiBZydnbFlyxZ069YNly5dwvbt23Hs2DHUrVsXAPD999+jTZs2mDt3Ltzc3HTbOCIiIpIdncOOi4sLrl69musy8wMHDqBixYr6mheuX7+O+Ph4tGzZUmqztbVFgwYNcOjQIXTr1g2HDh2CnZ2dFHQAoGXLllAqlThy5Ag6deqU59gZGRnS4TcASE5OBgCo1Wqo1Wq9bUPOWPock/LGWhsOa204rLVhsd6Go69aF/T9OoedgQMHYsSIEVi+fDkUCgXu3buHQ4cOYcyYMZg8ebLOE81PfHw8AMDZ2Vmr3dnZWVoWHx8PJycnreXGxsawt7eX+uQlPDw8z6vHoqOjYWFhUdSp5xITE6P3MSlvrLXhsNaGw1obFuttOEWtdVpaWoH66Rx2JkyYAI1GA39/f6SlpaFp06ZQqVQYM2YMhg0bpvNES8LEiRMxatQo6XVycjLc3d0REBAAGxsbva1HrVYjJiYGrVq10jq/ifSPtTYc1tpwWGvDYr0NR1+1zjky8zo6hx2FQoHPP/8cY8eOxdWrV5GSkgJvb29YWVnh2bNn0kNCi8rFxQUAcP/+fbi6ukrt9+/fR+3ataU+Dx480HpfVlYWEhISpPfnRaVSQaVS5Wo3MTEplg94cY1LubHWhsNaGw5rbVist+EUtdYFfW+h7rMDAKampvD29kb9+vVhYmKCefPmwcPDo7DD5eLh4QEXFxfs2rVLaktOTsaRI0fg6+sLAPD19UViYiJOnDgh9dm9ezc0Gg0aNGigt7kQERHR26vAYScjIwMTJ05E3bp10ahRI+meOBEREfDw8MD8+fMRFham08pTUlJw+vRpnD59GsDzk5JPnz6NW7duQaFQYOTIkfjqq6/w22+/4dy5c+jduzfc3NzQsWNHAEC1atUQFBSEgQMH4ujRozh48CCGDh2Kbt268UosIiIiAqDDYawpU6bgxx9/RMuWLfHXX3/h448/Rt++fXH48GHMmzcPH3/8MYyMjHRa+fHjx9GiRQvpdc55NCEhIYiMjMS4ceOQmpqKQYMGITExEY0bN8b27dthZmYmvWfVqlUYOnQo/P39oVQq0aVLFyxYsECneRAREZF8FTjsrF+/HitWrMCHH36I8+fP47333kNWVhbOnDkDhUJRqJU3b94cQoh8lysUCsyYMQMzZszIt4+9vT1Wr15dqPUTERGR/BX4MNadO3fg4+MDAKhRowZUKhXCwsIKHXSIiIiIDKHAYSc7OxumpqbSa2NjY1hZWRXLpIiIiIj0pcCHsYQQ6NOnj3TJdnp6OgYPHgxLS0utfps2bdLvDImIiIiKoMBhJyQkROt1z5499T4ZIiIiIn0rcNiJiIgoznkQERERFYtC31SQiIiI6G3AsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREsvZGh51p06ZBoVBofVWtWlVanp6ejtDQUDg4OMDKygpdunTB/fv3S3DGRERE9KZ5o8MOAFSvXh1xcXHS14EDB6RlYWFh2LZtG9avX499+/bh3r176Ny5cwnOloiIiN40xiU9gdcxNjaGi4tLrvakpCT8/PPPWL16NT744AMAQEREBKpVq4bDhw+jYcOGhp4qERERvYHe+LBz5coVuLm5wczMDL6+vggPD0e5cuVw4sQJqNVqtGzZUupbtWpVlCtXDocOHXpl2MnIyEBGRob0Ojk5GQCgVquhVqv1NvecsfQ5JuWNtTYc1tpwWGvDYr0NR1+1Luj7FUIIUaQ1FaM///wTKSkpqFKlCuLi4jB9+nTcvXsX58+fx7Zt29C3b1+t0AIA9evXR4sWLTB79ux8x502bRqmT5+eq3316tWwsLDQ+3YQERGR/qWlpaFHjx5ISkqCjY1Nvv3e6LDzssTERJQvXx7z5s2Dubl5ocNOXnt23N3d8ejRo1cWS1dqtRoxMTFo1aoVTExM9DYu5cZaGw5rbTistWGx3oajr1onJyfD0dHxtWHnjT+M9SI7OztUrlwZV69eRatWrZCZmYnExETY2dlJfe7fv5/nOT4vUqlUUKlUudpNTEyK5QNeXONSbqy14bDWhsNaGxbrbThFrXVB3/vGX431opSUFFy7dg2urq7w8fGBiYkJdu3aJS2/fPkybt26BV9f3xKcJREREb1J3ug9O2PGjEH79u1Rvnx53Lt3D1OnToWRkRG6d+8OW1tb9O/fH6NGjYK9vT1sbGwwbNgw+Pr68kosIiIikrzRYefOnTvo3r07Hj9+jNKlS6Nx48Y4fPgwSpcuDQCYP38+lEolunTpgoyMDAQGBuKHH34o4VkTERHRm+SNDjtr1qx55XIzMzMsWrQIixYtMtCMiIiI6G3zVp2zQ0RERKQrhh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNeOSnoCcTY+6hHNXlNiVeg5mpkZQGRtBZayEykT5//9vrISRkRKZWRpkZGUjQ61BRpYGQojXjq9UKv43xvOxTI2VUCoUBtiyN092djbO31cg+dgdGBkZ6fReY6UCKhMlTI2U//uvEZTvSBkFgMxszf8+d9nIyNIgK/vVn72i1NpICa2fg+KstUYAGVnZ//vZer59Cij+9/P3/OfG1FiJN/lbnZWdhX+SFCj172MYGxX9n2tjI6XWv0FGCgUys7OR/r9/dzLz+rdHAa16qYyVMHrpmyYEpPfnfI40mtf/G1ZcBIAsjUCGOvt/3/vnc3udony26fXavucKW3OTElk3w04xOnD1MW4kKHEmIa6kp/KOMMLafy+W9CTeEay14Rhh0cUTJT2Jdwg/28WlYUV7hh05CvP3woFjp1CpqjfUmud/+bz8l0ZGVjayskWuvzZf/sspL1nZQvor6vl42SjADiFZEkIgPj4eLi4uUOi4dytbI6S//J9/j17/F6CcmP5vr2DOnkZjo1fv7ShqrV/ek1Rcn1nFC3skcvbcAXirvtdCCDx9+hTW1tY61zrXWBD/+zfj//ciZ2ny+LfnpfVoxP9/zzKzNUhXZyP7pb02CgX+t2f0//dYGytL9iwJYyOF1vff5DWfa6Bon216PUtVyUUOhp1i1KamC3BboE2j8jAxKZk0+65Qq9X4448/0KZNbda6mLHWhvP/tW7EWhsAP9vyxROUiYiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWjEt6Am8CIQQAIDk5Wa/jqtVqpKWlITk5GSYmJnodm7Sx1obDWhsOa21YrLfh6KvWOb+3c36P54dhB8DTp08BAO7u7iU8EyIiItLV06dPYWtrm+9yhXhdHHoHaDQa3Lt3D9bW1lAoFHobNzk5Ge7u7rh9+zZsbGz0Ni7lxlobDmttOKy1YbHehqOvWgsh8PTpU7i5uUGpzP/MHO7ZAaBUKlG2bNliG9/GxoY/OAbCWhsOa204rLVhsd6Go49av2qPTg6eoExERESyxrBDREREssawU4xUKhWmTp0KlUpV0lORPdbacFhrw2GtDYv1NhxD15onKBMREZGscc8OERERyRrDDhEREckaww4RERHJGsMOERERyRrDTjFatGgRKlSoADMzMzRo0ABHjx4t6Sm99cLDw1GvXj1YW1vDyckJHTt2xOXLl7X6pKenIzQ0FA4ODrCyskKXLl1w//79EpqxPHz99ddQKBQYOXKk1MY669fdu3fRs2dPODg4wNzcHDVr1sTx48el5UIITJkyBa6urjA3N0fLli1x5cqVEpzx2yk7OxuTJ0+Gh4cHzM3N4enpiS+//FLr2UqsdeHExsaiffv2cHNzg0KhwJYtW7SWF6SuCQkJCA4Oho2NDezs7NC/f3+kpKQUfXKCisWaNWuEqampWL58ubhw4YIYOHCgsLOzE/fv3y/pqb3VAgMDRUREhDh//rw4ffq0aNOmjShXrpxISUmR+gwePFi4u7uLXbt2iePHj4uGDRuKRo0aleCs325Hjx4VFSpUEO+9954YMWKE1M46609CQoIoX7686NOnjzhy5Ij4999/xY4dO8TVq1elPl9//bWwtbUVW7ZsEWfOnBEffvih8PDwEM+ePSvBmb99Zs6cKRwcHERUVJS4fv26WL9+vbCyshL/+c9/pD6sdeH88ccf4vPPPxebNm0SAMTmzZu1lhekrkFBQaJWrVri8OHDYv/+/cLLy0t07969yHNj2Ckm9evXF6GhodLr7Oxs4ebmJsLDw0twVvLz4MEDAUDs27dPCCFEYmKiMDExEevXr5f6XLp0SQAQhw4dKqlpvrWePn0qKlWqJGJiYkSzZs2ksMM669f48eNF48aN812u0WiEi4uL+Oabb6S2xMREoVKpxK+//mqIKcpG27ZtRb9+/bTaOnfuLIKDg4UQrLW+vBx2ClLXixcvCgDi2LFjUp8///xTKBQKcffu3SLNh4exikFmZiZOnDiBli1bSm1KpRItW7bEoUOHSnBm8pOUlAQAsLe3BwCcOHECarVaq/ZVq1ZFuXLlWPtCCA0NRdu2bbXqCbDO+vbbb7+hbt26+Pjjj+Hk5IQ6depg2bJl0vLr168jPj5eq962trZo0KAB662jRo0aYdeuXfjnn38AAGfOnMGBAwfQunVrAKx1cSlIXQ8dOgQ7OzvUrVtX6tOyZUsolUocOXKkSOvng0CLwaNHj5CdnQ1nZ2etdmdnZ/z9998lNCv50Wg0GDlyJPz8/FCjRg0AQHx8PExNTWFnZ6fV19nZGfHx8SUwy7fXmjVrcPLkSRw7dizXMtZZv/79918sXrwYo0aNwqRJk3Ds2DEMHz4cpqamCAkJkWqa178prLduJkyYgOTkZFStWhVGRkbIzs7GzJkzERwcDACsdTEpSF3j4+Ph5OSktdzY2Bj29vZFrj3DDr21QkNDcf78eRw4cKCkpyI7t2/fxogRIxATEwMzM7OSno7saTQa1K1bF7NmzQIA1KlTB+fPn8eSJUsQEhJSwrOTl3Xr1mHVqlVYvXo1qlevjtOnT2PkyJFwc3NjrWWMh7GKgaOjI4yMjHJdmXL//n24uLiU0KzkZejQoYiKisKePXtQtmxZqd3FxQWZmZlITEzU6s/a6+bEiRN48OAB3n//fRgbG8PY2Bj79u3DggULYGxsDGdnZ9ZZj1xdXeHt7a3VVq1aNdy6dQsApJry35SiGzt2LCZMmIBu3bqhZs2a6NWrF8LCwhAeHg6AtS4uBamri4sLHjx4oLU8KysLCQkJRa49w04xMDU1hY+PD3bt2iW1aTQa7Nq1C76+viU4s7efEAJDhw7F5s2bsXv3bnh4eGgt9/HxgYmJiVbtL1++jFu3brH2OvD398e5c+dw+vRp6atu3boIDg6W/p911h8/P79ct1D4559/UL58eQCAh4cHXFxctOqdnJyMI0eOsN46SktLg1Kp/avPyMgIGo0GAGtdXApSV19fXyQmJuLEiRNSn927d0Oj0aBBgwZFm0CRTm+mfK1Zs0aoVCoRGRkpLl68KAYNGiTs7OxEfHx8SU/trTZkyBBha2sr9u7dK+Li4qSvtLQ0qc/gwYNFuXLlxO7du8Xx48eFr6+v8PX1LcFZy8OLV2MJwTrr09GjR4WxsbGYOXOmuHLlili1apWwsLAQK1eulPp8/fXXws7OTmzdulWcPXtWdOjQgZdDF0JISIgoU6aMdOn5pk2bhKOjoxg3bpzUh7UunKdPn4pTp06JU6dOCQBi3rx54tSpU+LmzZtCiILVNSgoSNSpU0ccOXJEHDhwQFSqVImXnr/pvv/+e1GuXDlhamoq6tevLw4fPlzSU3rrAcjzKyIiQurz7Nkz8dlnn4lSpUoJCwsL0alTJxEXF1dyk5aJl8MO66xf27ZtEzVq1BAqlUpUrVpVLF26VGu5RqMRkydPFs7OzkKlUgl/f39x+fLlEprt2ys5OVmMGDFClCtXTpiZmYmKFSuKzz//XGRkZEh9WOvC2bNnT57/PoeEhAghClbXx48fi+7duwsrKythY2Mj+vbtK54+fVrkuSmEeOG2kUREREQyw3N2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdojorXXjxg0oFAqcPn262NbRp08fdOzYsdjGJ6Lix7BDRCWmT58+UCgUub6CgoIK9H53d3fExcWhRo0axTxTInqbGZf0BIjo3RYUFISIiAitNpVKVaD3GhkZ8UnURPRa3LNDRCVKpVLBxcVF66tUqVIAAIVCgcWLF6N169YwNzdHxYoVsWHDBum9Lx/GevLkCYKDg1G6dGmYm5ujUqVKWkHq3Llz+OCDD2Bubg4HBwcMGjQIKSkp0vLs7GyMGjUKdnZ2cHBwwLhx4/DyE3U0Gg3Cw8Ph4eEBc3Nz1KpVS2tORPTmYdghojfa5MmT0aVLF5w5cwbBwcHo1q0bLl26lG/fixcv4s8//8SlS5ewePFiODo6AgBSU1MRGBiIUqVK4dixY1i/fj127tyJoUOHSu//9ttvERkZieXLl+PAgQNISEjA5s2btdYRHh6OFStWYMmSJbhw4QLCwsLQs2dP7Nu3r/iKQERFU+RHiRIRFVJISIgwMjISlpaWWl8zZ84UQjx/yv3gwYO13tOgQQMxZMgQIYQQ169fFwDEqVOnhBBCtG/fXvTt2zfPdS1dulSUKlVKpKSkSG2///67UCqVIj4+XgghhKurq5gzZ460XK1Wi7Jly4oOHToIIYRIT08XFhYW4q+//tIau3///qJ79+6FLwQRFSues0NEJapFixZYvHixVpu9vb30/76+vlrLfH198736asiQIejSpQtOnjyJgIAAdOzYEY0aNQIAXLp0CbVq1YKlpaXU38/PDxqNBpcvX4aZmRni4uLQoEEDabmxsTHq1q0rHcq6evUq0tLS0KpVK631ZmZmok6dOrpvPBEZBMMOEZUoS0tLeHl56WWs1q1b4+bNm/jjjz8QExMDf39/hIaGYu7cuXoZP+f8nt9//x1lypTRWlbQk6qJyPB4zg4RvdEOHz6c63W1atXy7V+6dGmEhIRg5cqV+O6777B06VIAQLVq1XDmzBmkpqZKfQ8ePAilUokqVarA1tYWrq6uOHLkiLQ8KysLJ06ckF57e3tDpVLh1q1b8PLy0vpyd3fX1yYTkZ5xzw4RlaiMjAzEx8drtRkbG0snFq9fvx5169ZF48aNsWrVKhw9ehQ///xznmNNmTIFPj4+qF69OjIyMhAVFSUFo+DgYEydOhUhISGYNm0aHj58iGHDhqFXr15wdnYGAIwYMQJff/01KlWqhKpVq2LevHlITEyUxre2tsaYMWMQFhYGjUaDxo0bIykpCQcPHoSNjQ1CQkKKoUJEVFQMO0RUorZv3w5XV1ettipVquDvv/8GAEyfPh1r1qzBZ599BldXV/z666/w9vbOcyxTU1NMnDgRN27cgLm5OZo0aYI1a9YAACwsLLBjxw6MGDEC9erVg4WFBbp06YJ58+ZJ7x89ejTi4uIQEhICpVKJfv36oVOnTkhKSpL6fPnllyhdujTCw8Px77//ws7ODu+//z4mTZqk79IQkZ4ohHjpJhJERG8IhUKBzZs383ENRFQkPGeHiIiIZI1hh4iIiGSN5+wQ0RuLR9mJSB+4Z4eIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGTt/wCresZdy4ALOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import psutil\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment_ma_reward_distance_dynamic_notrandom import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env, alpha, gamma, epsilon):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def choose_action(self, agent_idx, state):\n",
    "        state = tuple(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_action = self.q_tables[agent_idx][state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    def learn(self, agent_idx, state, action, reward, next_state, case_base=None):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "        current_q = self.q_tables[agent_idx][state][action]\n",
    "        max_next_q = max(self.q_tables[agent_idx][next_state])\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_tables[agent_idx][state][action] = new_q\n",
    "\n",
    "\n",
    "class Case:\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=0.5, total_time_steps=0):\n",
    "        self.problem = ast.literal_eval(problem) if isinstance(problem, str) else problem\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "        self.total_time_steps = total_time_steps  # New attribute for total time steps\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6\n",
    "        v = state1.size\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.1):\n",
    "        state = ast.literal_eval(state) if isinstance(state, str) else state\n",
    "        for case in case_base:\n",
    "            if state == case.problem: \n",
    "                return case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(agent_idx, c, own_temp_case_base, comm_temp_case_base, source='own'):\n",
    "        \"\"\"Reuse step for adding cases to temporary case bases.\"\"\"\n",
    "        if source == 'own':\n",
    "            own_temp_case_base.append(c)\n",
    "        elif source == 'comm':\n",
    "            comm_temp_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(agent_idx, case_base, temporary_case_base, successful_episodes):\n",
    "        for case in case_base:\n",
    "            if any((case.problem, case.solution) == (temp_case.problem, temp_case.solution) for temp_case in temporary_case_base):\n",
    "                if successful_episodes:\n",
    "                    case.trust_value += 0.1\n",
    "                else:\n",
    "                    case.trust_value -= 0.4\n",
    "            else:\n",
    "                if successful_episodes:\n",
    "                    case.trust_value -= 0.4\n",
    "            \n",
    "            case.trust_value = max(0, min(case.trust_value, 1))\n",
    "            print(f\"case content after REVISE for agent {agent_idx}, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "    @staticmethod\n",
    "    \n",
    "    def retain(agent_idx, case_base, own_temp_case_base, comm_temp_case_base, successful_episodes, threshold=0.49):\n",
    "\n",
    "        if successful_episodes:\n",
    "            for temp_case in reversed(own_temp_case_base):\n",
    "                state = tuple(np.atleast_1d(temp_case.problem))\n",
    "    \n",
    "                if not any(tuple(np.atleast_1d(case.problem)) == state for case in case_base):\n",
    "                    case_base.append(temp_case)\n",
    "                    # Case.added_states.add(state)\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} is empty. Temporary case base stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")         \n",
    "                else:\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} for agent {agent_idx} is not empty. Temporary case base that not stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")    \n",
    "        else:\n",
    "            print(f\"Episode not succeeded, temporary case base from own experience is not stored to the case base\")\n",
    "        \n",
    "\n",
    "        case_base_dict = {tuple(np.atleast_1d(case.problem)): case for case in case_base}\n",
    "\n",
    "        for temp_comm_case in reversed(comm_temp_case_base):\n",
    "            state_comm = tuple(np.atleast_1d(temp_comm_case.problem))\n",
    "            \n",
    "            existing_case = case_base_dict.get(state_comm) \n",
    "\n",
    "            if existing_case is None:\n",
    "                case_base.append(temp_comm_case)\n",
    "                case_base_dict[state_comm] = temp_comm_case \n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} is empty. Temporary case base stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")         \n",
    "            # elif existing_case.trust_value < temp_comm_case.trust_value:\n",
    "            #     existing_case.solution = temp_comm_case.solution\n",
    "            #     existing_case.trust_value = max(0, temp_comm_case.trust_value)\n",
    "            #     print(f\"Integrated case process. Similar comm case for agent {agent_idx} is found. Updated case base with higher trust value: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "            # else:\n",
    "            #     print(f\"Integrated case process. comm case {temp_comm_case.problem} for agent {agent_idx} is not empty. Temporary case base that not stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")   \n",
    "\n",
    "\n",
    "        case_base[:] = [case for case in case_base if case.trust_value >= threshold]\n",
    "\n",
    "        for case in case_base:\n",
    "            print(f\"cases content after RETAIN, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}\")\n",
    "\n",
    "        return case_base\n",
    "    \n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.render = render\n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.epsilon_min = epsilon_min  \n",
    "\n",
    "        self.problem_solvers = [ProblemSolver(num_actions, self.env, alpha, gamma, epsilon) for _ in range(self.env.num_agents)]\n",
    "        self.case_bases = [[] for _ in range(self.env.num_agents)]  # Individual case bases for each agent\n",
    "        self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for own experiences\n",
    "        self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for communication experiences\n",
    "        self.successful_episodes = [0] * self.env.num_agents\n",
    "        self.rewards_per_episode = [[] for _ in range(self.env.num_agents)]  \n",
    "        self.total_successful_episodes = 0 \n",
    "        self.action_type = [0] * self.env.num_agents\n",
    "\n",
    "    def run(self):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "        success_steps = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            states = self.env.reset()\n",
    "            episode_reward = [0] * self.env.num_agents\n",
    "            total_steps = 0 \n",
    "            self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            success_count = [0] * self.env.num_agents\n",
    "            dones = [False] * self.env.num_agents\n",
    "            win_states = [False] * self.env.num_agents\n",
    "            successful_episodes = False\n",
    "\n",
    "            while not(all(dones)):\n",
    "                print(f\"----- starting point of Episode {episode} in steps {total_steps} loop -----\")\n",
    "                \n",
    "                actions = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    # print(f\"state before take action: {state}\")\n",
    "                    action = self.take_action(agent_idx, state)\n",
    "                    actions.append(action)\n",
    "\n",
    "                # print(f\"actions pass to the environment\")\n",
    "                next_states, rewards, dones = self.env.step(actions)\n",
    "\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = actions[agent_idx]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "\n",
    "                    physical_state = tuple(state[0])\n",
    "                    win_state = state[1]\n",
    "                    comm_state = state[2]  # Communication state containing messages from other agents\n",
    "\n",
    "                    physical_next_state = tuple(next_state[0])\n",
    "                    win_next_state = next_state[1]\n",
    "                    comm_next_state = tuple(next_state[2]) if next_state[2] != 0 else next_state[2]\n",
    "\n",
    "                    physical_action = action[0]\n",
    "                    comm_action = action[1]\n",
    "\n",
    "                    # Process messages received from other agents\n",
    "                    print(f\"comm next state for agent {agent_idx}: {comm_next_state}\")\n",
    "                    # print(f\"comm next state content: {comm_next_state[0]}\")\n",
    "                    \n",
    "                    # if all(element is None for element in comm_next_state):\n",
    "                    # if (comm_next_state == [None]) or (comm_next_state is None):\n",
    "                    if (comm_next_state == 0):\n",
    "                        pass\n",
    "                    else:\n",
    "                        comm_case = Case(problem=comm_next_state[0], solution=comm_next_state[1], trust_value=comm_next_state[2], total_time_steps=comm_next_state[3])\n",
    "                        Case.reuse(agent_idx, comm_case, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='comm')\n",
    "\n",
    "                    # print(f\"state agent {agent_idx} before update: {physical_state}\")\n",
    "                    # print(f\"win state agent {agent_idx} before update: {win_next_state}\")\n",
    "                    # print(f\"action agent {agent_idx} before update: {physical_action}\")\n",
    "                    # print(f\"reward agent {agent_idx} before update: {reward}\")\n",
    "                    # print(f\"next state agent {agent_idx} before update: {physical_next_state}\")\n",
    "\n",
    "                    c = Case(physical_state, physical_action, total_time_steps=total_steps)\n",
    "                    Case.reuse(agent_idx, c, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='own')\n",
    "\n",
    "                    if self.action_type[agent_idx] == 0:\n",
    "                        if not env.locked[agent_idx]:\n",
    "                            print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                            self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                        else:\n",
    "                            print(f\"action type of agent: {agent_idx}: using problem solver but locked, no learning\")\n",
    "                    else:\n",
    "                        print(f\"action type of agent: {agent_idx}: using solution from case base, no learning\")\n",
    "\n",
    "                    if (win_next_state): \n",
    "                        success_count[agent_idx] += 1\n",
    "                        # print(f\"agent{agent_idx} hit !!!!!\")\n",
    "                    # else:\n",
    "                    #     print(f\"agent{agent_idx} not hit !!!!!\")\n",
    "\n",
    "                    episode_reward[agent_idx] += reward\n",
    "                    win_states.append(win_next_state)  \n",
    "\n",
    "                states = next_states\n",
    "                total_steps += 1\n",
    "\n",
    "                self.env.render()\n",
    "                \n",
    "            if self.env.win_flag:\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(total_steps)\n",
    "                successful_episodes = True\n",
    "                \n",
    "\n",
    "            \n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                print(f\"win status of agent {agent_idx}  before update the case base: {win_states[agent_idx]}\")\n",
    "                self.rewards_per_episode[agent_idx].append(episode_reward[agent_idx])\n",
    "\n",
    "                print(f\"agent{agent_idx} own temp case base: {self.own_temp_case_bases[agent_idx]}\")\n",
    "                print(f\"agent{agent_idx} comm temp case base: {self.comm_temp_case_bases[agent_idx]}\")\n",
    "                \n",
    "                \n",
    "                Case.revise(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], win_states[agent_idx])\n",
    "                self.case_bases[agent_idx] = Case.retain(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], win_states[agent_idx])\n",
    "               \n",
    "                \n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "            print(f\"Episode: {episode}, Total Steps: {total_steps}, Total Rewards: {episode_reward}, Status Episode: {successful_episodes}\")\n",
    "            print(f\"------------------------------------------End of episode {episode} loop--------------------\")\n",
    "\n",
    "        # self.save_case_base_temporary()  # Save temporary case base after training\n",
    "        # self.save_case_base()  # Save case base after training\n",
    "\n",
    "        success_rate = self.total_successful_episodes / episodes * 100\n",
    "\n",
    "        return self.rewards_per_episode, success_rate, memory_usage, gpu_memory_usage, success_steps\n",
    "\n",
    "    def take_action(self, agent_idx, state):\n",
    "        # print(f\"state detected in take action function: {state}\")\n",
    "        physical_state = tuple(state[0])\n",
    "        win_state = state[1]\n",
    "        comm_state = state[2]\n",
    "\n",
    "        similar_solution = Case.retrieve(physical_state, self.case_bases[agent_idx])\n",
    "        if similar_solution is not None:\n",
    "            physical_action = similar_solution.solution\n",
    "            comm_action = (similar_solution.problem, similar_solution.solution, similar_solution.trust_value, similar_solution.total_time_steps)\n",
    "            self.action_type[agent_idx] = 1\n",
    "            # print(f\"Problem detected as a similiar soulution in case base: {similar_solution.problem}\")\n",
    "            print(f\"Physical Action for Agent {agent_idx} from case base: {physical_action}\")\n",
    "            # print(f\"Communication Action for Agent {agent_idx} from case base: {comm_action}\")\n",
    "            # print(f\"Trust value detected as a similiar solution in case base: {similar_solution.trust_value}\")\n",
    "        else:\n",
    "            physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "            comm_action = 0  # No communication action if using problem solver action\n",
    "            self.action_type[agent_idx] = 0\n",
    "            print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        # print(f\"physical action returned from the take action: {physical_action}\")\n",
    "        # print(f\"comm action returned from the take action: {comm_action}\")\n",
    "\n",
    "        return (physical_action, comm_action)\n",
    "\n",
    "    def case_exists_in_case_base(self, case, case_base):\n",
    "        \"\"\"Check if a case exists in the given case base.\"\"\"\n",
    "        return any(existing_case.problem == case.problem and existing_case.solution == case.solution for existing_case in case_base)\n",
    "        \n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_temporary_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.own_temp_case_bases[agent_idx] + self.comm_temp_case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Temporary case base for Agent {agent_idx} saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Case base for Agent {agent_idx} saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            try:\n",
    "                with open(filename, 'r') as file:\n",
    "                    case_base_data = json.load(file)\n",
    "                    self.case_bases[agent_idx] = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"], case[\"total_time_steps\"]) for case in case_base_data]\n",
    "                    print(f\"Case base for Agent {agent_idx} loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Case base file for Agent {agent_idx} not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            plt.plot([reward for reward in rewards[agent_idx]], label=f'Agent {agent_idx}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 2\n",
    "    num_obstacles = 5\n",
    "    obstacles_random_steps = 35\n",
    "    is_agent_silent = False\n",
    "    episodes=100\n",
    "    max_steps=1000\n",
    "    alpha=0.1\n",
    "    gamma=0.9\n",
    "    epsilon=0.3\n",
    "    epsilon_decay = 0.995  \n",
    "    epsilon_min = 0.01  \n",
    "    render = True\n",
    "\n",
    "    env = Env(num_agents=num_agents, num_obstacles=num_obstacles, obstacles_random_steps = obstacles_random_steps, is_agent_silent=is_agent_silent)\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    agent = QCBRL(num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run()\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
