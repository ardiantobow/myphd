{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action probabilites: tensor([[0.2479, 0.2553, 0.2686, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2462, 0.2452, 0.2655, 0.2431]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2462, 0.2452, 0.2655, 0.2431]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2462, 0.2452, 0.2655, 0.2431]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2479, 0.2553, 0.2686, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2072, 0.2625, 0.2930, 0.2374]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2072, 0.2625, 0.2930, 0.2374]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2042, 0.2529, 0.2971, 0.2458]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2072, 0.2625, 0.2930, 0.2374]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2072, 0.2625, 0.2930, 0.2374]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2042, 0.2529, 0.2971, 0.2458]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2462, 0.2452, 0.2655, 0.2431]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2462, 0.2452, 0.2655, 0.2431]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2462, 0.2452, 0.2655, 0.2431]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2462, 0.2452, 0.2655, 0.2431]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2042, 0.2529, 0.2971, 0.2458]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2072, 0.2625, 0.2930, 0.2374]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2072, 0.2625, 0.2930, 0.2374]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2479, 0.2553, 0.2686, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2479, 0.2553, 0.2686, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2469, 0.2559, 0.2688, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2481, 0.2448, 0.2654, 0.2417]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2417, 0.2694, 0.2481]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2417, 0.2694, 0.2481]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2183, 0.2335, 0.2928, 0.2554]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1902, 0.2387, 0.3174, 0.2536]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2183, 0.2335, 0.2928, 0.2554]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2417, 0.2694, 0.2481]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2417, 0.2694, 0.2481]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2305, 0.2363, 0.2789, 0.2543]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2417, 0.2694, 0.2481]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2417, 0.2694, 0.2481]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2417, 0.2694, 0.2481]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2490, 0.2450, 0.2651, 0.2408]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2490, 0.2450, 0.2651, 0.2408]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2461, 0.2560, 0.2689, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2461, 0.2560, 0.2689, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2609, 0.2967, 0.2384]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2038, 0.2517, 0.3018, 0.2427]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 1, Total Timesteps: 46, Episode Reward: -145\n",
      "Total reward obtained for episode 1: -145\n",
      "action probabilites: tensor([[0.2461, 0.2560, 0.2689, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2609, 0.2967, 0.2384]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1836, 0.2504, 0.3362, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1617, 0.2367, 0.3849, 0.2166]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1397, 0.2229, 0.4352, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1397, 0.2229, 0.4352, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1617, 0.2367, 0.3849, 0.2166]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1571, 0.2409, 0.3847, 0.2174]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1349, 0.2274, 0.4348, 0.2029]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1397, 0.2229, 0.4352, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1617, 0.2367, 0.3849, 0.2166]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1836, 0.2504, 0.3362, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 2, Total Timesteps: 58, Episode Reward: -111\n",
      "Total reward obtained for episode 2: -111\n",
      "action probabilites: tensor([[0.2461, 0.2560, 0.2689, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2461, 0.2560, 0.2689, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2602, 0.2982, 0.2389]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2036, 0.2512, 0.3036, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2602, 0.2982, 0.2389]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2602, 0.2982, 0.2389]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2498, 0.2452, 0.2649, 0.2402]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2498, 0.2452, 0.2649, 0.2402]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2419, 0.2432, 0.2690, 0.2459]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 3, Total Timesteps: 80, Episode Reward: -121\n",
      "Total reward obtained for episode 3: -121\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2602, 0.2982, 0.2389]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2602, 0.2982, 0.2389]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2036, 0.2512, 0.3036, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2602, 0.2982, 0.2389]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2036, 0.2512, 0.3036, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2498, 0.2452, 0.2649, 0.2402]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2498, 0.2452, 0.2649, 0.2402]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2498, 0.2452, 0.2649, 0.2402]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2419, 0.2432, 0.2690, 0.2459]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2391, 0.2796, 0.2501]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1907, 0.2404, 0.3215, 0.2474]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 4, Total Timesteps: 93, Episode Reward: -112\n",
      "Total reward obtained for episode 4: -112\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2602, 0.2982, 0.2389]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1823, 0.2489, 0.3392, 0.2295]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1598, 0.2347, 0.3895, 0.2160]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1823, 0.2489, 0.3392, 0.2295]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1823, 0.2489, 0.3392, 0.2295]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 5, Total Timesteps: 99, Episode Reward: -105\n",
      "Total reward obtained for episode 5: -105\n",
      "action probabilites: tensor([[0.2454, 0.2560, 0.2690, 0.2296]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2018, 0.2595, 0.2995, 0.2393]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2035, 0.2507, 0.3051, 0.2407]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2504, 0.2453, 0.2647, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2504, 0.2453, 0.2647, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2035, 0.2507, 0.3051, 0.2407]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action probabilites: tensor([[0.2018, 0.2595, 0.2995, 0.2393]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1811, 0.2476, 0.3419, 0.2293]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 6, Total Timesteps: 107, Episode Reward: -107\n",
      "Total reward obtained for episode 6: -107\n",
      "action probabilites: tensor([[0.2448, 0.2561, 0.2690, 0.2301]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2504, 0.2453, 0.2647, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2423, 0.2437, 0.2688, 0.2451]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2423, 0.2437, 0.2688, 0.2451]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 7, Total Timesteps: 111, Episode Reward: -103\n",
      "Total reward obtained for episode 7: -103\n",
      "action probabilites: tensor([[0.2448, 0.2561, 0.2690, 0.2301]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2448, 0.2561, 0.2690, 0.2301]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2448, 0.2561, 0.2690, 0.2301]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2504, 0.2453, 0.2647, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2448, 0.2561, 0.2690, 0.2301]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2448, 0.2561, 0.2690, 0.2301]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2448, 0.2561, 0.2690, 0.2301]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2504, 0.2453, 0.2647, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2504, 0.2453, 0.2647, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2442, 0.2687, 0.2445]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 8, Total Timesteps: 121, Episode Reward: -109\n",
      "Total reward obtained for episode 8: -109\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2508, 0.2454, 0.2646, 0.2392]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2442, 0.2687, 0.2445]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2315, 0.2410, 0.2802, 0.2473]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2178, 0.2397, 0.2963, 0.2462]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1822, 0.2353, 0.3410, 0.2414]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2178, 0.2397, 0.2963, 0.2462]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2178, 0.2397, 0.2963, 0.2462]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2178, 0.2397, 0.2963, 0.2462]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2178, 0.2397, 0.2963, 0.2462]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2315, 0.2410, 0.2802, 0.2473]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2442, 0.2687, 0.2445]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 9, Total Timesteps: 137, Episode Reward: -115\n",
      "Total reward obtained for episode 9: -115\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2010, 0.2590, 0.3004, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2033, 0.2504, 0.3064, 0.2399]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 10, Total Timesteps: 140, Episode Reward: -102\n",
      "Total reward obtained for episode 10: -102\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2508, 0.2454, 0.2646, 0.2392]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2442, 0.2687, 0.2445]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 11, Total Timesteps: 143, Episode Reward: -102\n",
      "Total reward obtained for episode 11: -102\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2010, 0.2590, 0.3004, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2033, 0.2504, 0.3064, 0.2399]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2010, 0.2590, 0.3004, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1801, 0.2466, 0.3442, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 12, Total Timesteps: 149, Episode Reward: -105\n",
      "Total reward obtained for episode 12: -105\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2444, 0.2560, 0.2690, 0.2305]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2010, 0.2590, 0.3004, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2033, 0.2504, 0.3064, 0.2399]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2010, 0.2590, 0.3004, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2010, 0.2590, 0.3004, 0.2396]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1801, 0.2466, 0.3442, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1801, 0.2466, 0.3442, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2002, 0.2586, 0.3011, 0.2401]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2502, 0.3074, 0.2395]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 13, Total Timesteps: 162, Episode Reward: -112\n",
      "Total reward obtained for episode 13: -112\n",
      "action probabilites: tensor([[0.2440, 0.2560, 0.2692, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2002, 0.2586, 0.3011, 0.2401]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1792, 0.2461, 0.3455, 0.2293]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1792, 0.2461, 0.3455, 0.2293]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2002, 0.2586, 0.3011, 0.2401]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2002, 0.2586, 0.3011, 0.2401]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2440, 0.2560, 0.2692, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2440, 0.2560, 0.2692, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2440, 0.2560, 0.2692, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2440, 0.2560, 0.2692, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2510, 0.2456, 0.2646, 0.2388]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2502, 0.3074, 0.2395]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 14, Total Timesteps: 174, Episode Reward: -111\n",
      "Total reward obtained for episode 14: -111\n",
      "action probabilites: tensor([[0.2440, 0.2560, 0.2692, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2002, 0.2586, 0.3011, 0.2401]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1792, 0.2461, 0.3455, 0.2293]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1562, 0.2306, 0.3986, 0.2146]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1792, 0.2461, 0.3455, 0.2293]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1562, 0.2306, 0.3986, 0.2146]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.2339, 0.4009, 0.2132]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1280, 0.2181, 0.4579, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1280, 0.2181, 0.4579, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1321, 0.2135, 0.4565, 0.1978]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1321, 0.2135, 0.4565, 0.1978]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1321, 0.2135, 0.4565, 0.1978]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1321, 0.2135, 0.4565, 0.1978]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1280, 0.2181, 0.4579, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.2339, 0.4009, 0.2132]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1562, 0.2289, 0.4002, 0.2147]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1321, 0.2135, 0.4565, 0.1978]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1280, 0.2181, 0.4579, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1321, 0.2135, 0.4565, 0.1978]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1280, 0.2181, 0.4579, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1321, 0.2135, 0.4565, 0.1978]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1280, 0.2181, 0.4579, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1241, 0.2228, 0.4575, 0.1957]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1463, 0.2355, 0.4063, 0.2119]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.2339, 0.4009, 0.2132]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1463, 0.2355, 0.4063, 0.2119]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1372, 0.2305, 0.4261, 0.2061]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1461, 0.2325, 0.4099, 0.2115]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1372, 0.2305, 0.4261, 0.2061]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1339, 0.2291, 0.4371, 0.2000]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1372, 0.2305, 0.4261, 0.2061]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1648, 0.2370, 0.3737, 0.2245]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1911, 0.2405, 0.3279, 0.2404]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 15, Total Timesteps: 207, Episode Reward: -132\n",
      "Total reward obtained for episode 15: -132\n",
      "action probabilites: tensor([[0.2433, 0.2558, 0.2696, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2433, 0.2558, 0.2696, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2433, 0.2558, 0.2696, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1995, 0.2571, 0.3027, 0.2408]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1790, 0.2434, 0.3478, 0.2298]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1790, 0.2434, 0.3478, 0.2298]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 16, Total Timesteps: 213, Episode Reward: -105\n",
      "Total reward obtained for episode 16: -105\n",
      "action probabilites: tensor([[0.2433, 0.2558, 0.2696, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2516, 0.2453, 0.2650, 0.2381]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2433, 0.2558, 0.2696, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1995, 0.2571, 0.3027, 0.2408]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2033, 0.2485, 0.3094, 0.2388]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 17, Total Timesteps: 218, Episode Reward: -104\n",
      "Total reward obtained for episode 17: -104\n",
      "action probabilites: tensor([[0.2433, 0.2558, 0.2696, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2433, 0.2558, 0.2696, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2519, 0.2452, 0.2652, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2434, 0.2439, 0.2695, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2519, 0.2452, 0.2652, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2035, 0.2475, 0.3105, 0.2385]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 18, Total Timesteps: 224, Episode Reward: -105\n",
      "Total reward obtained for episode 18: -105\n",
      "action probabilites: tensor([[0.2430, 0.2557, 0.2698, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2430, 0.2557, 0.2698, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2430, 0.2557, 0.2698, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2430, 0.2557, 0.2698, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2430, 0.2557, 0.2698, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2519, 0.2452, 0.2652, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2434, 0.2439, 0.2695, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2315, 0.2416, 0.2828, 0.2441]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2172, 0.2410, 0.3004, 0.2415]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2315, 0.2416, 0.2828, 0.2441]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2315, 0.2416, 0.2828, 0.2441]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2172, 0.2410, 0.3004, 0.2415]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2315, 0.2416, 0.2828, 0.2441]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2315, 0.2416, 0.2828, 0.2441]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2172, 0.2410, 0.3004, 0.2415]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2172, 0.2410, 0.3004, 0.2415]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2326, 0.2417, 0.2817, 0.2440]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2186, 0.2413, 0.2989, 0.2411]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2326, 0.2417, 0.2817, 0.2440]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2443, 0.2438, 0.2687, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2443, 0.2438, 0.2687, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2326, 0.2417, 0.2817, 0.2440]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2443, 0.2438, 0.2687, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2443, 0.2438, 0.2687, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2524, 0.2450, 0.2648, 0.2377]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2555, 0.2702, 0.2317]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2555, 0.2702, 0.2317]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2524, 0.2450, 0.2648, 0.2377]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2443, 0.2438, 0.2687, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2524, 0.2450, 0.2648, 0.2377]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2555, 0.2702, 0.2317]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1990, 0.2555, 0.3041, 0.2413]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2040, 0.2468, 0.3110, 0.2383]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 19, Total Timesteps: 257, Episode Reward: -132\n",
      "Total reward obtained for episode 19: -132\n",
      "action probabilites: tensor([[0.2426, 0.2555, 0.2702, 0.2317]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2426, 0.2555, 0.2702, 0.2317]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1990, 0.2555, 0.3041, 0.2413]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2422, 0.2554, 0.2705, 0.2319]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2422, 0.2554, 0.2705, 0.2319]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2527, 0.2449, 0.2646, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2451, 0.2437, 0.2680, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 20, Total Timesteps: 264, Episode Reward: -106\n",
      "Total reward obtained for episode 20: -106\n",
      "action probabilites: tensor([[0.2422, 0.2554, 0.2705, 0.2319]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2422, 0.2554, 0.2705, 0.2319]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2422, 0.2554, 0.2705, 0.2319]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2527, 0.2449, 0.2646, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2527, 0.2449, 0.2646, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2451, 0.2437, 0.2680, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2527, 0.2449, 0.2646, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2451, 0.2437, 0.2680, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2527, 0.2449, 0.2646, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2422, 0.2554, 0.2705, 0.2319]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2549, 0.3047, 0.2415]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2549, 0.3047, 0.2415]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2044, 0.2461, 0.3114, 0.2382]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 21, Total Timesteps: 277, Episode Reward: -112\n",
      "Total reward obtained for episode 21: -112\n",
      "action probabilites: tensor([[0.2422, 0.2554, 0.2705, 0.2319]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2527, 0.2449, 0.2646, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2044, 0.2461, 0.3114, 0.2382]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2530, 0.2448, 0.2643, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2458, 0.2437, 0.2674, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2345, 0.2420, 0.2799, 0.2435]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2458, 0.2437, 0.2674, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2345, 0.2420, 0.2799, 0.2435]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2345, 0.2420, 0.2799, 0.2435]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2458, 0.2437, 0.2674, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 22, Total Timesteps: 287, Episode Reward: -109\n",
      "Total reward obtained for episode 22: -109\n",
      "action probabilites: tensor([[0.2417, 0.2552, 0.2708, 0.2322]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2543, 0.3053, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2417, 0.2552, 0.2708, 0.2322]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2543, 0.3053, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2543, 0.3053, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2417, 0.2552, 0.2708, 0.2322]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2417, 0.2552, 0.2708, 0.2322]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2543, 0.3053, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2543, 0.3053, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2543, 0.3053, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2543, 0.3053, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2417, 0.2552, 0.2708, 0.2322]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2530, 0.2448, 0.2643, 0.2378]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2533, 0.2447, 0.2641, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2533, 0.2447, 0.2641, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2448, 0.3120, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 23, Total Timesteps: 303, Episode Reward: -115\n",
      "Total reward obtained for episode 23: -115\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2537, 0.3058, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2537, 0.3058, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2537, 0.3058, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2537, 0.3058, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2448, 0.3120, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 24, Total Timesteps: 310, Episode Reward: -106\n",
      "Total reward obtained for episode 24: -106\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2533, 0.2447, 0.2641, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2436, 0.2668, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2436, 0.2668, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2436, 0.2668, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2353, 0.2422, 0.2791, 0.2434]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2436, 0.2668, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2353, 0.2422, 0.2791, 0.2434]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2436, 0.2668, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2436, 0.2668, 0.2432]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 25, Total Timesteps: 320, Episode Reward: -109\n",
      "Total reward obtained for episode 25: -109\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2533, 0.2447, 0.2641, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2533, 0.2447, 0.2641, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2413, 0.2552, 0.2711, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2537, 0.3058, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2448, 0.3120, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2537, 0.3058, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1802, 0.2373, 0.3518, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2537, 0.3058, 0.2416]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1802, 0.2373, 0.3518, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1580, 0.2184, 0.4080, 0.2156]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1802, 0.2373, 0.3518, 0.2308]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1580, 0.2184, 0.4080, 0.2156]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1539, 0.2216, 0.4122, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1300, 0.2035, 0.4720, 0.1945]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1331, 0.1994, 0.4686, 0.1988]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1323, 0.2030, 0.4700, 0.1947]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1323, 0.2030, 0.4700, 0.1947]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1323, 0.2030, 0.4700, 0.1947]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1600, 0.2183, 0.4061, 0.2156]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1600, 0.2183, 0.4061, 0.2156]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1600, 0.2183, 0.4061, 0.2156]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1323, 0.2030, 0.4700, 0.1947]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1994, 0.4660, 0.1990]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1600, 0.2183, 0.4061, 0.2156]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1586, 0.2216, 0.4071, 0.2128]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.2234, 0.4142, 0.2103]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 26, Total Timesteps: 362, Episode Reward: 59\n",
      "Total reward obtained for episode 26: 59\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2536, 0.2445, 0.2646, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2445, 0.3108, 0.2370]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2536, 0.2445, 0.2646, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2445, 0.3108, 0.2370]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2536, 0.2445, 0.2646, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2536, 0.2445, 0.2646, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2536, 0.2445, 0.2646, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2445, 0.3108, 0.2370]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 27, Total Timesteps: 375, Episode Reward: -112\n",
      "Total reward obtained for episode 27: -112\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2004, 0.2537, 0.3042, 0.2417]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1836, 0.2371, 0.3479, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 28, Total Timesteps: 380, Episode Reward: -104\n",
      "Total reward obtained for episode 28: -104\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2536, 0.2445, 0.2646, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2445, 0.3108, 0.2370]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 29, Total Timesteps: 384, Episode Reward: -103\n",
      "Total reward obtained for episode 29: -103\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2402, 0.2551, 0.2718, 0.2329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2004, 0.2537, 0.3042, 0.2417]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1836, 0.2371, 0.3479, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1627, 0.2190, 0.4018, 0.2165]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1390, 0.2008, 0.4597, 0.2004]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1390, 0.2008, 0.4597, 0.2004]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1627, 0.2190, 0.4018, 0.2165]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1627, 0.2190, 0.4018, 0.2165]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1627, 0.2190, 0.4018, 0.2165]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1836, 0.2371, 0.3479, 0.2313]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1627, 0.2190, 0.4018, 0.2165]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1390, 0.2008, 0.4597, 0.2004]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1390, 0.2008, 0.4597, 0.2004]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1356, 0.2039, 0.4647, 0.1958]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1390, 0.2008, 0.4597, 0.2004]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1420, 0.2012, 0.4553, 0.2015]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1420, 0.2012, 0.4553, 0.2015]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1420, 0.2012, 0.4553, 0.2015]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1420, 0.2012, 0.4553, 0.2015]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1420, 0.2012, 0.4553, 0.2015]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1651, 0.2190, 0.3987, 0.2173]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1610, 0.2213, 0.4044, 0.2133]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1541, 0.2233, 0.4122, 0.2104]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1610, 0.2213, 0.4044, 0.2133]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1651, 0.2190, 0.3987, 0.2173]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1854, 0.2370, 0.3460, 0.2317]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1651, 0.2190, 0.3987, 0.2173]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1610, 0.2213, 0.4044, 0.2133]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1651, 0.2190, 0.3987, 0.2173]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1610, 0.2213, 0.4044, 0.2133]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1651, 0.2190, 0.3987, 0.2173]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1610, 0.2213, 0.4044, 0.2133]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1651, 0.2190, 0.3987, 0.2173]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1420, 0.2012, 0.4553, 0.2015]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1651, 0.2190, 0.3987, 0.2173]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1444, 0.1995, 0.4539, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1444, 0.1995, 0.4539, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1444, 0.1995, 0.4539, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1444, 0.1995, 0.4539, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1404, 0.2018, 0.4601, 0.1977]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1354, 0.2056, 0.4632, 0.1959]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1554, 0.2215, 0.4126, 0.2106]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1354, 0.2056, 0.4632, 0.1959]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1554, 0.2215, 0.4126, 0.2106]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1354, 0.2056, 0.4632, 0.1959]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1404, 0.2018, 0.4601, 0.1977]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1444, 0.1995, 0.4539, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1444, 0.1995, 0.4539, 0.2022]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1404, 0.2018, 0.4601, 0.1977]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1354, 0.2056, 0.4632, 0.1959]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1404, 0.2018, 0.4601, 0.1977]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1626, 0.2196, 0.4039, 0.2139]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 30, Total Timesteps: 437, Episode Reward: -152\n",
      "Total reward obtained for episode 30: -152\n",
      "action probabilites: tensor([[0.2392, 0.2548, 0.2727, 0.2333]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2016, 0.2533, 0.3032, 0.2418]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2392, 0.2548, 0.2727, 0.2333]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2534, 0.2437, 0.2667, 0.2362]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2486, 0.2420, 0.2690, 0.2404]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 31, Total Timesteps: 443, Episode Reward: -105\n",
      "Total reward obtained for episode 31: -105\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2021, 0.2529, 0.3031, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2102, 0.2427, 0.3112, 0.2360]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2534, 0.2437, 0.2667, 0.2362]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2534, 0.2437, 0.2667, 0.2362]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2102, 0.2427, 0.3112, 0.2360]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 32, Total Timesteps: 452, Episode Reward: -108\n",
      "Total reward obtained for episode 32: -108\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2534, 0.2437, 0.2667, 0.2362]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2534, 0.2437, 0.2667, 0.2362]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2388, 0.2546, 0.2732, 0.2334]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2526, 0.3030, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2526, 0.3030, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2107, 0.2419, 0.3115, 0.2359]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2526, 0.3030, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2526, 0.3030, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2526, 0.3030, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2533, 0.2434, 0.2675, 0.2359]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2533, 0.2434, 0.2675, 0.2359]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2107, 0.2419, 0.3115, 0.2359]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 33, Total Timesteps: 474, Episode Reward: -121\n",
      "Total reward obtained for episode 33: -121\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2526, 0.3030, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2107, 0.2419, 0.3115, 0.2359]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 34, Total Timesteps: 478, Episode Reward: -103\n",
      "Total reward obtained for episode 34: -103\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2385, 0.2544, 0.2736, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2381, 0.2542, 0.2740, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2381, 0.2542, 0.2740, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2531, 0.2431, 0.2682, 0.2357]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2112, 0.2412, 0.3117, 0.2358]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 35, Total Timesteps: 484, Episode Reward: -105\n",
      "Total reward obtained for episode 35: -105\n",
      "action probabilites: tensor([[0.2381, 0.2542, 0.2740, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2381, 0.2542, 0.2740, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2029, 0.2523, 0.3029, 0.2419]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1903, 0.2332, 0.3441, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1903, 0.2332, 0.3441, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.2137, 0.3955, 0.2189]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1667, 0.2153, 0.4029, 0.2151]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1454, 0.1967, 0.4581, 0.1998]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1505, 0.1953, 0.4504, 0.2038]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.2137, 0.3955, 0.2189]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1505, 0.1953, 0.4504, 0.2038]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.2137, 0.3955, 0.2189]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1667, 0.2153, 0.4029, 0.2151]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1454, 0.1967, 0.4581, 0.1998]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1398, 0.2001, 0.4635, 0.1966]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1398, 0.2001, 0.4635, 0.1966]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1465, 0.1948, 0.4582, 0.2005]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1677, 0.2138, 0.4031, 0.2154]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1731, 0.2123, 0.3952, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1731, 0.2123, 0.3952, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1677, 0.2138, 0.4031, 0.2154]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1731, 0.2123, 0.3952, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1465, 0.1948, 0.4582, 0.2005]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1521, 0.1936, 0.4499, 0.2044]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1914, 0.4502, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1474, 0.1924, 0.4594, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1474, 0.1924, 0.4594, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1914, 0.4502, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1474, 0.1924, 0.4594, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1914, 0.4502, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1914, 0.4502, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1914, 0.4502, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1474, 0.1924, 0.4594, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1914, 0.4502, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1914, 0.4502, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1474, 0.1924, 0.4594, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1413, 0.1954, 0.4661, 0.1972]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1338, 0.1959, 0.4804, 0.1899]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1338, 0.1959, 0.4804, 0.1899]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1413, 0.1954, 0.4661, 0.1972]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1338, 0.1959, 0.4804, 0.1899]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1281, 0.1930, 0.5007, 0.1782]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1527, 0.2054, 0.4513, 0.1907]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1527, 0.2054, 0.4513, 0.1907]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1276, 0.1884, 0.5059, 0.1781]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1276, 0.1884, 0.5059, 0.1781]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1335, 0.1917, 0.4849, 0.1900]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1276, 0.1884, 0.5059, 0.1781]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1523, 0.2013, 0.4565, 0.1898]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1523, 0.2013, 0.4565, 0.1898]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1526, 0.2060, 0.4394, 0.2021]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1335, 0.1917, 0.4849, 0.1900]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1335, 0.1917, 0.4849, 0.1900]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1413, 0.1916, 0.4699, 0.1972]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1476, 0.1890, 0.4624, 0.2010]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1539, 0.1892, 0.4513, 0.2056]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1539, 0.1892, 0.4513, 0.2056]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1746, 0.2084, 0.3963, 0.2207]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1919, 0.2296, 0.3450, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2032, 0.2506, 0.3039, 0.2423]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2383, 0.3144, 0.2357]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2523, 0.2417, 0.2708, 0.2352]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2374, 0.2534, 0.2755, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2523, 0.2417, 0.2708, 0.2352]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2500, 0.3043, 0.2426]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1918, 0.2285, 0.3454, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1918, 0.2285, 0.3454, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1918, 0.2285, 0.3454, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 36, Total Timesteps: 565, Episode Reward: -180\n",
      "Total reward obtained for episode 36: -180\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2500, 0.3043, 0.2426]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2530, 0.2759, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2521, 0.2410, 0.2716, 0.2352]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2521, 0.2410, 0.2716, 0.2352]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2115, 0.2370, 0.3153, 0.2362]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2494, 0.3047, 0.2429]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2527, 0.2763, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2494, 0.3047, 0.2429]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1918, 0.2274, 0.3457, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2494, 0.3047, 0.2429]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2494, 0.3047, 0.2429]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2494, 0.3047, 0.2429]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2114, 0.2358, 0.3161, 0.2367]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 37, Total Timesteps: 588, Episode Reward: -122\n",
      "Total reward obtained for episode 37: -122\n",
      "action probabilites: tensor([[0.2373, 0.2527, 0.2763, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2520, 0.2403, 0.2723, 0.2354]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2520, 0.2403, 0.2723, 0.2354]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2520, 0.2403, 0.2723, 0.2354]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2527, 0.2763, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2527, 0.2763, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2527, 0.2763, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2373, 0.2527, 0.2763, 0.2337]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2030, 0.2494, 0.3047, 0.2429]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1918, 0.2274, 0.3457, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1918, 0.2274, 0.3457, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1751, 0.2050, 0.3968, 0.2231]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1916, 0.2264, 0.3461, 0.2358]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1753, 0.2037, 0.3971, 0.2239]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1753, 0.2037, 0.3971, 0.2239]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1694, 0.2019, 0.4103, 0.2184]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1491, 0.1813, 0.4667, 0.2028]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1418, 0.1812, 0.4768, 0.2001]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1601, 0.2007, 0.4246, 0.2146]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1694, 0.2019, 0.4103, 0.2184]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1491, 0.1813, 0.4667, 0.2028]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1491, 0.1813, 0.4667, 0.2028]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1491, 0.1813, 0.4667, 0.2028]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1491, 0.1813, 0.4667, 0.2028]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1549, 0.1844, 0.4524, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1834, 0.4539, 0.2093]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1834, 0.4539, 0.2093]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1834, 0.4539, 0.2093]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1740, 0.2030, 0.3983, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1834, 0.4539, 0.2093]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1834, 0.4539, 0.2093]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1477, 0.1799, 0.4688, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1477, 0.1799, 0.4688, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.1834, 0.4539, 0.2093]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1740, 0.2030, 0.3983, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1740, 0.2030, 0.3983, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1904, 0.2259, 0.3471, 0.2365]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2023, 0.2484, 0.3058, 0.2435]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1904, 0.2259, 0.3471, 0.2365]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1904, 0.2259, 0.3471, 0.2365]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 38, Total Timesteps: 635, Episode Reward: -146\n",
      "Total reward obtained for episode 38: -146\n",
      "action probabilites: tensor([[0.2370, 0.2521, 0.2771, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2370, 0.2521, 0.2771, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2370, 0.2521, 0.2771, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2516, 0.2391, 0.2739, 0.2354]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2516, 0.2391, 0.2739, 0.2354]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2495, 0.2336, 0.2787, 0.2381]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2310, 0.2923, 0.2360]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2023, 0.2182, 0.3460, 0.2335]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 39, Total Timesteps: 643, Episode Reward: -107\n",
      "Total reward obtained for episode 39: -107\n",
      "action probabilites: tensor([[0.2369, 0.2518, 0.2775, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2017, 0.2481, 0.3065, 0.2438]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2369, 0.2518, 0.2775, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2017, 0.2481, 0.3065, 0.2438]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2017, 0.2481, 0.3065, 0.2438]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2098, 0.2332, 0.3191, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2017, 0.2481, 0.3065, 0.2438]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1893, 0.2255, 0.3480, 0.2372]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1893, 0.2255, 0.3480, 0.2372]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1893, 0.2255, 0.3480, 0.2372]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2017, 0.2481, 0.3065, 0.2438]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2098, 0.2332, 0.3191, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 40, Total Timesteps: 655, Episode Reward: -111\n",
      "Total reward obtained for episode 40: -111\n",
      "action probabilites: tensor([[0.2369, 0.2518, 0.2775, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2017, 0.2481, 0.3065, 0.2438]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2098, 0.2332, 0.3191, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 41, Total Timesteps: 658, Episode Reward: -102\n",
      "Total reward obtained for episode 41: -102\n",
      "action probabilites: tensor([[0.2369, 0.2518, 0.2775, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2514, 0.2385, 0.2747, 0.2354]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2011, 0.2478, 0.3071, 0.2440]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2011, 0.2478, 0.3071, 0.2440]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2092, 0.2325, 0.3200, 0.2383]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 42, Total Timesteps: 664, Episode Reward: -105\n",
      "Total reward obtained for episode 42: -105\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2512, 0.2380, 0.2755, 0.2353]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2512, 0.2380, 0.2755, 0.2353]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2011, 0.2478, 0.3071, 0.2440]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2011, 0.2478, 0.3071, 0.2440]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2092, 0.2325, 0.3200, 0.2383]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 43, Total Timesteps: 677, Episode Reward: -112\n",
      "Total reward obtained for episode 43: -112\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2367, 0.2516, 0.2779, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2006, 0.2475, 0.3076, 0.2443]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2085, 0.2320, 0.3208, 0.2388]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2006, 0.2475, 0.3076, 0.2443]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2085, 0.2320, 0.3208, 0.2388]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 44, Total Timesteps: 684, Episode Reward: -106\n",
      "Total reward obtained for episode 44: -106\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2006, 0.2475, 0.3076, 0.2443]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2085, 0.2320, 0.3208, 0.2388]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 45, Total Timesteps: 692, Episode Reward: -107\n",
      "Total reward obtained for episode 45: -107\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2509, 0.2376, 0.2762, 0.2352]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2509, 0.2376, 0.2762, 0.2352]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2366, 0.2513, 0.2783, 0.2338]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2507, 0.2373, 0.2769, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2364, 0.2511, 0.2786, 0.2339]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2364, 0.2511, 0.2786, 0.2339]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2507, 0.2373, 0.2769, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2507, 0.2373, 0.2769, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2507, 0.2373, 0.2769, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2486, 0.2314, 0.2821, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2507, 0.2373, 0.2769, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2507, 0.2373, 0.2769, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2078, 0.2315, 0.3215, 0.2392]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 46, Total Timesteps: 710, Episode Reward: -117\n",
      "Total reward obtained for episode 46: -117\n",
      "action probabilites: tensor([[0.2364, 0.2511, 0.2786, 0.2339]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2364, 0.2511, 0.2786, 0.2339]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2507, 0.2373, 0.2769, 0.2351]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2486, 0.2314, 0.2821, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2486, 0.2314, 0.2821, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2486, 0.2314, 0.2821, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2283, 0.2958, 0.2353]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2300, 0.2241, 0.3162, 0.2297]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2407, 0.2283, 0.2958, 0.2353]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2300, 0.2241, 0.3162, 0.2297]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2406, 0.2277, 0.2969, 0.2348]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2004, 0.2128, 0.3523, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1745, 0.1990, 0.4069, 0.2197]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1467, 0.1831, 0.4587, 0.2116]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1455, 0.1769, 0.4811, 0.1966]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1207, 0.1612, 0.5310, 0.1871]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1269, 0.1676, 0.5072, 0.1983]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1363, 0.1716, 0.4907, 0.2014]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1269, 0.1676, 0.5072, 0.1983]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1363, 0.1716, 0.4907, 0.2014]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1423, 0.1745, 0.4769, 0.2064]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1363, 0.1716, 0.4907, 0.2014]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1550, 0.1917, 0.4352, 0.2181]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1467, 0.1831, 0.4587, 0.2116]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1745, 0.1990, 0.4069, 0.2197]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1467, 0.1831, 0.4587, 0.2116]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1550, 0.1917, 0.4352, 0.2181]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1467, 0.1831, 0.4587, 0.2116]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1550, 0.1917, 0.4352, 0.2181]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1363, 0.1716, 0.4907, 0.2014]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1416, 0.1741, 0.4793, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1357, 0.1712, 0.4938, 0.1993]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1416, 0.1741, 0.4793, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1625, 0.1956, 0.4208, 0.2211]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1545, 0.1914, 0.4377, 0.2164]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1625, 0.1956, 0.4208, 0.2211]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1545, 0.1914, 0.4377, 0.2164]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1357, 0.1712, 0.4938, 0.1993]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1416, 0.1741, 0.4793, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1416, 0.1741, 0.4793, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1669, 0.2004, 0.4054, 0.2273]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1468, 0.1793, 0.4616, 0.2123]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1465, 0.1789, 0.4652, 0.2094]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1465, 0.1789, 0.4652, 0.2094]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1665, 0.2003, 0.4083, 0.2249]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1846, 0.2239, 0.3542, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2470, 0.3109, 0.2433]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2470, 0.3109, 0.2433]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2351, 0.2506, 0.2807, 0.2336]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2351, 0.2506, 0.2807, 0.2336]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2351, 0.2506, 0.2807, 0.2336]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2470, 0.3109, 0.2433]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2351, 0.2506, 0.2807, 0.2336]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1989, 0.2470, 0.3109, 0.2433]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1846, 0.2239, 0.3542, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1846, 0.2239, 0.3542, 0.2373]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1665, 0.2003, 0.4083, 0.2249]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1465, 0.1789, 0.4652, 0.2094]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1465, 0.1789, 0.4652, 0.2094]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1413, 0.1739, 0.4831, 0.2017]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1355, 0.1709, 0.4984, 0.1952]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1413, 0.1739, 0.4831, 0.2017]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1674, 0.2003, 0.4108, 0.2215]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1674, 0.2003, 0.4108, 0.2215]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1423, 0.1738, 0.4859, 0.1980]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1423, 0.1738, 0.4859, 0.1980]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1423, 0.1738, 0.4859, 0.1980]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1674, 0.2003, 0.4108, 0.2215]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1674, 0.2003, 0.4108, 0.2215]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1674, 0.2003, 0.4108, 0.2215]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1478, 0.1786, 0.4682, 0.2054]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1500, 0.1779, 0.4713, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1500, 0.1779, 0.4713, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1500, 0.1779, 0.4713, 0.2008]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1377, 0.1713, 0.5040, 0.1870]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1265, 0.1687, 0.5212, 0.1836]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1196, 0.1640, 0.5471, 0.1692]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1265, 0.1687, 0.5212, 0.1836]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1377, 0.1713, 0.5040, 0.1870]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1377, 0.1713, 0.5040, 0.1870]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1377, 0.1713, 0.5040, 0.1870]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1377, 0.1713, 0.5040, 0.1870]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1441, 0.1736, 0.4880, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1641, 0.1963, 0.4282, 0.2115]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1721, 0.1992, 0.4152, 0.2134]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1889, 0.2233, 0.3594, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2006, 0.2470, 0.3146, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1889, 0.2233, 0.3594, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1721, 0.1992, 0.4152, 0.2134]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1526, 0.1770, 0.4744, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1526, 0.1770, 0.4744, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1526, 0.1770, 0.4744, 0.1960]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1721, 0.1992, 0.4152, 0.2134]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1889, 0.2233, 0.3594, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 47, Total Timesteps: 830, Episode Reward: -219\n",
      "Total reward obtained for episode 47: -219\n",
      "action probabilites: tensor([[0.2332, 0.2496, 0.2852, 0.2320]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2332, 0.2496, 0.2852, 0.2320]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2332, 0.2496, 0.2852, 0.2320]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2006, 0.2470, 0.3146, 0.2379]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2064, 0.2330, 0.3283, 0.2323]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 48, Total Timesteps: 835, Episode Reward: -104\n",
      "Total reward obtained for episode 48: -104\n",
      "action probabilites: tensor([[0.2332, 0.2496, 0.2852, 0.2320]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2376, 0.2835, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2376, 0.2835, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2064, 0.2330, 0.3283, 0.2323]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2464, 0.2376, 0.2835, 0.2324]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2065, 0.2335, 0.3294, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 49, Total Timesteps: 841, Episode Reward: -105\n",
      "Total reward obtained for episode 49: -105\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2452, 0.2377, 0.2850, 0.2320]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2013, 0.2468, 0.3158, 0.2360]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2065, 0.2335, 0.3294, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 50, Total Timesteps: 848, Episode Reward: -106\n",
      "Total reward obtained for episode 50: -106\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2452, 0.2377, 0.2850, 0.2320]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2452, 0.2377, 0.2850, 0.2320]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2325, 0.2492, 0.2868, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2381, 0.2314, 0.2988, 0.2316]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2250, 0.2285, 0.3202, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2097, 0.2240, 0.3498, 0.2164]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2097, 0.2240, 0.3498, 0.2164]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2250, 0.2285, 0.3202, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2381, 0.2314, 0.2988, 0.2316]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 51, Total Timesteps: 868, Episode Reward: -119\n",
      "Total reward obtained for episode 51: -119\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2381, 0.2314, 0.2988, 0.2316]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 52, Total Timesteps: 872, Episode Reward: -103\n",
      "Total reward obtained for episode 52: -103\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2338, 0.3305, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 53, Total Timesteps: 880, Episode Reward: -107\n",
      "Total reward obtained for episode 53: -107\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1923, 0.2225, 0.3624, 0.2228]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1763, 0.1978, 0.4195, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1763, 0.1978, 0.4195, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1923, 0.2225, 0.3624, 0.2228]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1923, 0.2225, 0.3624, 0.2228]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1763, 0.1978, 0.4195, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1763, 0.1978, 0.4195, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1687, 0.1962, 0.4338, 0.2013]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 54, Total Timesteps: 891, Episode Reward: -110\n",
      "Total reward obtained for episode 54: -110\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2338, 0.3305, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 55, Total Timesteps: 894, Episode Reward: -102\n",
      "Total reward obtained for episode 55: -102\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1923, 0.2225, 0.3624, 0.2228]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 56, Total Timesteps: 900, Episode Reward: -105\n",
      "Total reward obtained for episode 56: -105\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2338, 0.3305, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 57, Total Timesteps: 910, Episode Reward: -109\n",
      "Total reward obtained for episode 57: -109\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2338, 0.3305, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2442, 0.2378, 0.2865, 0.2315]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2381, 0.2314, 0.2988, 0.2316]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 58, Total Timesteps: 917, Episode Reward: -106\n",
      "Total reward obtained for episode 58: -106\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2318, 0.2488, 0.2883, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2467, 0.3169, 0.2344]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2465, 0.3180, 0.2328]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2465, 0.3180, 0.2328]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1938, 0.2221, 0.3637, 0.2203]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 59, Total Timesteps: 925, Episode Reward: -107\n",
      "Total reward obtained for episode 59: -107\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2465, 0.3180, 0.2328]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1938, 0.2221, 0.3637, 0.2203]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1938, 0.2221, 0.3637, 0.2203]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2465, 0.3180, 0.2328]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2067, 0.2341, 0.3315, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2432, 0.2379, 0.2878, 0.2311]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2027, 0.2465, 0.3180, 0.2328]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2312, 0.2485, 0.2897, 0.2306]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2070, 0.2343, 0.3324, 0.2262]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2070, 0.2343, 0.3324, 0.2262]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2423, 0.2379, 0.2890, 0.2307]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2423, 0.2379, 0.2890, 0.2307]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2306, 0.2482, 0.2909, 0.2302]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2070, 0.2343, 0.3324, 0.2262]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 60, Total Timesteps: 950, Episode Reward: -124\n",
      "Total reward obtained for episode 60: -124\n",
      "action probabilites: tensor([[0.2306, 0.2482, 0.2909, 0.2302]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2070, 0.2343, 0.3324, 0.2262]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1953, 0.2217, 0.3649, 0.2181]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1953, 0.2217, 0.3649, 0.2181]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2070, 0.2343, 0.3324, 0.2262]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2034, 0.2463, 0.3190, 0.2314]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2460, 0.3199, 0.2300]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2415, 0.2380, 0.2903, 0.2302]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2415, 0.2380, 0.2903, 0.2302]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2415, 0.2380, 0.2903, 0.2302]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2460, 0.3199, 0.2300]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2460, 0.3199, 0.2300]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2344, 0.3333, 0.2249]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 61, Total Timesteps: 973, Episode Reward: -122\n",
      "Total reward obtained for episode 61: -122\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2301, 0.2479, 0.2921, 0.2299]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2460, 0.3199, 0.2300]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2460, 0.3199, 0.2300]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2297, 0.2476, 0.2932, 0.2295]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2297, 0.2476, 0.2932, 0.2295]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2408, 0.2381, 0.2914, 0.2298]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2408, 0.2381, 0.2914, 0.2298]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2408, 0.2381, 0.2914, 0.2298]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2297, 0.2476, 0.2932, 0.2295]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2076, 0.2346, 0.3341, 0.2237]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2297, 0.2476, 0.2932, 0.2295]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2408, 0.2381, 0.2914, 0.2298]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2076, 0.2346, 0.3341, 0.2237]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1979, 0.2209, 0.3669, 0.2143]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1979, 0.2209, 0.3669, 0.2143]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2047, 0.2457, 0.3207, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1992, 0.2205, 0.3677, 0.2127]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1992, 0.2205, 0.3677, 0.2127]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2054, 0.2454, 0.3214, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2292, 0.2474, 0.2943, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2398, 0.2381, 0.2933, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2398, 0.2381, 0.2933, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2309, 0.2318, 0.3098, 0.2275]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 62, Total Timesteps: 1023, Episode Reward: -149\n",
      "Total reward obtained for episode 62: -149\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2063, 0.2451, 0.3219, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2398, 0.2381, 0.2933, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2309, 0.2318, 0.3098, 0.2275]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2398, 0.2381, 0.2933, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2398, 0.2381, 0.2933, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2063, 0.2451, 0.3219, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2287, 0.2472, 0.2953, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2393, 0.2381, 0.2942, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2393, 0.2381, 0.2942, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2393, 0.2381, 0.2942, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2393, 0.2381, 0.2942, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2393, 0.2381, 0.2942, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2281, 0.2471, 0.2962, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2071, 0.2448, 0.3224, 0.2257]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2195, 0.3683, 0.2097]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2081, 0.2445, 0.3227, 0.2247]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2042, 0.2190, 0.3684, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2042, 0.2190, 0.3684, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1903, 0.1927, 0.4280, 0.1889]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1802, 0.1940, 0.4419, 0.1840]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1903, 0.1927, 0.4280, 0.1889]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1903, 0.1927, 0.4280, 0.1889]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1903, 0.1927, 0.4280, 0.1889]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1802, 0.1940, 0.4419, 0.1840]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1630, 0.1688, 0.5050, 0.1633]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1733, 0.1694, 0.4904, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1630, 0.1688, 0.5050, 0.1633]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1492, 0.1679, 0.5271, 0.1558]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1346, 0.1716, 0.5497, 0.1442]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1213, 0.1704, 0.5754, 0.1329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1346, 0.1716, 0.5497, 0.1442]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1213, 0.1704, 0.5754, 0.1329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1346, 0.1716, 0.5497, 0.1442]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1346, 0.1716, 0.5497, 0.1442]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1213, 0.1704, 0.5754, 0.1329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1346, 0.1716, 0.5497, 0.1442]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1213, 0.1704, 0.5754, 0.1329]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1346, 0.1716, 0.5497, 0.1442]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1495, 0.1691, 0.5263, 0.1552]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1346, 0.1716, 0.5497, 0.1442]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1495, 0.1691, 0.5263, 0.1552]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1641, 0.1702, 0.5025, 0.1633]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1747, 0.1708, 0.4878, 0.1668]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1747, 0.1708, 0.4878, 0.1668]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1915, 0.1937, 0.4262, 0.1886]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1747, 0.1708, 0.4878, 0.1668]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1915, 0.1937, 0.4262, 0.1886]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1747, 0.1708, 0.4878, 0.1668]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1743, 0.1747, 0.4808, 0.1701]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1743, 0.1747, 0.4808, 0.1701]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1743, 0.1747, 0.4808, 0.1701]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1636, 0.1744, 0.4951, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1743, 0.1747, 0.4808, 0.1701]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1743, 0.1747, 0.4808, 0.1701]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1636, 0.1744, 0.4951, 0.1670]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1743, 0.1747, 0.4808, 0.1701]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1911, 0.1968, 0.4210, 0.1911]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2046, 0.2218, 0.3641, 0.2094]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1911, 0.1968, 0.4210, 0.1911]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2046, 0.2218, 0.3641, 0.2094]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 63, Total Timesteps: 1112, Episode Reward: -188\n",
      "Total reward obtained for episode 63: -188\n",
      "action probabilites: tensor([[0.2256, 0.2472, 0.2992, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2368, 0.2381, 0.2974, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2265, 0.2318, 0.3161, 0.2256]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 64, Total Timesteps: 1115, Episode Reward: -102\n",
      "Total reward obtained for episode 64: -102\n",
      "action probabilites: tensor([[0.2256, 0.2472, 0.2992, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2256, 0.2472, 0.2992, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2256, 0.2472, 0.2992, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2368, 0.2381, 0.2974, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2368, 0.2381, 0.2974, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2076, 0.2376, 0.3339, 0.2209]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2356, 0.2383, 0.2984, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2247, 0.2322, 0.3173, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2247, 0.2322, 0.3173, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2356, 0.2383, 0.2984, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2356, 0.2383, 0.2984, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2247, 0.2322, 0.3173, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2356, 0.2383, 0.2984, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2246, 0.2473, 0.3002, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2356, 0.2383, 0.2984, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2076, 0.2376, 0.3339, 0.2209]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2469, 0.3200, 0.2254]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2469, 0.3200, 0.2254]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2246, 0.2473, 0.3002, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2469, 0.3200, 0.2254]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2238, 0.3613, 0.2109]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2469, 0.3200, 0.2254]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2469, 0.3200, 0.2254]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2077, 0.2469, 0.3200, 0.2254]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2246, 0.2473, 0.3002, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2474, 0.3012, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2345, 0.2384, 0.2993, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2474, 0.3012, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2068, 0.2388, 0.3328, 0.2216]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 65, Total Timesteps: 1147, Episode Reward: -131\n",
      "Total reward obtained for episode 65: -131\n",
      "action probabilites: tensor([[0.2236, 0.2474, 0.3012, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2068, 0.2388, 0.3328, 0.2216]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 66, Total Timesteps: 1150, Episode Reward: -102\n",
      "Total reward obtained for episode 66: -102\n",
      "action probabilites: tensor([[0.2236, 0.2474, 0.3012, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2474, 0.3012, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2474, 0.3012, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2474, 0.3012, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2073, 0.2478, 0.3190, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2069, 0.2486, 0.3182, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2334, 0.2386, 0.3001, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2060, 0.2399, 0.3319, 0.2223]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 67, Total Timesteps: 1164, Episode Reward: -113\n",
      "Total reward obtained for episode 67: -113\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2069, 0.2486, 0.3182, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2032, 0.2272, 0.3564, 0.2131]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1898, 0.2044, 0.4085, 0.1973]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1770, 0.2063, 0.4233, 0.1934]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1615, 0.1849, 0.4779, 0.1757]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1734, 0.1847, 0.4635, 0.1784]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1734, 0.1847, 0.4635, 0.1784]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1734, 0.1847, 0.4635, 0.1784]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1734, 0.1847, 0.4635, 0.1784]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1898, 0.2044, 0.4085, 0.1973]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2032, 0.2272, 0.3564, 0.2131]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 68, Total Timesteps: 1180, Episode Reward: -115\n",
      "Total reward obtained for episode 68: -115\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2334, 0.2386, 0.3001, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2069, 0.2486, 0.3182, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2069, 0.2486, 0.3182, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2069, 0.2486, 0.3182, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2069, 0.2486, 0.3182, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2060, 0.2399, 0.3319, 0.2223]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2334, 0.2386, 0.3001, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2216, 0.2326, 0.3196, 0.2261]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2028, 0.2306, 0.3487, 0.2178]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2028, 0.2306, 0.3487, 0.2178]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2216, 0.2326, 0.3196, 0.2261]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 69, Total Timesteps: 1197, Episode Reward: -116\n",
      "Total reward obtained for episode 69: -116\n",
      "action probabilites: tensor([[0.2227, 0.2475, 0.3020, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2334, 0.2386, 0.3001, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2334, 0.2386, 0.3001, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2493, 0.3174, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2493, 0.3174, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2493, 0.3174, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2493, 0.3174, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2052, 0.2409, 0.3310, 0.2229]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2493, 0.3174, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2028, 0.2285, 0.3544, 0.2142]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2493, 0.3174, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2066, 0.2493, 0.3174, 0.2267]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2219, 0.2476, 0.3028, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2063, 0.2500, 0.3166, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2045, 0.2418, 0.3302, 0.2234]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2063, 0.2500, 0.3166, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2045, 0.2418, 0.3302, 0.2234]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2063, 0.2500, 0.3166, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2298, 0.3526, 0.2152]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2298, 0.3526, 0.2152]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2298, 0.3526, 0.2152]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2063, 0.2500, 0.3166, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2210, 0.2477, 0.3034, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2210, 0.2477, 0.3034, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2210, 0.2477, 0.3034, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2210, 0.2477, 0.3034, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2316, 0.2388, 0.3017, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2191, 0.2330, 0.3216, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1994, 0.2313, 0.3509, 0.2184]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1994, 0.2313, 0.3509, 0.2184]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1767, 0.2335, 0.3846, 0.2052]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 70, Total Timesteps: 1238, Episode Reward: -140\n",
      "Total reward obtained for episode 70: -140\n",
      "action probabilites: tensor([[0.2210, 0.2477, 0.3034, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2210, 0.2477, 0.3034, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2060, 0.2507, 0.3160, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2426, 0.3296, 0.2239]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 71, Total Timesteps: 1242, Episode Reward: -103\n",
      "Total reward obtained for episode 71: -103\n",
      "action probabilites: tensor([[0.2202, 0.2478, 0.3041, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2202, 0.2478, 0.3041, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2308, 0.2389, 0.3023, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2426, 0.3296, 0.2239]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 72, Total Timesteps: 1246, Episode Reward: -103\n",
      "Total reward obtained for episode 72: -103\n",
      "action probabilites: tensor([[0.2202, 0.2478, 0.3041, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2060, 0.2507, 0.3160, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2060, 0.2507, 0.3160, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2426, 0.3296, 0.2239]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 73, Total Timesteps: 1250, Episode Reward: -103\n",
      "Total reward obtained for episode 73: -103\n",
      "action probabilites: tensor([[0.2202, 0.2478, 0.3041, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2202, 0.2478, 0.3041, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2308, 0.2389, 0.3023, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2180, 0.2332, 0.3225, 0.2263]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 74, Total Timesteps: 1254, Episode Reward: -103\n",
      "Total reward obtained for episode 74: -103\n",
      "action probabilites: tensor([[0.2202, 0.2478, 0.3041, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2308, 0.2389, 0.3023, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2202, 0.2478, 0.3041, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2308, 0.2389, 0.3023, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2426, 0.3296, 0.2239]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2060, 0.2507, 0.3160, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2016, 0.2318, 0.3499, 0.2167]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1882, 0.2113, 0.3974, 0.2031]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1882, 0.2113, 0.3974, 0.2031]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1584, 0.1942, 0.4641, 0.1833]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1719, 0.1941, 0.4480, 0.1861]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1882, 0.2113, 0.3974, 0.2031]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2016, 0.2318, 0.3499, 0.2167]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2016, 0.2318, 0.3499, 0.2167]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1882, 0.2113, 0.3974, 0.2031]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1880, 0.2123, 0.3961, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1880, 0.2123, 0.3961, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1716, 0.1955, 0.4460, 0.1868]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1716, 0.1955, 0.4460, 0.1868]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1580, 0.1957, 0.4622, 0.1841]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1716, 0.1955, 0.4460, 0.1868]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1716, 0.1955, 0.4460, 0.1868]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1716, 0.1955, 0.4460, 0.1868]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1880, 0.2123, 0.3961, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1880, 0.2123, 0.3961, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1880, 0.2123, 0.3961, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1734, 0.2147, 0.4120, 0.2000]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1880, 0.2123, 0.3961, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2014, 0.2325, 0.3491, 0.2170]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 75, Total Timesteps: 1294, Episode Reward: -139\n",
      "Total reward obtained for episode 75: -139\n",
      "action probabilites: tensor([[0.2188, 0.2480, 0.3052, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2296, 0.2391, 0.3036, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2162, 0.2334, 0.3244, 0.2261]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2162, 0.2334, 0.3244, 0.2261]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1954, 0.2318, 0.3539, 0.2189]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1954, 0.2318, 0.3539, 0.2189]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2154, 0.2334, 0.3254, 0.2258]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2290, 0.2392, 0.3043, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2182, 0.2480, 0.3058, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2053, 0.2520, 0.3150, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2444, 0.3288, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2053, 0.2520, 0.3150, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2182, 0.2480, 0.3058, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2053, 0.2520, 0.3150, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2053, 0.2520, 0.3150, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2053, 0.2520, 0.3150, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2182, 0.2480, 0.3058, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2290, 0.2392, 0.3043, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2290, 0.2392, 0.3043, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2290, 0.2392, 0.3043, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2290, 0.2392, 0.3043, 0.2274]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2444, 0.3288, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 76, Total Timesteps: 1316, Episode Reward: -121\n",
      "Total reward obtained for episode 76: -121\n",
      "action probabilites: tensor([[0.2182, 0.2480, 0.3058, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2182, 0.2480, 0.3058, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2182, 0.2480, 0.3058, 0.2280]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2053, 0.2520, 0.3150, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2022, 0.2448, 0.3287, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 77, Total Timesteps: 1321, Episode Reward: -104\n",
      "Total reward obtained for episode 77: -104\n",
      "action probabilites: tensor([[0.2176, 0.2481, 0.3063, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2051, 0.2523, 0.3149, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2051, 0.2523, 0.3149, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2009, 0.2338, 0.3478, 0.2174]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2009, 0.2338, 0.3478, 0.2174]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 78, Total Timesteps: 1326, Episode Reward: -104\n",
      "Total reward obtained for episode 78: -104\n",
      "action probabilites: tensor([[0.2176, 0.2481, 0.3063, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2051, 0.2523, 0.3149, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2176, 0.2481, 0.3063, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2176, 0.2481, 0.3063, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2285, 0.2393, 0.3050, 0.2272]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2285, 0.2393, 0.3050, 0.2272]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2022, 0.2448, 0.3287, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2051, 0.2523, 0.3149, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2051, 0.2523, 0.3149, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2009, 0.2338, 0.3478, 0.2174]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 79, Total Timesteps: 1336, Episode Reward: -109\n",
      "Total reward obtained for episode 79: -109\n",
      "action probabilites: tensor([[0.2176, 0.2481, 0.3063, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2285, 0.2393, 0.3050, 0.2272]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2176, 0.2481, 0.3063, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2285, 0.2393, 0.3050, 0.2272]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2170, 0.2481, 0.3068, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2280, 0.2394, 0.3055, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2018, 0.2452, 0.3286, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 80, Total Timesteps: 1343, Episode Reward: -106\n",
      "Total reward obtained for episode 80: -106\n",
      "action probabilites: tensor([[0.2170, 0.2481, 0.3068, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2170, 0.2481, 0.3068, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2049, 0.2526, 0.3148, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2049, 0.2526, 0.3148, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2170, 0.2481, 0.3068, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2280, 0.2394, 0.3055, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2280, 0.2394, 0.3055, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2140, 0.2335, 0.3272, 0.2253]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1926, 0.2320, 0.3570, 0.2184]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1926, 0.2320, 0.3570, 0.2184]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1710, 0.2377, 0.3848, 0.2066]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.2342, 0.4226, 0.1899]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1354, 0.2356, 0.4471, 0.1819]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1354, 0.2356, 0.4471, 0.1819]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1534, 0.2342, 0.4226, 0.1899]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1354, 0.2356, 0.4471, 0.1819]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1236, 0.2207, 0.4927, 0.1630]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1350, 0.2210, 0.4643, 0.1797]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1515, 0.2179, 0.4388, 0.1918]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1391, 0.1991, 0.4861, 0.1758]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1391, 0.1991, 0.4861, 0.1758]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1515, 0.2179, 0.4388, 0.1918]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1350, 0.2210, 0.4643, 0.1797]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1212, 0.2037, 0.5115, 0.1637]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1391, 0.1991, 0.4861, 0.1758]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1564, 0.2005, 0.4566, 0.1866]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1717, 0.2186, 0.4082, 0.2015]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1870, 0.2159, 0.3921, 0.2050]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2005, 0.2349, 0.3469, 0.2177]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 81, Total Timesteps: 1372, Episode Reward: -128\n",
      "Total reward obtained for episode 81: -128\n",
      "action probabilites: tensor([[0.2165, 0.2482, 0.3072, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2046, 0.2529, 0.3148, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2165, 0.2482, 0.3072, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2165, 0.2482, 0.3072, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2276, 0.2395, 0.3061, 0.2269]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2015, 0.2456, 0.3286, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 82, Total Timesteps: 1378, Episode Reward: -105\n",
      "Total reward obtained for episode 82: -105\n",
      "action probabilites: tensor([[0.2165, 0.2482, 0.3072, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2165, 0.2482, 0.3072, 0.2281]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2272, 0.2395, 0.3065, 0.2268]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2012, 0.2460, 0.3286, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2272, 0.2395, 0.3065, 0.2268]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2012, 0.2460, 0.3286, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2272, 0.2395, 0.3065, 0.2268]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2159, 0.2482, 0.3077, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2272, 0.2395, 0.3065, 0.2268]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2012, 0.2460, 0.3286, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2044, 0.2531, 0.3148, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2159, 0.2482, 0.3077, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2044, 0.2531, 0.3148, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2159, 0.2482, 0.3077, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2044, 0.2531, 0.3148, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2159, 0.2482, 0.3077, 0.2282]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2044, 0.2531, 0.3148, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2003, 0.2353, 0.3465, 0.2178]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2003, 0.2353, 0.3465, 0.2178]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1867, 0.2166, 0.3914, 0.2052]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1713, 0.2194, 0.4075, 0.2018]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1511, 0.2187, 0.4382, 0.1920]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1711, 0.2198, 0.4071, 0.2019]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1866, 0.2170, 0.3910, 0.2053]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1866, 0.2170, 0.3910, 0.2053]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1866, 0.2170, 0.3910, 0.2053]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1866, 0.2170, 0.3910, 0.2053]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1558, 0.2020, 0.4549, 0.1872]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1866, 0.2170, 0.3910, 0.2053]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1866, 0.2170, 0.3910, 0.2053]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2016, 0.4384, 0.1896]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1698, 0.2006, 0.4399, 0.1897]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1698, 0.2006, 0.4399, 0.1897]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1698, 0.2006, 0.4399, 0.1897]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1698, 0.2006, 0.4399, 0.1897]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1861, 0.2162, 0.3922, 0.2055]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1698, 0.2006, 0.4399, 0.1897]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1861, 0.2162, 0.3922, 0.2055]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1705, 0.2190, 0.4084, 0.2020]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 83, Total Timesteps: 1428, Episode Reward: -149\n",
      "Total reward obtained for episode 83: -149\n",
      "action probabilites: tensor([[0.2146, 0.2481, 0.3089, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2146, 0.2481, 0.3089, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2146, 0.2481, 0.3089, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2146, 0.2481, 0.3089, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2146, 0.2481, 0.3089, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2146, 0.2481, 0.3089, 0.2284]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2042, 0.2527, 0.3155, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2001, 0.2348, 0.3472, 0.2180]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1861, 0.2162, 0.3922, 0.2055]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1698, 0.2006, 0.4399, 0.1897]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1698, 0.2006, 0.4399, 0.1897]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1861, 0.2162, 0.3922, 0.2055]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1693, 0.1999, 0.4408, 0.1900]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1693, 0.1999, 0.4408, 0.1900]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1857, 0.2156, 0.3930, 0.2057]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1999, 0.2342, 0.3478, 0.2182]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 84, Total Timesteps: 1444, Episode Reward: -115\n",
      "Total reward obtained for episode 84: -115\n",
      "action probabilites: tensor([[0.2138, 0.2480, 0.3096, 0.2285]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2138, 0.2480, 0.3096, 0.2285]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2256, 0.2397, 0.3089, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2256, 0.2397, 0.3089, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2105, 0.2336, 0.3314, 0.2245]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2105, 0.2336, 0.3314, 0.2245]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2256, 0.2397, 0.3089, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2138, 0.2480, 0.3096, 0.2285]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2523, 0.3161, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2006, 0.2452, 0.3302, 0.2240]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 85, Total Timesteps: 1454, Episode Reward: -109\n",
      "Total reward obtained for episode 85: -109\n",
      "action probabilites: tensor([[0.2138, 0.2480, 0.3096, 0.2285]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2041, 0.2523, 0.3161, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1999, 0.2342, 0.3478, 0.2182]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 86, Total Timesteps: 1457, Episode Reward: -102\n",
      "Total reward obtained for episode 86: -102\n",
      "action probabilites: tensor([[0.2138, 0.2480, 0.3096, 0.2285]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2138, 0.2480, 0.3096, 0.2285]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2256, 0.2397, 0.3089, 0.2259]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2098, 0.2336, 0.3324, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2250, 0.2397, 0.3097, 0.2256]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2250, 0.2397, 0.3097, 0.2256]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2098, 0.2336, 0.3324, 0.2243]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2250, 0.2397, 0.3097, 0.2256]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2004, 0.2448, 0.3308, 0.2239]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 87, Total Timesteps: 1466, Episode Reward: -108\n",
      "Total reward obtained for episode 87: -108\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2250, 0.2397, 0.3097, 0.2256]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2519, 0.3166, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2519, 0.3166, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2519, 0.3166, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2519, 0.3166, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2132, 0.2479, 0.3103, 0.2286]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2038, 0.2515, 0.3171, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1995, 0.2332, 0.3488, 0.2185]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2038, 0.2515, 0.3171, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1995, 0.2332, 0.3488, 0.2185]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1849, 0.2146, 0.3944, 0.2061]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1691, 0.2173, 0.4110, 0.2026]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 88, Total Timesteps: 1489, Episode Reward: -122\n",
      "Total reward obtained for episode 88: -122\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2038, 0.2515, 0.3171, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2245, 0.2397, 0.3105, 0.2253]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2245, 0.2397, 0.3105, 0.2253]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2091, 0.2336, 0.3333, 0.2240]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 89, Total Timesteps: 1495, Episode Reward: -105\n",
      "Total reward obtained for episode 89: -105\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2126, 0.2478, 0.3109, 0.2287]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2038, 0.2515, 0.3171, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2038, 0.2515, 0.3171, 0.2276]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2036, 0.2512, 0.3175, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2000, 0.2441, 0.3320, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2240, 0.2398, 0.3112, 0.2250]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2085, 0.2336, 0.3341, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1866, 0.2315, 0.3652, 0.2166]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1632, 0.2283, 0.4048, 0.2037]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1632, 0.2283, 0.4048, 0.2037]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1484, 0.2278, 0.4270, 0.1968]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1304, 0.2335, 0.4542, 0.1819]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1182, 0.2188, 0.4994, 0.1635]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1304, 0.2335, 0.4542, 0.1819]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1490, 0.2321, 0.4284, 0.1904]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 90, Total Timesteps: 1512, Episode Reward: 84\n",
      "Total reward obtained for episode 90: 84\n",
      "action probabilites: tensor([[0.2121, 0.2477, 0.3114, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2036, 0.2512, 0.3175, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2036, 0.2512, 0.3175, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2000, 0.2441, 0.3320, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 91, Total Timesteps: 1516, Episode Reward: -103\n",
      "Total reward obtained for episode 91: -103\n",
      "action probabilites: tensor([[0.2121, 0.2477, 0.3114, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2121, 0.2477, 0.3114, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2240, 0.2398, 0.3112, 0.2250]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2240, 0.2398, 0.3112, 0.2250]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2398, 0.3118, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2079, 0.2336, 0.3349, 0.2236]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2398, 0.3118, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2398, 0.3118, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1999, 0.2438, 0.3325, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 92, Total Timesteps: 1525, Episode Reward: -108\n",
      "Total reward obtained for episode 92: -108\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2035, 0.2509, 0.3179, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2035, 0.2509, 0.3179, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2398, 0.3118, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2116, 0.2476, 0.3119, 0.2288]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2236, 0.2398, 0.3118, 0.2248]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1999, 0.2438, 0.3325, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2035, 0.2509, 0.3179, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1990, 0.2320, 0.3502, 0.2188]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1840, 0.2133, 0.3962, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1680, 0.2159, 0.4131, 0.2030]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1840, 0.2133, 0.3962, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1840, 0.2133, 0.3962, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1840, 0.2133, 0.3962, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1840, 0.2133, 0.3962, 0.2065]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1672, 0.1973, 0.4446, 0.1910]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1661, 0.1970, 0.4457, 0.1912]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1661, 0.1970, 0.4457, 0.1912]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1831, 0.2131, 0.3970, 0.2068]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1831, 0.2131, 0.3970, 0.2068]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1985, 0.2317, 0.3508, 0.2190]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1985, 0.2317, 0.3508, 0.2190]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2031, 0.2504, 0.3188, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2031, 0.2504, 0.3188, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2031, 0.2504, 0.3188, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2031, 0.2504, 0.3188, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1985, 0.2317, 0.3508, 0.2190]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 93, Total Timesteps: 1571, Episode Reward: -145\n",
      "Total reward obtained for episode 93: -145\n",
      "action probabilites: tensor([[0.2106, 0.2474, 0.3130, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2031, 0.2504, 0.3188, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1993, 0.2433, 0.3337, 0.2237]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2031, 0.2504, 0.3188, 0.2277]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2106, 0.2474, 0.3130, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2106, 0.2474, 0.3130, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2106, 0.2474, 0.3130, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2106, 0.2474, 0.3130, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2106, 0.2474, 0.3130, 0.2289]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2220, 0.2399, 0.3141, 0.2240]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2220, 0.2399, 0.3141, 0.2240]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2220, 0.2399, 0.3141, 0.2240]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2028, 0.2502, 0.3193, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2028, 0.2502, 0.3193, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2028, 0.2502, 0.3193, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2430, 0.3344, 0.2237]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 94, Total Timesteps: 1597, Episode Reward: -125\n",
      "Total reward obtained for episode 94: -125\n",
      "action probabilites: tensor([[0.2101, 0.2473, 0.3136, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2028, 0.2502, 0.3193, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1988, 0.2430, 0.3344, 0.2237]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2215, 0.2399, 0.3148, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1984, 0.2428, 0.3350, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 95, Total Timesteps: 1602, Episode Reward: -104\n",
      "Total reward obtained for episode 95: -104\n",
      "action probabilites: tensor([[0.2096, 0.2472, 0.3142, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1984, 0.2428, 0.3350, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1975, 0.2312, 0.3519, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 96, Total Timesteps: 1609, Episode Reward: -106\n",
      "Total reward obtained for episode 96: -106\n",
      "action probabilites: tensor([[0.2096, 0.2472, 0.3142, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1975, 0.2312, 0.3519, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1975, 0.2312, 0.3519, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1975, 0.2312, 0.3519, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1975, 0.2312, 0.3519, 0.2194]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 97, Total Timesteps: 1618, Episode Reward: -108\n",
      "Total reward obtained for episode 97: -108\n",
      "action probabilites: tensor([[0.2096, 0.2472, 0.3142, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2025, 0.2500, 0.3197, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2023, 0.2498, 0.3201, 0.2278]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1971, 0.2310, 0.3524, 0.2195]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1971, 0.2310, 0.3524, 0.2195]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1971, 0.2310, 0.3524, 0.2195]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1809, 0.2125, 0.3992, 0.2074]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1634, 0.1963, 0.4484, 0.1920]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1634, 0.1963, 0.4484, 0.1920]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1809, 0.2125, 0.3992, 0.2074]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1649, 0.2148, 0.4167, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1809, 0.2125, 0.3992, 0.2074]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1809, 0.2125, 0.3992, 0.2074]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1649, 0.2148, 0.4167, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1443, 0.2132, 0.4498, 0.1927]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1303, 0.1942, 0.4980, 0.1775]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1483, 0.1964, 0.4658, 0.1895]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1649, 0.2148, 0.4167, 0.2036]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 98, Total Timesteps: 1636, Episode Reward: -117\n",
      "Total reward obtained for episode 98: -117\n",
      "action probabilites: tensor([[0.2092, 0.2471, 0.3147, 0.2290]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2210, 0.2400, 0.3154, 0.2236]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1981, 0.2426, 0.3356, 0.2238]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2210, 0.2400, 0.3154, 0.2236]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2039, 0.2339, 0.3398, 0.2223]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 99, Total Timesteps: 1641, Episode Reward: -104\n",
      "Total reward obtained for episode 99: -104\n",
      "action probabilites: tensor([[0.2087, 0.2471, 0.3151, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2206, 0.2400, 0.3160, 0.2234]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2206, 0.2400, 0.3160, 0.2234]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2087, 0.2471, 0.3151, 0.2291]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2496, 0.3205, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.2020, 0.2496, 0.3205, 0.2279]], grad_fn=<SoftmaxBackward0>)\n",
      "action probabilites: tensor([[0.1978, 0.2424, 0.3361, 0.2237]], grad_fn=<SoftmaxBackward0>)\n",
      "Episode: 100, Total Timesteps: 1648, Episode Reward: -106\n",
      "Total reward obtained for episode 100: -106\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of episodes: 0.02\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVa0lEQVR4nO3dd3xT5f4H8E+StmlpaUuhEwqUIQWZMstQgbJEZRS5IGoZgkhFhoKiFxAUi3gFRBk/vAxBvChLLyhbtuwlyBBl9UJbEOhgdCXP7w84h6RJ26Rk9PR83q9XXpBzTpMnp0n76fc8QyOEECAiIiJSIK27G0BERERUXAwyREREpFgMMkRERKRYDDJERESkWAwyREREpFgMMkRERKRYDDJERESkWAwyREREpFgMMkRERKRYDDJUoO3bt0Oj0WD79u3ubgo98PTTT+Ppp5926GNqNBp88MEHDn3M0kSj0eCNN95wdzNKhdTUVPTq1Qvly5eHRqPBzJkz7fp6Z7z/beWOz0n//v1RtWpVlz6nEjHIlDAajcammy3h4uOPP8YPP/zg9DYDwIkTJ9CrVy9UqVIF3t7eqFixIjp06IAvvvjCbW0qrv79+0Oj0aB+/fqwtoKHGn+xXb16FR988AGOHTvm0MeVwrJ00+l0CAkJQa9evXD69GmHPpe7XL9+HSNGjEB0dDR8fHwQEhKCZs2a4Z133sHt27fd3TyXGjVqFDZu3Ihx48Zh6dKl6Ny5s1Oep2rVqgX+7HTWc5L7eLi7AWRu6dKlZveXLFmCzZs3W2yvXbt2kY/18ccfo1evXujevbsjm2jh119/Rdu2bVG5cmUMHjwYYWFhSEpKwr59+/D5559j+PDhLm+TI5w4cQKrV69GXFycu5vidlevXsWkSZNQtWpVNGzY0OGP/+abb6Jp06bIzc3Fb7/9hnnz5mH79u04efIkwsLCHP58rnLz5k00adIEGRkZGDhwIKKjo3Hjxg389ttvmDt3Ll5//XX4+fm5u5ku88svv6Bbt254++23nf5cDRs2xFtvvWWxPSIioliPd+/ePXh48FdmScTvSgnz0ksvmd3ft28fNm/ebLG9JJkyZQoCAgJw8OBBBAYGmu27du2aexr1iHx8fBAZGYnJkyejZ8+e0Gg07m5SqdamTRv06tVLvl+rVi28/vrrWLJkCcaOHevGlj2aBQsW4PLly9izZw9atmxpti8jIwNeXl5uapl7XLt2zeJnhLNUrFjRoT83vb29HfZY5Fi8tKRAd+7cwVtvvYXIyEjo9XrUqlUL//rXv8wug2g0Gty5cwdff/21XFLt378/AODSpUsYNmwYatWqBR8fH5QvXx4vvPACLl68WKz2/PXXX3j88cet/oAKCQmxqU0AcOXKFQwcOBChoaHQ6/V4/PHHsXDhQrPHky5FfPfdd3jvvfcQFhYGX19fPP/880hKSjI79ty5c4iLi0NYWBi8vb1RqVIl9OnTB+np6UW+Jq1Wi3/+85/47bffsGbNmiKPz87OxsSJE1GjRg3o9XpERkZi7NixyM7Otjj2m2++QePGjeHj44OgoCD06dPHou0AMH/+fFSvXh0+Pj5o1qwZdu3a9UjPnZ2djVGjRiE4OBhly5bF888/j//9739Fvrbt27ejadOmAIABAwbI37vFixfLx6xYsUJ+TRUqVMBLL72EK1euFPnYBWnTpg2A++8tU7a8R3JycjBhwgQ0btwYAQEB8PX1RZs2bbBt2zaL5zEajfj8889Rr149eHt7Izg4GJ07d8ahQ4csjv3hhx9Qt25d+Xk3bNhQ5Ov466+/oNPp0KJFC4t9/v7+Zr8cq1atavZ5kFjrF5KVlYUPPvgAjz32GLy9vREeHo6ePXuanS9bX5st70dbPkubN29G69atERgYCD8/P9SqVQvvvfceAGDx4sXQaDQQQmD27NnyewgAPvjgA6t/KEhfU9yfS7bo378//Pz8cP78eXTq1Am+vr6IiIjA5MmTLS4r5+8jk5mZiZEjR6Jq1arQ6/UICQlBhw4dcOTIEbOvs/WzIb2/vL29Ubdu3QJ/7hiNRsycOROPP/44vL29ERoaitdeew23bt0yO+7QoUPo1KkTKlSoAB8fH0RFRWHgwIHFPFMlGysyCiOEwPPPP49t27Zh0KBBaNiwITZu3IgxY8bgypUrmDFjBoD7l6heffVVNGvWDEOGDAEAVK9eHQBw8OBB/Prrr+jTpw8qVaqEixcvYu7cuXj66adx6tQplClTxq42ValSBXv37sXJkydRt27dAo8rrE2pqalo0aKF3P8kODgY69evx6BBg5CRkYGRI0eaPdaUKVOg0Wjwzjvv4Nq1a5g5cyZiY2Nx7Ngx+Pj4ICcnB506dUJ2djaGDx+OsLAwXLlyBevWrUNaWhoCAgKKfF0vvvgiPvzwQ0yePBk9evQosCpjNBrx/PPPY/fu3RgyZAhq166NEydOYMaMGfjjjz/M+gRNmTIF48ePR+/evfHqq6/i+vXr+OKLL/Dkk0/i6NGjchhcsGABXnvtNbRs2RIjR47E+fPn8fzzzyMoKAiRkZHFeu5XX30V33zzDV588UW0bNkSv/zyC7p27VrkeahduzYmT56MCRMmYMiQIXLIkCoMixcvxoABA9C0aVMkJiYiNTUVn3/+Ofbs2WP2muwh/fIqV66cvM3W90hGRgb+/e9/o2/fvhg8eDAyMzOxYMECdOrUCQcOHDC7NDZo0CAsXrwYXbp0wauvvoq8vDzs2rUL+/btQ5MmTeTjdu/ejdWrV2PYsGEoW7YsZs2ahbi4OFy+fBnly5cv8HVUqVIFBoMBS5cuRXx8vN3nwRqDwYBnn30WW7duRZ8+fTBixAhkZmZi8+bNOHnypPyZsuW12fJ+tOWz9Pvvv+PZZ59F/fr1MXnyZOj1evz555/Ys2cPAODJJ5/E0qVL8fLLL6NDhw545ZVXHHIuCpObm4u///7bYruvry98fHzMzmfnzp3RokULTJs2DRs2bMDEiRORl5eHyZMnF/j4Q4cOxcqVK/HGG2+gTp06uHHjBnbv3o3Tp0/jiSeeAGD7Z2PTpk2Ii4tDnTp1kJiYiBs3bmDAgAGoVKmSxfO+9tpr8uO++eabuHDhAr788kscPXoUe/bsgaenJ65du4aOHTsiODgY7777LgIDA3Hx4kWsXr36Ec9qCSWoREtISBCm36YffvhBABAfffSR2XG9evUSGo1G/Pnnn/I2X19fER8fb/GYd+/etdi2d+9eAUAsWbJE3rZt2zYBQGzbtq3QNm7atEnodDqh0+lETEyMGDt2rNi4caPIycmxOLagNg0aNEiEh4eLv//+22x7nz59REBAgNxmqU0VK1YUGRkZ8nHff/+9ACA+//xzIYQQR48eFQDEihUrCm27NfHx8cLX11cIIcTXX38tAIjVq1fL+wGIhIQE+f7SpUuFVqsVu3btMnucefPmCQBiz549QgghLl68KHQ6nZgyZYrZcSdOnBAeHh7y9pycHBESEiIaNmwosrOz5ePmz58vAIinnnrK7uc+duyYACCGDRtmdtyLL74oAIiJEycWek4OHjwoAIhFixaZbZfaWrduXXHv3j15+7p16wQAMWHChEIfV/p+Lly4UFy/fl1cvXpVbNiwQdSoUUNoNBpx4MAB+Vhb3yN5eXlm500IIW7duiVCQ0PFwIED5W2//PKLACDefPNNi3YZjUb5/wCEl5eX2Wfr+PHjAoD44osvCn19KSkpIjg4WAAQ0dHRYujQoeLbb78VaWlpFsdWqVLF6mfjqaeeMvueL1y4UAAQ06dPL7Ddtrw2W9+PtnyWZsyYIQCI69evF3iMEJafHSGEmDhxorD2q2jRokUCgLhw4YK8Lf+5KEiVKlUEAKu3xMRE+bj4+HgBQAwfPlzeZjQaRdeuXYWXl5fZ68n/OQkICLB4Labs+Ww0bNhQhIeHm70vNm3aJACIKlWqyNt27dolAIhly5aZPdeGDRvMtq9Zs0YAEAcPHizyXJUGvLSkMD///DN0Oh3efPNNs+1vvfUWhBBYv359kY9h+tdIbm4ubty4gRo1aiAwMNCiLGqLDh06YO/evXj++edx/PhxTJs2DZ06dULFihXx3//+t8ivF0Jg1apVeO655yCEwN9//y3fOnXqhPT0dIt2vfLKKyhbtqx8v1evXggPD8fPP/8MAHLFZePGjbh7967dr0nSr18/1KxZ02qpWbJixQrUrl0b0dHRZm1v164dAMiXNFavXg2j0YjevXubHRcWFoaaNWvKxx06dAjXrl3D0KFDzfpQ9O/f36KSZOtzS+cl//smf6XLXlJbhw0bZnaZpGvXroiOjsZPP/1k0+MMHDgQwcHBiIiIQOfOnZGeno6lS5fKl7TseY/odDr5vBmNRty8eRN5eXlo0qSJ2fto1apV0Gg0mDhxokV78lffYmNj5UoHANSvXx/+/v44f/58oa8rNDQUx48fx9ChQ3Hr1i3MmzcPL774IkJCQvDhhx8W+J4qzKpVq1ChQgWzTvT5223La7P1/WjLZ0mqLPz4448wGo12vyZnaN68OTZv3mxx69u3r8WxpqMQpYpfTk4OtmzZUuDjBwYGYv/+/bh69arV/bZ+NpKTk3Hs2DHEx8ebfb47dOiAOnXqmD3mihUrEBAQgA4dOph9zxo3bgw/Pz/5eyZ9P9atW4fc3NwizpTyMcgozKVLlxAREWH2Sxx4OIrp0qVLRT7GvXv3MGHCBLmPTYUKFRAcHIy0tDSb+o9Y07RpU6xevRq3bt3CgQMHMG7cOGRmZqJXr144depUoV97/fp1pKWlYf78+QgODja7DRgwAIBlp+GaNWua3ddoNKhRo4Z8SSIqKgqjR4/Gv//9b1SoUAGdOnXC7Nmz7X59Op0O//znP3Hs2LECh42fO3cOv//+u0XbH3vsMbO2nzt3DkII1KxZ0+LY06dPy8dJ38P8r9HT0xPVqlUr1nNfunQJWq3W7JcxcL9T7aOQ2mrtcaKjo216PwLAhAkTsHnzZqxZswavvPIK0tPTodU+/PFk73vk66+/Rv369eHt7Y3y5csjODgYP/30k9n3/6+//kJERASCgoKKbF/lypUttpUrV86iX4I14eHhmDt3LpKTk3H27FnMmjULwcHBmDBhAhYsWFDk1+f3119/oVatWoWOoLHltdn6frTls/SPf/wDrVq1wquvvorQ0FD06dMH33//vVtDTYUKFRAbG2txq1KlitlxWq3W4nMlfX4K658zbdo0nDx5EpGRkWjWrBk++OADs2Br62ejoM+7ta89d+4c0tPTERISYvE9u337tvw9e+qppxAXF4dJkyahQoUK6NatGxYtWmS1z15pwD4yKjR8+HAsWrQII0eORExMDAICAqDRaNCnT59H/sHj5eWFpk2bomnTpnjssccwYMAArFixwupfhhLpOV966aUC+xHUr1/f7rZ89tln6N+/P3788Uds2rQJb775JhITE7Fv3z6r154L0q9fP7mvjLVh40ajEfXq1cP06dOtfr3Up8VoNEKj0WD9+vXQ6XQWxxVnGK6tz13S1atXD7GxsQCA7t274+7duxg8eDBat26NyMhIu94j33zzDfr374/u3btjzJgxCAkJgU6nQ2JiokXnYVtZ+34BsKuiotFo8Nhjj+Gxxx5D165dUbNmTSxbtgyvvvqqvN8ag8FQ4PM/Cnvej0V9lnx8fLBz505s27YNP/30EzZs2IDvvvsO7dq1w6ZNmwptf2GvuyTr3bs32rRpgzVr1mDTpk349NNP8cknn2D16tXo0qWLU57TaDQiJCQEy5Yts7o/ODgYwP1zunLlSuzbtw9r167Fxo0bMXDgQHz22WfYt29fqRvyzyCjMFWqVMGWLVuQmZlpVpU5c+aMvF9S0A+IlStXIj4+Hp999pm8LSsrC2lpaQ5tq9ShMDk5udA2SaNoDAaD/MusKOfOnTO7L4TAn3/+aRF46tWrh3r16uGf//wnfv31V7Rq1Qrz5s3DRx99ZPPrkKoy0g/y/KpXr47jx4+jffv2hQ7Trl69OoQQiIqKkv/is0b6Hp47d06+RATcvwx44cIFNGjQwO7nrlKlCoxGo/zXvOTs2bMFfo2pgh5bauvZs2fN2ipty//Xr62mTp2KNWvWYMqUKZg3b55d75GVK1eiWrVqWL16tVm784fp6tWrY+PGjbh586ZNVRlHqlatGsqVK2f22ShXrpzVz+ClS5fMKgbVq1fH/v37kZubC09PT6uPb8trs/X9KCnqs6TVatG+fXu0b98e06dPx8cff4z3338f27ZtK/R7JnXoTktLM+sYbms171EZjUacP3/e7Bz88ccfAFDkrLrh4eEYNmwYhg0bhmvXruGJJ57AlClT0KVLF5s/G6af9/zyfz6rV6+OLVu2oFWrVmZdBArSokULtGjRAlOmTMG3336Lfv36Yfny5XJ4Li14aUlhnnnmGRgMBnz55Zdm22fMmAGNRmP2l4Cvr6/VH4w6nc7iL8kvvvii2H8Bbdu2zepfplK/DNNfnNbapNPpEBcXh1WrVuHkyZMWj3P9+nWLbUuWLEFmZqZ8f+XKlUhOTpZff0ZGBvLy8sy+pl69etBqtcUqr7700kuoUaMGJk2aZLGvd+/euHLlCr766iuLfffu3cOdO3cAAD179oROp8OkSZMszpcQAjdu3ABwPwAGBwdj3rx5yMnJkY9ZvHixxbmz9bml8zJr1iyzY2ydIt7X1xcALJ6/SZMmCAkJwbx588zO6/r163H69GmbRkVZU716dcTFxWHx4sVISUmx6z0i/fVveo7379+PvXv3mn1NXFwchBBWv6fF6btizf79++XvgakDBw7gxo0bZp+N6tWrY9++fWbf83Xr1lkMhY6Li8Pff/9t8TPAtN22vDZb34+2fJZu3rxp8TzS6LCiPm/S5c6dO3fK26RpGlzF9FwKIfDll1/C09MT7du3t3q8wWCwuEwdEhKCiIgI+fXa+tkIDw9Hw4YN8fXXX1sMZ89/Wb53794wGAz48MMPLdqUl5cnfz5v3bpl8T219fuhRKzIKMxzzz2Htm3b4v3338fFixfRoEEDbNq0CT/++CNGjhxp1geicePG2LJlC6ZPn46IiAhERUWhefPmePbZZ7F06VIEBASgTp062Lt3L7Zs2VLoMNLCDB8+HHfv3kWPHj0QHR2NnJwc/Prrr/juu+9QtWpVuQ9DYW2aOnUqtm3bhubNm2Pw4MGoU6cObt68iSNHjmDLli0WPyiDgoLQunVrDBgwAKmpqZg5cyZq1KiBwYMHA7g/g+gbb7yBF154AY899hjy8vKwdOlS+ReivXQ6Hd5//32z1yJ5+eWX8f3332Po0KHYtm0bWrVqBYPBgDNnzuD777/Hxo0b0aRJE1SvXh0fffQRxo0bh4sXL6J79+4oW7YsLly4gDVr1mDIkCF4++234enpiY8++givvfYa2rVrh3/84x+4cOECFi1aZHEt39bnbtiwIfr27Ys5c+YgPT0dLVu2xNatW/Hnn3/a9PqrV6+OwMBAzJs3D2XLloWvry+aN2+OqKgofPLJJxgwYACeeuop9O3bVx5iWrVqVYwaNcrucy0ZM2YMvv/+e8ycORNTp061+T3y7LPPYvXq1ejRowe6du2KCxcuYN68eahTp47ZkgBt27bFyy+/jFmzZuHcuXPo3LkzjEYjdu3ahbZt2zpkGYqlS5di2bJl6NGjBxo3bgwvLy+cPn0aCxcuhLe3tzzPCnB/ePzKlSvRuXNn9O7dG3/99Re++eYbi35Nr7zyCpYsWYLRo0fjwIEDaNOmDe7cuYMtW7Zg2LBh6Natm02vzdb3oy2fpcmTJ2Pnzp3o2rUrqlSpgmvXrmHOnDmoVKkSWrduXeg56tixIypXroxBgwZhzJgx0Ol0WLhwIYKDg3H58uVin/srV67gm2++sdju5+dndonY29sbGzZsQHx8PJo3b47169fjp59+wnvvvSdfqskvMzMTlSpVQq9evdCgQQP4+flhy5YtOHjwoFzp9vT0tPmzkZiYiK5du6J169YYOHAgbt68iS+++AKPP/642Xv2qaeewmuvvYbExEQcO3YMHTt2hKenJ86dO4cVK1bg888/R69evfD1119jzpw56NGjB6pXr47MzEx89dVX8Pf3xzPPPFPsc1piuWh0FBVT/uHXQgiRmZkpRo0aJSIiIoSnp6eoWbOm+PTTT82GjAohxJkzZ8STTz4pfHx8BAB5aOetW7fEgAEDRIUKFYSfn5/o1KmTOHPmjMXwT1uHX69fv14MHDhQREdHCz8/P+Hl5SVq1Kghhg8fLlJTU21qkxBCpKamioSEBBEZGSk8PT1FWFiYaN++vZg/f75Fm/7zn/+IcePGiZCQEOHj4yO6du0qLl26JB93/vx5MXDgQFG9enXh7e0tgoKCRNu2bcWWLVuKPOemw69N5ebmiurVq1sdQpqTkyM++eQT8fjjjwu9Xi/KlSsnGjduLCZNmiTS09PNjl21apVo3bq18PX1Fb6+viI6OlokJCSIs2fPmh03Z84cERUVJfR6vWjSpInYuXOn1eGntj73vXv3xJtvvinKly8vfH19xXPPPSeSkpJsGn4thBA//vijqFOnjvDw8LAYiv3dd9+JRo0aCb1eL4KCgkS/fv3E//73vyIfU/p+FjS09+mnnxb+/v7ysFRb3iNGo1F8/PHHokqVKkKv14tGjRqJdevWifj4eLOhrELcH6r96aefiujoaOHl5SWCg4NFly5dxOHDh+VjrH2/hSh4uLSp3377TYwZM0Y88cQTIigoSHh4eIjw8HDxwgsviCNHjlgc/9lnn4mKFSsKvV4vWrVqJQ4dOmT1e3737l3x/vvvi6ioKPk89OrVS/z11192vTYhin4/2vJZ2rp1q+jWrZuIiIgQXl5eIiIiQvTt21f88ccfZs9V0Lk8fPiwaN68ufDy8hKVK1cW06dPd9rwa9P3gPRZ/+uvv0THjh1FmTJlRGhoqJg4caIwGAwWbZc+J9nZ2WLMmDGiQYMGomzZssLX11c0aNBAzJkzx6Ittn42Vq1aJWrXri30er2oU6eOWL16tdX3rBD3p2Jo3Lix8PHxEWXLlhX16tUTY8eOFVevXhVCCHHkyBHRt29fUblyZaHX60VISIh49tlnxaFDh4o8d0qkEcJBNVQiF9i+fTvatm2LFStWmE1pT0Rkr/79+2PlypWqW7yztGEfGSIiIlIsBhkiIiJSLAYZIiIiUiz2kSEiIiLFYkWGiIiIFItBhoiIiBSr1E+IZzQacfXqVZQtW7bQKdyJiIio5BBCIDMzExEREWaLyOZX6oPM1atXFbNwHhEREZlLSkoqdKHfUh9kpIUVk5KS4O/v7+bWEBERkS0yMjIQGRlptkCyNaU+yEiXk/z9/RlkiIiIFKaobiHs7EtERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIEOqdy/H4O4mEBFRMTHIkKpN23AGDSZvwqmrGe5uChERFQODDKnasaQ05OQZcSaFQYaISIkYZEjVDEZh9i8RESkLgwypGoMMEZGyMciQqhmEMPuXiIiUhUGGVI0VGSIiZWOQIVVjkCEiUjYGGVI1BhkiImVjkCFVY5AhIlI2BhlSNSnA5DHIEBEpEoMMqZo0WsnIIENEpEgMMqRqeQZWZIiIlIxBhlTNKFVkOI8MEZEilZggM3XqVGg0GowcOVLelpWVhYSEBJQvXx5+fn6Ii4tDamqq+xpJpQ47+xIRKVuJCDIHDx7E//3f/6F+/fpm20eNGoW1a9dixYoV2LFjB65evYqePXu6qZVUGjHIEBEpm9uDzO3bt9GvXz989dVXKFeunLw9PT0dCxYswPTp09GuXTs0btwYixYtwq+//op9+/a5scVUmshLFDDIEBEpktuDTEJCArp27YrY2Fiz7YcPH0Zubq7Z9ujoaFSuXBl79+4t8PGys7ORkZFhdiMqiIGdfYmIFM3DnU++fPlyHDlyBAcPHrTYl5KSAi8vLwQGBpptDw0NRUpKSoGPmZiYiEmTJjm6qVRKGdjZl4hI0dxWkUlKSsKIESOwbNkyeHt7O+xxx40bh/T0dPmWlJTksMem0iePE+IRESma24LM4cOHce3aNTzxxBPw8PCAh4cHduzYgVmzZsHDwwOhoaHIyclBWlqa2delpqYiLCyswMfV6/Xw9/c3uxEVRJoIjxPiEREpk9suLbVv3x4nTpww2zZgwABER0fjnXfeQWRkJDw9PbF161bExcUBAM6ePYvLly8jJibGHU2mUogVGSIiZXNbkClbtizq1q1rts3X1xfly5eXtw8aNAijR49GUFAQ/P39MXz4cMTExKBFixbuaDKVMqZVGFZkiIiUya2dfYsyY8YMaLVaxMXFITs7G506dcKcOXPc3SwqJUyrMKzIEBEpU4kKMtu3bze77+3tjdmzZ2P27NnuaRCVaqYjlQwctUREpEhun0eGyF1MqzDSfDJERKQsDDKkWqaz+bIiQ0SkTAwypFpmQYZ9ZIiIFIlBhlSLQYaISPkYZEi1TMMLlyggIlImBhlSLdN+MXns7EtEpEgMMqRapiOV2NmXiEiZGGRItUzDC/vIEBEpE4MMqRY7+xIRKR+DDKkWgwwRkfIxyJBqMcgQESkfgwypFoMMEZHyMciQahm4aCQRkeIxyJBqGYxGk/8zyBARKRGDDKmWwWj6fwYZIiIlYpAh1cpjRYaISPEYZEi1jKzIEBEpHoMMqZZpRSaPQYaISJEYZEi1TFe85urXRETKxCBDqmW64nWeac9fIiJSDAYZUi3ziowbG0JERMXGIEOqlceZfYmIFI9BhlSLSxQQESkfgwypllmQYWdfIiJFYpAh1cpfkREMM0REisMgQ6qVf8g1ry4RESkPgwypVv5J8EwnyCMiImVgkCHVMuYLMswxRETKwyBDqsWKDBGR8jHIkGrlH3LNHENEpDwMMqRa+YMMKzJERMrDIEOqlX/uGM4lQ0SkPAwypFoGQ74gw/HXRESKwyBDqmVRkWGQISJSHAYZUq38wYVBhohIeRhkSLUYZIiIlI9BhlTLYvg1O/sSESkOgwypluXwawYZIiKlYZAh1cofXHhpiYhIeRhkSLXyX0pikCEiUh4GGVItVmSIiJSPQYZUK//q1wwyRETKwyBDqsWKDBGR8jHIkGqxIkNEpHwMMqRaXDSSiEj5GGRItfJfWuI8MkREysMgQ6qV/9JS/vtERFTyMciQarEiQ0SkfAwypFqsyBARKR+DDKkWKzJERMrHIEOqlX+JAq5+TUSkPAwypFp5BlHofSIiKvkYZEi1OI8MEZHyMciQauWfyZedfYmIlIdBhlRLCjIeWg0AdvYlIlIiBhlSLSnIeHnc/xiwsy8RkfIwyJBq5Q8y7OxLRKQ8DDKkWnKQ0bEiQ0SkVAwypFrSKCW5IsM+MkREisMgQ6qV/9JS/lFMRERU8jHIkGrlv7TEIENEpDwMMqRaUnDRsyJDRKRYDDKkWry0RESkfAwypFr5O/tyiQIiIuVhkCHVkiownuwjQ0SkWAwypFrs7EtEpHwMMqRa7CNDRKR8DDKkWgwyRETKxyBDqmUx/JqdfYmIFMetQWbu3LmoX78+/P394e/vj5iYGKxfv17en5WVhYSEBJQvXx5+fn6Ii4tDamqqG1tMpYk8aknqI8NFI4mIFMetQaZSpUqYOnUqDh8+jEOHDqFdu3bo1q0bfv/9dwDAqFGjsHbtWqxYsQI7duzA1atX0bNnT3c2mUoJIYTlpSVWZIiIFMfDnU/+3HPPmd2fMmUK5s6di3379qFSpUpYsGABvv32W7Rr1w4AsGjRItSuXRv79u1DixYt3NFkKiVMu8OwjwwRkXKVmD4yBoMBy5cvx507dxATE4PDhw8jNzcXsbGx8jHR0dGoXLky9u7dW+DjZGdnIyMjw+xGlF+e0Sj/30unA8AgQ0SkRG4PMidOnICfnx/0ej2GDh2KNWvWoE6dOkhJSYGXlxcCAwPNjg8NDUVKSkqBj5eYmIiAgAD5FhkZ6eRXQEpkkmNYkSEiUjC3B5latWrh2LFj2L9/P15//XXEx8fj1KlTxX68cePGIT09Xb4lJSU5sLVUWphVZBhkiIgUy619ZADAy8sLNWrUAAA0btwYBw8exOeff45//OMfyMnJQVpamllVJjU1FWFhYQU+nl6vh16vd3azSeGsVWTyGGSIiBTH7RWZ/IxGI7Kzs9G4cWN4enpi69at8r6zZ8/i8uXLiImJcWMLqTQw7yOjAQAYOWqJiEhx3FqRGTduHLp06YLKlSsjMzMT3377LbZv346NGzciICAAgwYNwujRoxEUFAR/f38MHz4cMTExHLFEj0waaq3VAB5aVmSIiJTKrUHm2rVreOWVV5CcnIyAgADUr18fGzduRIcOHQAAM2bMgFarRVxcHLKzs9GpUyfMmTPHnU2mUkIqyOi0Gui0DyoyDDJERIrj1iCzYMGCQvd7e3tj9uzZmD17totaRGohXVrSah4GGdPLTUREpAwlro8MkStImcXDrCLjxgYREVGxMMiQKskVGS0rMkRESsYgQ6okjVDy0Gqg09wPMlwzkohIeRhkSJWkEUqmnX0NrMgQESkOgwypksFqkHFni4iIqDgYZEiV5CCj4fBrIiIlY5AhVZKDjI6dfYmIlIxBhlTJakWGBRkiIsV55CCTkZGBH374AadPn3ZEe4hcwlofGVZkiIiUx+4g07t3b3z55ZcAgHv37qFJkybo3bs36tevj1WrVjm8gUTOYBZkNJwQj4hIqewOMjt37kSbNm0AAGvWrIEQAmlpaZg1axY++ugjhzeQyBmkRSN1Wi0rMkRECmZ3kElPT0dQUBAAYMOGDYiLi0OZMmXQtWtXnDt3zuENJHKGh/PIgMOviYgUzO4gExkZib179+LOnTvYsGEDOnbsCAC4desWvL29Hd5AImcwGh9WZDw4IR4RkWLZvfr1yJEj0a9fP/j5+aFKlSp4+umnAdy/5FSvXj1Ht4/IKeSKjOb+ekvAw34zRESkHHYHmWHDhqFZs2ZISkpChw4doNXeL+pUq1aNfWRIMaSKjIdZRYZBhohIaewOMgDQpEkTNGnSBABgMBhw4sQJtGzZEuXKlXNo44icRarIaLWAVl40kkGGiEhp7O4jM3LkSCxYsADA/RDz1FNP4YknnkBkZCS2b9/u6PYROYVRPBx+7aFjRYaISKnsDjIrV65EgwYNAABr167FhQsXcObMGYwaNQrvv/++wxtI5AwGk86+0jwyDDJERMpjd5D5+++/ERYWBgD4+eef8cILL+Cxxx7DwIEDceLECYc3kMgZTDv7mi5RIHh5iYhIUewOMqGhoTh16hQMBgM2bNiADh06AADu3r0LnU7n8AYSOYPp8GspyACsyhARKY3dnX0HDBiA3r17Izw8HBqNBrGxsQCA/fv3Izo62uENJHIGaxPiSds9mMeJiBTD7iDzwQcfoG7dukhKSsILL7wAvV4PANDpdHj33Xcd3kAiZ5A6+3rkq8gYeWmJiEhRijX8ulevXgCArKwseVt8fLxjWkTkAnkGafi1Rh5+DfDSEhGR0tjdR8ZgMODDDz9ExYoV4efnh/PnzwMAxo8fLw/LJirpHlZkNPKEeACDDBGR0tgdZKZMmYLFixdj2rRp8PLykrfXrVsX//73vx3aOCJnkSfE02jY2ZeISMHsDjJLlizB/Pnz0a9fP7NRSg0aNMCZM2cc2jgiZzEYH1ZkNBoNpCzDIENEpCx2B5krV66gRo0aFtuNRiNyc3Md0igiZzMYH/aRAR6OXOIyBUREymJ3kKlTpw527dplsX3lypVo1KiRQxpF5GymFRngYZCROgETEZEy2D1qacKECYiPj8eVK1dgNBqxevVqnD17FkuWLMG6deuc0UYih3u4RMGDIKORZvdlkCEiUhK7KzLdunXD2rVrsWXLFvj6+mLChAk4ffo01q5dK8/yS1TSGUS+ICNVZNhHhohIUYo1j0ybNm2wefNmR7eFyGUsKjLSeksMMkREimJ3RebgwYPYv3+/xfb9+/fj0KFDDmkUkbMZTIZfA/fXXAJYkSEiUhq7g0xCQgKSkpIstl+5cgUJCQkOaRSRs1l29jXfTkREymB3kDl16hSeeOIJi+2NGjXCqVOnHNIoImfLP/za40FFhkGGiEhZ7A4yer0eqampFtuTk5Ph4VGsLjdELmcQ5hUZrdZ8OxERKYPdQaZjx44YN24c0tPT5W1paWl47733OGqJFMNgMO/sy4oMEZEy2V1C+de//oUnn3wSVapUkSfAO3bsGEJDQ7F06VKHN5DIGfIPv+YSBUREymR3kKlYsSJ+++03LFu2DMePH4ePjw8GDBiAvn37wtPT0xltJHI4efi1xrwiw+HXRETKUqxOLb6+vhgyZIij20LkMvnnkdFyQjwiIkWyu49MYmIiFi5caLF94cKF+OSTTxzSKCJns5wQ78F2dvYlIlIUu4PM//3f/yE6Otpi++OPP4558+Y5pFFEzmYZZB509uWikUREimJ3kElJSUF4eLjF9uDgYCQnJzukUUTOlmexaOT97azIEBEpi91BJjIyEnv27LHYvmfPHkRERDikUUTOZhQcfk1EVBrY3dl38ODBGDlyJHJzc9GuXTsAwNatWzF27Fi89dZbDm8gkTPk5Ru1pOUSBUREimR3kBkzZgxu3LiBYcOGIScnBwDg7e2Nd955B+PGjXN4A4mcQRpm7aFjRYaISMnsDjIajQaffPIJxo8fj9OnT8PHxwc1a9aEXq93RvuInCLPaATwcPVrafg1gwwRkbIUe3EkPz8/NG3a1JFtIXKZBzlGXmvJg0GGiEiR7A4ybdu2hebBX7HW/PLLL4/UICJXkCsy8hIFD4IMRy0RESmK3UGmYcOGZvdzc3Nx7NgxnDx5EvHx8Y5qF5FTSdPFPFyigDP7EhEpkd1BZsaMGVa3f/DBB7h9+/YjN4jIFQwPKjI6nTQh3v1/udYSEZGy2D2PTEFeeuklq0sXEJVEhgd9ZKSKjI4VGSIiRXJYkNm7dy+8vb0d9XBETiVVZDy0rMgQESmZ3ZeWevbsaXZfCIHk5GQcOnQI48ePd1jDiJxJGp2k1bIiQ0SkZHYHmYCAALP7Wq0WtWrVwuTJk9GxY0eHNYzImaS8IldkHlxiMnLUEhGRotgdZBYtWuSMdhC5VP7h11KnX84jQ0SkLMWeEA8AsrKy8N133+HOnTvo0KEDatas6ah2ETlV/gnxpIoMLy0RESmLzUFm9OjRyM3NxRdffAEAyMnJQYsWLXDq1CmUKVMGY8eOxebNmxETE+O0xhI5Sv4lCtjZl4hImWwetbRp0yZ06NBBvr9s2TJcvnwZ586dw61bt/DCCy/go48+ckojiRxNGn7toWNnXyIiJbM5yFy+fBl16tSR72/atAm9evVClSpVoNFoMGLECBw9etQpjSRyNHlCvPwVGXb2JSJSFJuDjFarhTD5Ib9v3z60aNFCvh8YGIhbt245tnVETiJ16tXlH35tYJAhIlISm4NM7dq1sXbtWgDA77//jsuXL6Nt27by/kuXLiE0NNTxLSRyAosgw+HXRESKZHNn37Fjx6JPnz746aef8Pvvv+OZZ55BVFSUvP/nn39Gs2bNnNJIIkeTVrm2qMhIw5mIiEgRbK7I9OjRAz///DPq16+PUaNG4bvvvjPbX6ZMGQwbNszhDSRyhoIuLRmYY4iIFMWueWTat2+P9u3bW903ceJEhzSIyBXkIKPJH2SYZIiIlMRhi0YSKYUQQl6igBUZIiJlY5Ah1TFdhkAKMB6syBARKRKDDKlOnpUgI83wy9HXRETKwiBDqmM6xFquyOhYkSEiUqJiLxp5/fp1nD17FgBQq1YtBAcHO6xRRM5UaEWGSxQQESmK3RWZO3fuYODAgYiIiMCTTz6JJ598EhERERg0aBDu3r1r12MlJiaiadOmKFu2LEJCQtC9e3c5HEmysrKQkJCA8uXLw8/PD3FxcUhNTbW32UQy04UhpVFLHuzsS0SkSHYHmdGjR2PHjh3473//i7S0NKSlpeHHH3/Ejh078NZbb9n1WDt27EBCQgL27duHzZs3Izc3Fx07dsSdO3fkY0aNGoW1a9dixYoV2LFjB65evYqePXva22wimbXOvlp29iUiUiS7Ly2tWrUKK1euxNNPPy1ve+aZZ+Dj44PevXtj7ty5Nj/Whg0bzO4vXrwYISEhOHz4MJ588kmkp6djwYIF+Pbbb9GuXTsAwKJFi1C7dm2LtZ6IbCUFGa0G0OSvyPDKEhGRothdkbl7967VNZVCQkLsvrSUX3p6OgAgKCgIAHD48GHk5uYiNjZWPiY6OhqVK1fG3r17rT5GdnY2MjIyzG5EpqTlCTy0D9/+nBCPiEiZ7A4yMTExmDhxIrKysuRt9+7dw6RJkxATE1PshhiNRowcORKtWrVC3bp1AQApKSnw8vJCYGCg2bGhoaFISUmx+jiJiYkICAiQb5GRkcVuE5VO0grXJjnGJMiwJENEpCR2X1qaOXMmOnfujEqVKqFBgwYAgOPHj8Pb2xsbN24sdkMSEhJw8uRJ7N69u9iPAQDjxo3D6NGj5fsZGRkMM2TGaK0iw1FLRESKZHeQqVevHs6dO4dly5bhzJkzAIC+ffuiX79+8PHxKVYj3njjDaxbtw47d+5EpUqV5O1hYWHIyclBWlqaWVUmNTUVYWFhVh9Lr9dDr9cXqx2kDnkmfWQkrMgQESmT3UFm586daNmyJQYPHmy2PS8vDzt37sSTTz5p82MJITB8+HCsWbMG27dvR1RUlNn+xo0bw9PTE1u3bkVcXBwA4OzZs7h8+fIjXcYidZOGX3vorPWRYZAhIlISu4NM27ZtkZycjJCQELPt6enpaNu2LQwGg82PlZCQgG+//RY//vgjypYtK/d7CQgIgI+PDwICAjBo0CCMHj0aQUFB8Pf3x/DhwxETE8MRS1RsDysyD0sy8vBrwSBDRKQkdgcZIYQ8ZNXUjRs34Ovra9djSUO1TYdyA/eHWPfv3x8AMGPGDGi1WsTFxSE7OxudOnXCnDlz7G02kUyquniYXFuS/p/H8ddERIpic5CRJqHTaDTo37+/WT8Ug8GA3377DS1btrTryYUNf/16e3tj9uzZmD17tl2PTVQQKcjoTIKM1NnXyIoMEZGi2BxkAgICANwPH2XLljXr2Ovl5YUWLVpY9JshKomky0fWhl/nsY8MEZGi2BxkFi1aBACoWrUq3n77bbsvIxGVFA8vLVl29jUyyBARKYrdfWQmTpzojHYQuYyhkOHXrMgQESmL3TP7EikdKzJERKUHgwypjlyRMe3sy4oMEZEiMciQ6lgbfi1XZDhqiYhIURwSZNLS0hzxMEQuYa0i48GZfYmIFMnuIPPJJ5/gu+++k+/37t0b5cuXR8WKFXH8+HGHNo7IGQzCsiIjzfLLS0tERMpid5CZN2+evJr05s2bsXnzZqxfvx5dunTBmDFjHN5AIkeTJ8TTmFZk7n8U2NmXiEhZ7B5+nZKSIgeZdevWoXfv3ujYsSOqVq2K5s2bO7yBRI5mbWZfaQATKzJERMpid0WmXLlySEpKAgBs2LABsbGxAO7P+GvPgpFE7mItyMgVGXb2JSJSFLsrMj179sSLL76ImjVr4saNG+jSpQsA4OjRo6hRo4bDG0jkaKzIEBGVHnYHmRkzZqBq1apISkrCtGnT4OfnBwBITk7GsGHDHN5AIkcrrCIjxP1+MqYjmoiIqOSyO8h4enri7bffttg+atQohzSIyNmkUUvWVr+W9mvBIENEpAR2BxkAOHv2LL744gucPn0aAFC7dm0MHz4ctWrVcmjjiJwhz8qoJdOVsA1GAU+dq1tFRETFYXdn31WrVqFu3bo4fPgwGjRogAYNGuDIkSOoW7cuVq1a5Yw2EjmUsZBLSwAnxSMi98jKNUBwwIHd7K7IjB07FuPGjcPkyZPNtk+cOBFjx45FXFycwxpH5Ax5hXT2BR5eeiIicpXrmdlo96/taBsdgll9G7m7OYpid0UmOTkZr7zyisX2l156CcnJyQ5pFJEzFVmRMTDIEJFrnbuWiczsPBy5fMvdTVEcu4PM008/jV27dlls3717N9q0aeOQRhE5k9WKjEnfXlZkiMjVsnONAICsB/+S7ey+tPT888/jnXfeweHDh9GiRQsAwL59+7BixQpMmjQJ//3vf82OJSpppEnvTDv7ajQa6LQaGIyCfWSIyOWy8wxm/5Lt7A4y0lwxc+bMwZw5c6zuA+7/YuBMv1QS5T24dKTTmQ+x1mk0MIBBhohcT6rEZLMiYze7g4zRyJNMymawUpEBHlxqMnDUEhG5nlSJyTEYOSmnnezuI2MqKyvLUe0gchnDgzCu01oJMmCQISLXM+0bk53HgoE97A4yBoMBH374ISpWrAg/Pz+cP38eADB+/HgsWLDA4Q0kcjTDg58RBQYZdvYlIhcz7RvDfjL2sTvITJkyBYsXL8a0adPg5eUlb69bty7+/e9/O7RxRM4gVWQ8WJEhohLCtCLDkUv2sTvILFmyBPPnz0e/fv2g0z2cx71BgwY4c+aMQxtH5AxSRSb/NWgGGSJyl6xcg9X/U9HsDjJXrlxBjRo1LLYbjUbk5uY6pFFEziQNv7aoyGgYZIjIPUz7xbCPjH3sDjJ16tSxOiHeypUr0agRp1Wmki/vwaUlrbVRS2CQISLXY0Wm+Owefj1hwgTEx8fjypUrMBqNWL16Nc6ePYslS5Zg3bp1zmgjkUNJl5YK6iOTxyBDRC7Gikzx2V2R6datG9auXYstW7bA19cXEyZMwOnTp7F27Vp06NDBGW0kciips2/+PjJSsDFy1BIRuRgrMsVnd0UGANq0aYPNmzc7ui1ELlFQRUYKNnlcNJKIXIwVmeKzuyJTrVo13Lhxw2J7WloaqlWr5pBGETlTQRPisSJDRO7Cikzx2R1kLl68aHUNpezsbFy5csUhjSJyJqngkr+zr3SffWSIyNWyzeaRYZCxh82XlkxXtd64cSMCAgLk+waDAVu3bkXVqlUd2jgiZ5AnxMu/aKRUkWGQISIXM5/Zl5eW7GFzkOnevTuA+6tax8fHm+3z9PRE1apV8dlnnzm0cUTOIA2vLmj4NSsyRORqWazIFJvNQUZa9ToqKgoHDx5EhQoVnNYoImeSggyXKCCikoIVmeKze9TShQsXnNEOIpeRKzIMMkRUQpitfs2KjF1s7uy7d+9eiwnvlixZgqioKISEhGDIkCHIzs52eAOJHC2voIqMhqtfE5F7sCJTfDYHmcmTJ+P333+X7584cQKDBg1CbGws3n33XaxduxaJiYlOaSSRI0nDqy2GX+vY2ZeI3IN9ZIrP5iBz7NgxtG/fXr6/fPlyNG/eHF999RVGjx6NWbNm4fvvv3dKI4kcSZrwLn+Q4fBrInIHIQSy8kznkWFFxh42B5lbt24hNDRUvr9jxw506dJFvt+0aVMkJSU5tnVETiBXZDQFTIjHIENELpRrEDC9om16mYmKZnOQCQ0NlTv65uTk4MiRI2jRooW8PzMzE56eno5vIZGDSRUXi4oMh18TkRtk5QsurMjYx+Yg88wzz+Ddd9/Frl27MG7cOJQpUwZt2rSR9//222+oXr26UxpJ5EjGAoKMVJFhZ18icqXsfMGFFRn72Dz8+sMPP0TPnj3x1FNPwc/PD19//TW8vLzk/QsXLkTHjh2d0kgiRzIU0NlXqsgYDPxriIhcJ3/nXlZk7GNzkKlQoQJ27tyJ9PR0+Pn5QafTme1fsWIF/Pz8HN5AIkcrqLPvw4qMy5tERCqWf7g1KzL2sXtCPNM1lkwFBQU9cmOIXKGg4dfyPDJG/jVERK7DisyjsXv1ayKlkzv7FrDWEq8sEZEr5a/A5O/8S4VjkCHVKaiz78MgwyRDRK5j0dmXFRm7MMiQ6hQ0/JoVGSJyB6kC4+Vx/1cy+8jYh0GGVIcVGSIqSaQKTICPp9l9sg2DDKlOkRUZziNDRC4kVWSkIMM+MvZhkCHVKWrUEmf2JSJXyl+RyTUIGPhzyGYMMqQ6UlDxKKAiw7WWiMiVpOHXUpAB2E/GHgwypDrSXzpaDr8mohIgK8+8IgNwLhl7MMiQ6hjkioz525+dfYnIHaRLSz5eOnjpOHLJXgwypDpyRSbfu5+dfYnIHaTOvXoPLfQPhmCzImM7BhlSnQIrMvISBQwyROQ6UkXG21MHvef9dQxZkbEdgwypjlRxsajI6BhkiMj1WJF5NAwypCpGo4B05aigigyHXxORK5lWZLw9H/SRyWVFxlYMMqQqpv1fClo0ksOviciVzCsyugfbWJGxFYMMqYrpZSPpUpJ8X8uKDBG5nlR9Ma3IZLEiYzMGGVIVsyCTryIjTZBn5KglInKh7Dzp0pIW3nJnX1ZkbMUgQ6pidmkp38y+WqkiY2CQISLXkaoveg+dSWdfVmRsxSBDqmIwFBxkWJEhIndgRebRMMiQqphWZPLlGHnJAvaRISJXslaR4agl2zHIkKpIfWR0Wg00+fvIcB4ZInIDVmQeDYMMqYocZPKFGOBhRYZBhohciX1kHg2DDKmKaUUmP2mCPAYZInKlrFzLigyDjO0YZEhVCgsyDxadZZAhIpfKzjOpyPDSkt3cGmR27tyJ5557DhEREdBoNPjhhx/M9gshMGHCBISHh8PHxwexsbE4d+6cexpLpUJeIUFGvrTEUUtE5CJCCLkio/fU8tJSMbg1yNy5cwcNGjTA7Nmzre6fNm0aZs2ahXnz5mH//v3w9fVFp06dkJWV5eKWUmkhDa22emlJxyUKiMi1cgwPKy/3Z/ZlRcZeHu588i5duqBLly5W9wkhMHPmTPzzn/9Et27dAABLlixBaGgofvjhB/Tp08eVTaVSQprsrrCKDIdfE5GrmK5ybb76NSsytiqxfWQuXLiAlJQUxMbGytsCAgLQvHlz7N27t8Cvy87ORkZGhtmNSCJXZKyMWmJnXyJyNal/jEYDeOk4/Lo4SmyQSUlJAQCEhoaabQ8NDZX3WZOYmIiAgAD5FhkZ6dR2krIU2keGnX2JyMWypRFLHjpoNBpWZIqhxAaZ4ho3bhzS09PlW1JSkrubRCWITcOv2dmXiFxEnkPmwarXD4dfsyJjqxIbZMLCwgAAqampZttTU1Plfdbo9Xr4+/ub3YgkUpDx4PBrIioB5Fl9Pe4HGHmJAl5aslmJDTJRUVEICwvD1q1b5W0ZGRnYv38/YmJi3NgyUjIppGitBhn2kSEi1yqoIsO1lmzn1lFLt2/fxp9//infv3DhAo4dO4agoCBUrlwZI0eOxEcffYSaNWsiKioK48ePR0REBLp37+6+RpOiFVqR4RIFRORi+Ssy3p6syNjLrUHm0KFDaNu2rXx/9OjRAID4+HgsXrwYY8eOxZ07dzBkyBCkpaWhdevW2LBhA7y9vd3VZFI4qf+L1sqoJanfDIMMEblK/oqM3oNLFNjLrUHm6aefhiikY6VGo8HkyZMxefJkF7aKSjNjoUsUMMgQkWuxIvPoSmwfGSJnKGz4tRxkOGqJiFyEFZlHxyBDqlL4opEPgoyBQYaIXENeZylfRSbPKJBnYFXGFgwypCqFzyPDigwRuZY0s693vorM/X0MMrZgkCFVMRSyRIE0JJtrLRGRq+SvyEjzyNzfx8tLtmCQIVUxGO//0JBWujYlVWS4+jURuUr+ioxWq4EXJ8WzC4MMqYp0ydna8Guufk1EriZVZKSJ8ABwvSU7MciQqsgVmUL6yACsyhCRa0gVGdNLSlwB2z4MMqQqckXG6urXD7exwy8RuQIrMo+OQYZUpbCKjOlIJk6KR0SuIK2pZK0iwxWwbcMgQ6pS2KKRHgwyRORi8sy+Vioy0mUnKhyDDKlKXiGLRpp2AGaHXyJyhSxWZB4ZgwypirGQeWTY2ZeIXM1aRebhekusyNiCQYZUpbC1lkwvN7EiQ0SuIFVkpPACPJwcL5sVGZswyJCqFLb6NWAyKR5HLRGRC0gVGdOlCViRsQ+DDKlKYRUZgMsUEJFr5V/9GjBdAZsVGVswyJCq2FyRYZAhIhfIkifEs6zIcB4Z2zDIkKpIE91ZW6IAeNgJmBUZInKFbHlCPCt9ZDizr00YZEhVCht+DQC6B4tJch4ZInKFh8OvTeaRYUXGLgwypCpFXVqSKjIMMkTkCg+HX5vMI8OKjF0YZEhViursK21nkCEiZxNCWJ/ZlxUZuzDIkKoUWZFhkCEiFzGtuJjN7MuKjF0YZEhVbK7IcB4ZInIy0wnvWJEpPgYZUpXCligAWJEhIteRhl5rNeYDEKSKTBYrMjZhkCFVyTM8CDI6Bhkicq+HQ6910Jj8cSVVZLJZkbEJgwypiqGoigxHLRGRi0gVGdPLSgArMvZikCFVMbCzLxGVEFJFxrSjL/Aw2LAiYxsGGVIVm4MMO/sSkZMVVJGRLy2xImMTBhlSFUNRM/vKFRn+ACEi5yqwIuPBiow9GGRIVaQgoy0yyLisSUSkUg9XvrZekWEfGdswyJCqFFmR0bAiQ0Su8XDla+sVGc4jYxsGGVKVIle/ZkWGiFzEdPi1KfaRsQ+DDKmKrZ1981iRISInkzv7FlCRMRgFcvlXVZEYZEhVbA0yRo5aIiInkzv7FlCRAViVsQWDDKmKrWstSTMAExE5S0EVGdM+M+wnUzQGGVIVYxGdfT1YkSEiF3lYkTH/VazRaOQww4pM0RhkSFWK6uwrbc/jzL5E5GQPKzI6i31SkGFFpmgMMqQq8vDrAhaNlLYbGWSIyMkKqsgAD0cyMcgUjUGGVEWeEI8VGSJys+zCKjIcgm0zBhlSlYcT4ll/63tw0UgicpGsAuaRATgpnj0YZEhVHi5RYH2/lp19ichFpIpMYZeWWJEpGoMMqYqtFRleWiIiZ5MrMoV09uXCkUVjkCFVkUYt6Qp458sT4jHIEJGTsSLjGAwypCrSRHe6AioyOlZkiMhFpIqMnsOvHwmDDKmK1PdFV9CikRpWZIjINaSQ4l3o8GtWZIrCIEOqUtQSBVpWZIjIRaTLRoVVZKTLT1QwBhlSFWMRQUYefs1RS0TkZIVVZPSsyNiMQYZUxdaKjIGLRhKRkxVWkfH2ZEXGVgwypCqsyBBRSVFoRcaDFRlbMciQqsgVmSI6+3JmXyJyNrkiY21mX1ZkbMYgQ6oizyNTwKKR0rBsBhkiciajUSAnT5oQjxWZR8EgQ6piKKoiozU/jojIGUwnurO61pIn55GxFYMMqYqhiD4yrMgQkSuYXjLSF1KR4cy+RWOQIdUwneSu4CBz/18GGSJyJumSkYdWAw8ra6awImM7BhlSjTybgsyDigxHLRGRE8nrLFmpxtzfzoqMrRhkSDWMwoYg82AzKzJE5EzyytdW+sfc387Vr23FIEOqYVqR8SgoyOjYR4aInK+oigxXv7Ydgwyphmk40XIeGSJyo6IqMlz92nYMMqQaBhsqMvLMvgwyROREUkCxNhkewNWv7cEgQ6phVpEpaq0ldvYlIid6uM5SQZ19ObOvrRhkSDWkIFNQNcZ0HysyRORMha2zdH87KzK2YpAh1ZCqLAVVY0z3McgQkTMVtvL1/e0PKzKCFeJCMciQahgMtldk8hhkiMiJiqrISH1njALINfDnUWEYZEg15AUjCxixBDwczWRkkCEiJ5IqMkXNI3P/WPaTKQyDDKmGwXj/B0dhl5Z0rMgQkQvIo5YK6OzrpdNC+puL/WQKxyBDqmF48LPAlktLRl6TJiInypYvLVmvyGg0Gs4lYyMGGVKNPBsqMtK+PF6TJiInKmr49f19nN3XFgwypBpGVmSIqITIKqIic38fKzK2YJAh1ZArMjZ09mUfGSJyJlZkHIdBhlRDqrJ46AqpyOg4aomInM+eigxXwC6cIoLM7NmzUbVqVXh7e6N58+Y4cOCAu5tECiT1e7Fl+DUrMkTkTHJFptAgw4qMLUp8kPnuu+8wevRoTJw4EUeOHEGDBg3QqVMnXLt2zd1NI4WR55HhEgVE5GZFDb823cc+MoXzcHcDijJ9+nQMHjwYAwYMAADMmzcPP/30ExYuXIh3333Xbe26dScHd3Ly3Pb8ZL9rGdkACg8y0r4cgxH/u3XXJe0iIvXJyLr/+6PwS0v39yWnZ5X4n0eBZbzgp3dPpCjRQSYnJweHDx/GuHHj5G1arRaxsbHYu3ev1a/Jzs5Gdna2fD8jI8Mpbft001l8u/+yUx6bnKuwzr5SkLmemY3Wn2xzVZOISKVsqchMXncKk9edclWTiuXjHvXwYvPKbnnuEh1k/v77bxgMBoSGhpptDw0NxZkzZ6x+TWJiIiZNmuT0tnlqNYW+Aalk0mk16Fo/vMD91YJ9Ub9SAM6mZLqwVUSkRuEB3niicrkC93euG479528ix1Dy+8jo3PjrUCNK8LKaV69eRcWKFfHrr78iJiZG3j527Fjs2LED+/fvt/gaaxWZyMhIpKenw9/f3yXtJiIiokeTkZGBgICAIn9/l+iKTIUKFaDT6ZCammq2PTU1FWFhYVa/Rq/XQ6/Xu6J5RERE5GYl+tqIl5cXGjdujK1bt8rbjEYjtm7dalahISIiInUq0RUZABg9ejTi4+PRpEkTNGvWDDNnzsSdO3fkUUxERESkXiU+yPzjH//A9evXMWHCBKSkpKBhw4bYsGGDRQdgIiIiUp8S3dnXEWztLEREREQlh62/v0t0HxkiIiKiwjDIEBERkWIxyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFilfglCh6VNHFxRkaGm1tCREREtpJ+bxe1AEGpDzKZmZkAgMjISDe3hIiIiOyVmZmJgICAAveX+rWWjEYjrl69irJly0Kj0TjscTMyMhAZGYmkpCSu4eQCPN+uw3PtOjzXrsNz7TqOOtdCCGRmZiIiIgJabcE9YUp9RUar1aJSpUpOe3x/f39+KFyI59t1eK5dh+fadXiuXccR57qwSoyEnX2JiIhIsRhkiIiISLEYZIpJr9dj4sSJ0Ov17m6KKvB8uw7PtevwXLsOz7XruPpcl/rOvkRERFR6sSJDREREisUgQ0RERIrFIENERESKxSBDREREisUgU0yzZ89G1apV4e3tjebNm+PAgQPubpLiJSYmomnTpihbtixCQkLQvXt3nD171uyYrKwsJCQkoHz58vDz80NcXBxSU1Pd1OLSY+rUqdBoNBg5cqS8jefaca5cuYKXXnoJ5cuXh4+PD+rVq4dDhw7J+4UQmDBhAsLDw+Hj44PY2FicO3fOjS1WJoPBgPHjxyMqKgo+Pj6oXr06PvzwQ7O1eniui2fnzp147rnnEBERAY1Ggx9++MFsvy3n9ebNm+jXrx/8/f0RGBiIQYMG4fbt24/eOEF2W758ufDy8hILFy4Uv//+uxg8eLAIDAwUqamp7m6aonXq1EksWrRInDx5Uhw7dkw888wzonLlyuL27dvyMUOHDhWRkZFi69at4tChQ6JFixaiZcuWbmy18h04cEBUrVpV1K9fX4wYMULeznPtGDdv3hRVqlQR/fv3F/v37xfnz58XGzduFH/++ad8zNSpU0VAQID44YcfxPHjx8Xzzz8voqKixL1799zYcuWZMmWKKF++vFi3bp24cOGCWLFihfDz8xOff/65fAzPdfH8/PPP4v333xerV68WAMSaNWvM9ttyXjt37iwaNGgg9u3bJ3bt2iVq1Kgh+vbt+8htY5AphmbNmomEhAT5vsFgEBERESIxMdGNrSp9rl27JgCIHTt2CCGESEtLE56enmLFihXyMadPnxYAxN69e93VTEXLzMwUNWvWFJs3bxZPPfWUHGR4rh3nnXfeEa1bty5wv9FoFGFhYeLTTz+Vt6WlpQm9Xi/+85//uKKJpUbXrl3FwIEDzbb17NlT9OvXTwjBc+0o+YOMLef11KlTAoA4ePCgfMz69euFRqMRV65ceaT28NKSnXJycnD48GHExsbK27RaLWJjY7F37143tqz0SU9PBwAEBQUBAA4fPozc3Fyzcx8dHY3KlSvz3BdTQkICunbtanZOAZ5rR/rvf/+LJk2a4IUXXkBISAgaNWqEr776St5/4cIFpKSkmJ3rgIAANG/enOfaTi1btsTWrVvxxx9/AACOHz+O3bt3o0uXLgB4rp3FlvO6d+9eBAYGokmTJvIxsbGx0Gq12L9//yM9f6lfNNLR/v77bxgMBoSGhpptDw0NxZkzZ9zUqtLHaDRi5MiRaNWqFerWrQsASElJgZeXFwIDA82ODQ0NRUpKihtaqWzLly/HkSNHcPDgQYt9PNeOc/78ecydOxejR4/Ge++9h4MHD+LNN9+El5cX4uPj5fNp7WcKz7V93n33XWRkZCA6Oho6nQ4GgwFTpkxBv379AIDn2klsOa8pKSkICQkx2+/h4YGgoKBHPvcMMlQiJSQk4OTJk9i9e7e7m1IqJSUlYcSIEdi8eTO8vb3d3ZxSzWg0okmTJvj4448BAI0aNcLJkycxb948xMfHu7l1pcv333+PZcuW4dtvv8Xjjz+OY8eOYeTIkYiIiOC5LsV4aclOFSpUgE6nsxi9kZqairCwMDe1qnR54403sG7dOmzbtg2VKlWSt4eFhSEnJwdpaWlmx/Pc2+/w4cO4du0annjiCXh4eMDDwwM7duzArFmz4OHhgdDQUJ5rBwkPD0edOnXMttWuXRuXL18GAPl88mfKoxszZgzeffdd9OnTB/Xq1cPLL7+MUaNGITExEQDPtbPYcl7DwsJw7do1s/15eXm4efPmI597Bhk7eXl5oXHjxti6dau8zWg0YuvWrYiJiXFjy5RPCIE33ngDa9aswS+//IKoqCiz/Y0bN4anp6fZuT979iwuX77Mc2+n9u3b48SJEzh27Jh8a9KkCfr16yf/n+faMVq1amUxjcAff/yBKlWqAACioqIQFhZmdq4zMjKwf/9+nms73b17F1qt+a81nU4Ho9EIgOfaWWw5rzExMUhLS8Phw4flY3755RcYjUY0b9780RrwSF2FVWr58uVCr9eLxYsXi1OnTokhQ4aIwMBAkZKS4u6mKdrrr78uAgICxPbt20VycrJ8u3v3rnzM0KFDReXKlcUvv/wiDh06JGJiYkRMTIwbW116mI5aEoLn2lEOHDggPDw8xJQpU8S5c+fEsmXLRJkyZcQ333wjHzN16lQRGBgofvzxR/Hbb7+Jbt26cUhwMcTHx4uKFSvKw69Xr14tKlSoIMaOHSsfw3NdPJmZmeLo0aPi6NGjAoCYPn26OHr0qLh06ZIQwrbz2rlzZ9GoUSOxf/9+sXv3blGzZk0Ov3anL774QlSuXFl4eXmJZs2aiX379rm7SYoHwOpt0aJF8jH37t0Tw4YNE+XKlRNlypQRPXr0EMnJye5rdCmSP8jwXDvO2rVrRd26dYVerxfR0dFi/vz5ZvuNRqMYP368CA0NFXq9XrRv316cPXvWTa1VroyMDDFixAhRuXJl4e3tLapVqybef/99kZ2dLR/Dc10827Zts/rzOT4+Xghh23m9ceOG6Nu3r/Dz8xP+/v5iwIABIjMz85HbphHCZMpDIiIiIgVhHxkiIiJSLAYZIiIiUiwGGSIiIlIsBhkiIiJSLAYZIiIiUiwGGSIiIlIsBhkiIiJSLAYZIiqRLl68CI1Gg2PHjjntOfr374/u3bs77fGJyPkYZIjIKfr37w+NRmNx69y5s01fHxkZieTkZNStW9fJLSUiJfNwdwOIqPTq3LkzFi1aZLZNr9fb9LU6nY4rEhNRkViRISKn0ev1CAsLM7uVK1cOAKDRaDB37lx06dIFPj4+qFatGlauXCl/bf5LS7du3UK/fv0QHBwMHx8f1KxZ0ywknThxAu3atYOPjw/Kly+PIUOG4Pbt2/J+g8GA0aNHIzAwEOXLl8fYsWORf4UWo9GIxMREREVFwcfHBw0aNDBrExGVPAwyROQ248ePR1xcHI4fP45+/fqhT58+OH36dIHHnjp1CuvXr8fp06cxd+5cVKhQAQBw584ddOrUCeXKlcPBgwexYsUKbNmyBW+88Yb89Z999hkWL16MhQsXYvfu3bh58ybWrFlj9hyJiYlYsmQJ5s2bh99//x2jRo3CSy+9hB07djjvJBDRo3nkZSeJiKyIj48XOp1O+Pr6mt2mTJkihLi/2vnQoUPNvqZ58+bi9ddfF0IIceHCBQFAHD16VAghxHPPPScGDBhg9bnmz58vypUrJ27fvi1v++mnn4RWqxUpKSlCCCHCw8PFtGnT5P25ubmiUqVKolu3bkIIIbKyskSZMmXEr7/+avbYgwYNEn379i3+iSAip2IfGSJymrZt22Lu3Llm24KCguT/x8TEmO2LiYkpcJTS66+/jri4OBw5cgQdO3ZE9+7d0bJlSwDA6dOn0aBBA/j6+srHt2rVCkajEWfPnoW3tzeSk5PRvHlzeb+HhweaNGkiX176888/cffuXXTo0MHseXNyctCoUSP7XzwRuQSDDBE5ja+vL2rUqOGQx+rSpQsuXbqEn3/+GZs3b0b79u2RkJCAf/3rXw55fKk/zU8//YSKFSua7bO1gzIRuR77yBCR2+zbt8/ifu3atQs8Pjg4GPHx8fjmm28wc+ZMzJ8/HwBQu3ZtHD9+HHfu3JGP3bNnD7RaLWrVqoWAgACEh4dj//798v68vDwcPnxYvl+nTh3o9XpcvnwZNWrUMLtFRkY66iUTkYOxIkNETpOdnY2UlBSzbR4eHnIn3RUrVqBJkyZo3bo1li1bhgMHDmDBggVWH2vChAlo3LgxHn/8cWRnZ2PdunVy6OnXrx8mTpyI+Ph4fPDBB7h+/TqGDx+Ol19+GaGhoQCAESNGYOrUqahZsyaio6Mxffp0pKWlyY9ftmxZvP322xg1ahSMRiNat26N9PR07NmzB/7+/oiPj3fCGSKiR8UgQ0ROs2HDBoSHh5ttq1WrFs6cOQMAmDRpEpYvX45hw4YhPDwc//nPf1CnTh2rj+Xl5YVx48bh4sWL8PHxQZs2bbB8+XIAQJkyZbBx40aMGDECTZs2RZkyZRAXF4fp06fLX//WW28hOTkZ8fHx0Gq1GDhwIHr06IH09HT5mA8//BDBwcFITEzE+fPnERgYiCeeeALvvfeeo08NETmIRoh8EykQEbmARqPBmjVruEQAET0S9pEhIiIixWKQISIiIsViHxkicgte1SYiR2BFhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFOv/AcDSAia/+NRaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from environment import Env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.99\n",
    "eps_clip = 0.2\n",
    "learning_rate = 0.0005\n",
    "update_timestep = 20\n",
    "\n",
    "# Create environment\n",
    "env = Env()\n",
    "state_dim = len(env.reset())\n",
    "action_dim = len(env.action_space)\n",
    "\n",
    "# Actor-critic network architecture\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "# Proximal Policy Optimization (PPO) algorithm\n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.policy = ActorCritic()\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits, _ = self.policy(state)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(action_probs, 1)\n",
    "        # print(f\"action probabilites: {action_probs}\")\n",
    "        return action.item()\n",
    "\n",
    "    def train(self, states, actions, advantages, returns):\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        advantages = torch.FloatTensor(advantages).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "\n",
    "        unique_actions, unique_indices = torch.unique(actions, return_inverse=True)\n",
    "\n",
    "        logits, values = self.policy(states)\n",
    "        values = values.squeeze()\n",
    "\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        action_masks = torch.zeros_like(action_probs).scatter_(1, unique_actions.unsqueeze(1), 1)\n",
    "        old_action_probs = torch.sum(action_probs * action_masks[unique_indices.unsqueeze(1)], dim=1)\n",
    "        ratios = torch.exp(torch.log(old_action_probs + 1e-10) - torch.log(action_probs + 1e-10))\n",
    "\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - eps_clip, 1 + eps_clip) * advantages\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        critic_loss = nn.MSELoss()(returns, values)\n",
    "\n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "class Analyzer:\n",
    "    @staticmethod\n",
    "    def plot_total_rewards(episode_rewards):\n",
    "        plt.plot(episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Rewards')\n",
    "        plt.title('Total Rewards Obtained per Episode')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_success_rate(success_count, n_episodes):\n",
    "        return success_count / n_episodes\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_steps_to_success(success_steps):\n",
    "        plt.plot(success_steps)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps to Success')\n",
    "        plt.title('Total Steps Needed to Reach Successful Episodes')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize PPO agent\n",
    "ppo_agent = PPO()\n",
    "\n",
    "# Main training loop\n",
    "total_timesteps = 0\n",
    "episode_rewards = []\n",
    "success_count = 0\n",
    "success_steps = []\n",
    "\n",
    "for episode in range(100):  # Run 100 episodes\n",
    "    total_rewards = 0\n",
    "    episode_steps = 0\n",
    "    max_steps = 1000\n",
    "\n",
    "    states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while episode_steps < max_steps :\n",
    "        # env.render()\n",
    "        action = ppo_agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        next_states.append(next_state)\n",
    "\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        episode_steps += 1\n",
    "\n",
    "        total_timesteps += 1\n",
    "\n",
    "        if done:\n",
    "            total_rewards += episode_reward\n",
    "            if episode_reward >= 0:  # Considered successful if the total reward is 200 or more\n",
    "                success_count += 1\n",
    "                success_steps.append(episode_steps)\n",
    "            else:\n",
    "                success_steps.append(0)\n",
    "            break\n",
    "\n",
    "        if total_timesteps % update_timestep == 0:\n",
    "            _, next_value = ppo_agent.policy(torch.FloatTensor(next_states))\n",
    "            returns, advantages = [], []\n",
    "            discounted_sum = 0\n",
    "            for i in range(len(rewards) - 1, -1, -1):\n",
    "                discounted_sum = rewards[i] + gamma * discounted_sum * (1 - dones[i])\n",
    "                advantage = discounted_sum - next_value[i].item()\n",
    "                advantages.insert(0, advantage)\n",
    "                returns.insert(0, discounted_sum)\n",
    "\n",
    "            ppo_agent.train(states, actions, advantages, returns)\n",
    "\n",
    "    episode_rewards.append(total_rewards)\n",
    "    print(f\"Episode: {episode + 1}, Total Timesteps: {total_timesteps}, Episode Reward: {episode_reward}\")\n",
    "\n",
    "    # Print total reward obtained for each episode\n",
    "    print(f\"Total reward obtained for episode {episode + 1}: {total_rewards}\")\n",
    "\n",
    "# Call Analyzer functions to generate outputs\n",
    "Analyzer.plot_total_rewards(episode_rewards)\n",
    "success_rate = Analyzer.calculate_success_rate(success_count, 100)\n",
    "print(\"Success rate of episodes:\", success_rate)\n",
    "if success_count > 0:\n",
    "    Analyzer.plot_steps_to_success(success_steps)\n",
    "else:\n",
    "    print(\"No successful episodes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
