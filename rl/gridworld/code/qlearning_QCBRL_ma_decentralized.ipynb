{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- starting point of Episode 0 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 hit an obstacle! Next state: [62.5, 162.5, 87.5, 187.5]\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'agent', 'obstacle'], ['empty', 'obstacle', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 3], False, [['empty', 'agent', 'obstacle'], ['empty', 'obstacle', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 4], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'obstacle', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 4], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'obstacle', None]], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 4], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'obstacle', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 0 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "Agent 1 hit an obstacle! Next state: [462.5, 262.5, 487.5, 287.5]\n",
      "state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 4], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'obstacle', None]], 0]\n",
      "next state for agent 1: [[9, 5], False, [['empty', 'agent', None], ['empty', 'obstacle', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12de40>, <__main__.Case object at 0x794a0f12ded0>, <__main__.Case object at 0x794a0f12dff0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e1d0>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12e350>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12cc70>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12dea0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12e2f0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 0, Total Steps: 14, Total Rewards: [-107, -113], Status Episode: False\n",
      "------------------------------------------End of episode 0 loop--------------------\n",
      "----- starting point of Episode 1 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 3], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[3, 3], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[4, 3], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[4, 3], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[4, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[4, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "next state for agent 0: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "next state for agent 0: [[4, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'target'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[4, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'target'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 5], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[3, 5], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[4, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'target'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[4, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'target'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'target']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'target']], 0]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 0: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'target']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'target']], 0]\n",
      "next state for agent 0: [[3, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 212.5, 137.5, 237.5]\n",
      "state for agent 0: [[3, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'obstacle', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'obstacle', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 3], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 3], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 1 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "Agent 1 hit an obstacle! Next state: [262.5, 112.5, 287.5, 137.5]\n",
      "state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 4], False, [['obstacle', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e200>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12ed70>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e890>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12e740>, <__main__.Case object at 0x794a0f12e5c0>, <__main__.Case object at 0x794a0f12e530>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f400>, <__main__.Case object at 0x794a0f12f4c0>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f12e3e0>, <__main__.Case object at 0x794a0f12dff0>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12e1d0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12e170>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12eaa0>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12dc00>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12f0a0>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12f280>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12f460>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 1, Total Steps: 32, Total Rewards: [-126, -131], Status Episode: False\n",
      "------------------------------------------End of episode 1 loop--------------------\n",
      "----- starting point of Episode 2 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'obstacle'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'obstacle'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'obstacle'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'obstacle'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'agent', 'obstacle'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 5], False, [[None, 'agent', 'obstacle'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'empty', 'obstacle'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 5], False, [[None, 'empty', 'obstacle'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'empty', 'obstacle'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 5], False, [[None, 'empty', 'obstacle'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 5], False, [['empty', 'obstacle', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 hit an obstacle! Next state: [62.5, 212.5, 87.5, 237.5]\n",
      "state for agent 0: [[1, 5], False, [['empty', 'obstacle', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'obstacle', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'obstacle', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 1: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[6, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 2 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "Agent 1 hit an obstacle! Next state: [262.5, 112.5, 287.5, 137.5]\n",
      "state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 1: [[5, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12dff0>, <__main__.Case object at 0x794a0f12e1d0>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12f1c0>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f12fe50>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12ed70>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12f400>, <__main__.Case object at 0x794a0f12e3e0>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12ed40>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12feb0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Episode: 2, Total Steps: 32, Total Rewards: [-114, -131], Status Episode: False\n",
      "------------------------------------------End of episode 2 loop--------------------\n",
      "----- starting point of Episode 3 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 3], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 3], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 112.5, 137.5, 137.5]\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['target', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['target', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 6], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 6], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 6], False, [['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 6], False, [['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 6], False, [['target', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 6], False, [['target', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 6], False, [['target', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 6], False, [['target', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'target']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'target']], 0]\n",
      "next state for agent 1: [[4, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[4, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "next state for agent 1: [[4, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'target'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'target'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'target']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'target']], 0]\n",
      "next state for agent 1: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'target']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 3 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12f1c0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12e3e0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12ed40>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12f670>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f12fac0>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12fdc0>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f1343a0>, <__main__.Case object at 0x794a0f134490>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f134670>, <__main__.Case object at 0x794a0f134760>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f1349d0>, <__main__.Case object at 0x794a0f134af0>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f134d00>, <__main__.Case object at 0x794a0f134dc0>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f1351e0>, <__main__.Case object at 0x794a0f1352a0>, <__main__.Case object at 0x794a0f135390>, <__main__.Case object at 0x794a0f135450>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f1355d0>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12f4f0>, <__main__.Case object at 0x794a0f12cc70>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12f400>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12f970>, <__main__.Case object at 0x794a0f12fa60>, <__main__.Case object at 0x794a0f12ed70>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f134280>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f1344f0>, <__main__.Case object at 0x794a0f1345e0>, <__main__.Case object at 0x794a0f1346d0>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f134970>, <__main__.Case object at 0x794a0f134ac0>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f134bb0>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f134e20>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f135060>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f135240>, <__main__.Case object at 0x794a0f135300>, <__main__.Case object at 0x794a0f1353f0>, <__main__.Case object at 0x794a0f1354b0>, <__main__.Case object at 0x794a0f135570>]\n",
      "agent1 comm temp case base: []\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) is empty. Temporary case base stored to the case base: ((4, 5), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 6) is empty. Temporary case base stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 1, 0.5)\n",
      "Episode succeeded, case (8, 7) is empty. Temporary case base stored to the case base: ((8, 7), 1, 0.5)\n",
      "Episode succeeded, case (7, 7) is empty. Temporary case base stored to the case base: ((7, 7), 4, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 0.5)\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.5)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 4, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 1, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (4, 4), solution: 4, tv: 0.5, time steps: 67\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5, time steps: 63\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 60\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.5, time steps: 58\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.5, time steps: 57\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5, time steps: 52\n",
      "cases content after RETAIN, problem: (8, 6), solution: 1, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (8, 7), solution: 1, tv: 0.5, time steps: 49\n",
      "cases content after RETAIN, problem: (7, 7), solution: 4, tv: 0.5, time steps: 48\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 34\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 32\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 2), solution: 1, tv: 0.5, time steps: 28\n",
      "cases content after RETAIN, problem: (7, 1), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (6, 3), solution: 1, tv: 0.5, time steps: 9\n",
      "Episode: 3, Total Steps: 69, Total Rewards: [-115, 32], Status Episode: False\n",
      "------------------------------------------End of episode 3 loop--------------------\n",
      "----- starting point of Episode 4 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 32)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 32)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 32)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.5, 33)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.5, 33)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.5, 33)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 2, 0.5, 34)]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.5, 34)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 2, 0.5, 34)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 4 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 2), 1, 0.5, 28)]\n",
      "comm next state for agent 0: ((9, 2), 1, 0.5, 28)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 2), 1, 0.5, 28)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 1), 3, 0.5, 29)]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.5, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 1), 3, 0.5, 29)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], ((8, 1), 2, 0.5, 34)]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.5, 34)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], ((8, 1), 2, 0.5, 34)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], ((8, 2), 3, 0.5, 36)]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.5, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], ((8, 2), 3, 0.5, 36)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 2), 2, 0.5, 38)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5, 38)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 2), 2, 0.5, 38)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 3), 2, 0.5, 39)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.5, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 3), 2, 0.5, 39)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 4 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 39)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.5, 39)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 39)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.5, 45)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.5, 45)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.5, 45)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.5, 54)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.5, 54)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.5, 54)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 0.5, 59)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.5, 59)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 0.5, 59)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 4), 3, 0.5, 60)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 60)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 4), 3, 0.5, 60)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 4 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 4 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 4 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 4 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 112.5, 137.5, 137.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f1c0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12f4f0>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12e3b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12e1d0>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12ed40>, <__main__.Case object at 0x794a0f12eef0>, <__main__.Case object at 0x794a0f12dcf0>, <__main__.Case object at 0x794a0f12f7c0>, <__main__.Case object at 0x794a0f12fbe0>]\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 1, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.5)\n",
      "Integrated case process. comm case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 1, 0.5)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 60\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 34\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (9, 2), solution: 1, tv: 0.5, time steps: 28\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 32\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12f670>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12cc70>, <__main__.Case object at 0x794a0f12e830>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 4, tv: 0.3, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.3, time steps: 65\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.3, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 60\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 59\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 0.3, time steps: 58\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.3, time steps: 57\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 54\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 0.3, time steps: 52\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 1, tv: 0.3, time steps: 50\n",
      "case content after REVISE for agent 1, problem: (8, 7), solution: 1, tv: 0.3, time steps: 49\n",
      "case content after REVISE for agent 1, problem: (7, 7), solution: 4, tv: 0.3, time steps: 48\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.6, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.6, time steps: 34\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 32\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 1, tv: 0.6, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 4, tv: 0.3, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 4, tv: 0.3, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 1, tv: 0.3, time steps: 9\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 2, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 21)\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 2), 3, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 1), 2, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 1), 3, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 2), 1, 0.5, 21)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 2, 0.5, 21)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 0), 3, 0.5, 21)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (9, 2), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 20\n",
      "Episode: 4, Total Steps: 21, Total Rewards: [-120, 84], Status Episode: False\n",
      "------------------------------------------End of episode 4 loop--------------------\n",
      "----- starting point of Episode 5 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 21)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 21)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.5, 21)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.5, 21)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 2, 0.5, 21)]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 2, 0.5, 21)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 2), 3, 0.5, 21)]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 2), 3, 0.5, 21)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((7, 2), 2, 0.5, 21)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((7, 2), 2, 0.5, 21)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'obstacle']], ((7, 3), 2, 0.5, 21)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'obstacle']], ((7, 3), 2, 0.5, 21)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'obstacle']], ((7, 4), 2, 0.5, 21)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'obstacle']], ((7, 4), 2, 0.5, 21)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'obstacle']], ((7, 5), 3, 0.5, 21)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'obstacle']], ((7, 5), 3, 0.5, 21)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 0.5, 21)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 0.5, 21)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((6, 4), 3, 0.5, 21)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((6, 4), 3, 0.5, 21)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.5, 21)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.5, 21)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 5 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 5 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 5 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 5 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12f670>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f134d90>, <__main__.Case object at 0x794a0f1349a0>, <__main__.Case object at 0x794a0f134610>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f134490>, <__main__.Case object at 0x794a0f134760>, <__main__.Case object at 0x794a0f1349d0>, <__main__.Case object at 0x794a0f134d00>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f1352a0>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f135060>, <__main__.Case object at 0x794a0f136050>, <__main__.Case object at 0x794a0f135ff0>, <__main__.Case object at 0x794a0f135cc0>, <__main__.Case object at 0x794a0f135de0>, <__main__.Case object at 0x794a0f135c60>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f135a50>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f135720>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f136350>, <__main__.Case object at 0x794a0f1364a0>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f136830>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f1355a0>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a0f134880>, <__main__.Case object at 0x794a0f134c10>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f135600>, <__main__.Case object at 0x794a0f1342b0>, <__main__.Case object at 0x794a0f1348e0>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f1353f0>, <__main__.Case object at 0x794a0f136020>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135d80>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f135ab0>, <__main__.Case object at 0x794a0f1359c0>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f136410>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f136680>, <__main__.Case object at 0x794a0f1367a0>, <__main__.Case object at 0x794a0f1368c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 60\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 59\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.3, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.3, time steps: 34\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.3, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 1, tv: 0.3, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.3, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.3, time steps: 32\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 54)\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 54)\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 3, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 1, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.5)\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 1, 0.5)\n",
      "Episode succeeded, case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 4, 0.5)\n",
      "Episode succeeded, case (0, 3) is empty. Temporary case base stored to the case base: ((0, 3), 1, 0.5)\n",
      "Episode succeeded, case (0, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (8, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 2), 3, 0.5)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 54\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 39\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (5, 3), solution: 2, tv: 0.5, time steps: 49\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 28\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 2), solution: 1, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (0, 3), solution: 1, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 20\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12e5c0>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12cc70>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f135420>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f134b50>, <__main__.Case object at 0x794a0f134df0>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f135390>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f134bb0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f1360b0>, <__main__.Case object at 0x794a0f135f60>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f135e70>, <__main__.Case object at 0x794a0f135c00>, <__main__.Case object at 0x794a0f135b40>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f136140>, <__main__.Case object at 0x794a0f136290>, <__main__.Case object at 0x794a0f1363e0>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f136650>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f136890>, <__main__.Case object at 0x794a0f1369e0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.3, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 1, tv: 0.3, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "Episode: 5, Total Steps: 54, Total Rewards: [47, 90], Status Episode: True\n",
      "------------------------------------------End of episode 5 loop--------------------\n",
      "----- starting point of Episode 6 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.6, 21)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.5, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.6, 21)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.6, 21)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.6, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.5, 3)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.5, 15)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.6, 21)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 2, 0.6, 21)]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.6, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.5, 15)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.5, 16)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 2, 0.6, 21)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 2), 3, 0.6, 21)]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.6, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.5, 16)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.5, 17)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 2), 3, 0.6, 21)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 0.6, 21)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.5, 17)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 24)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 24)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 0.6, 21)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 24)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 6 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((3, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 6 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 hit an obstacle! Next state: [212.5, 62.5, 237.5, 87.5]\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[4, 1], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((3, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 6 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[4, 1], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[8, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 6 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "Agent 1 hit an obstacle! Next state: [462.5, 262.5, 487.5, 287.5]\n",
      "state for agent 0: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 5], False, [['empty', 'empty', None], ['agent', 'obstacle', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12dff0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12dc00>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 2, tv: 0.5, time steps: 49\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.5, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.5, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.09999999999999998, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.09999999999999998, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 1, tv: 0.5, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.09999999999999998, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.09999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.09999999999999998, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 1, tv: 0.5, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.5, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 3), solution: 1, tv: 0.5, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.09999999999999998, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.5, time steps: 20\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.6)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.6)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.6)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.6)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.6)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 54\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 39\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (5, 3), solution: 2, tv: 0.5, time steps: 49\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 28\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (1, 2), solution: 1, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (0, 3), solution: 1, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 21\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12ee30>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12e980>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.19999999999999996, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.19999999999999996, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.19999999999999996, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.19999999999999996, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.5)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "Episode: 6, Total Steps: 10, Total Rewards: [-107, -109], Status Episode: False\n",
      "------------------------------------------End of episode 6 loop--------------------\n",
      "----- starting point of Episode 7 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 hit an obstacle! Next state: [212.5, 62.5, 237.5, 87.5]\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['agent', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['agent', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['agent', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['agent', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['agent', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['agent', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['agent', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['agent', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['agent', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 7 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 hit an obstacle! Next state: [212.5, 62.5, 237.5, 87.5]\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[4, 1], False, [['empty', 'empty', 'empty'], ['agent', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['agent', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 1: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12dff0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12e1d0>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12fcd0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12f730>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f134880>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f1342b0>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f1353f0>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f136410>, <__main__.Case object at 0x794a0f136680>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f1349d0>]\n",
      "agent0 comm temp case base: []\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 2, tv: 0.5, time steps: 49\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.5, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.5, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.5, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 1, tv: 0.09999999999999998, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 1, tv: 0.09999999999999998, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.09999999999999998, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 3), solution: 1, tv: 0.5, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.5, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 21\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 54\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 45\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 39\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (5, 3), solution: 2, tv: 0.5, time steps: 49\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 28\n",
      "cases content after RETAIN, problem: (0, 3), solution: 1, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 21\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12cc70>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12fdc0>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12fc10>, <__main__.Case object at 0x794a0f12f7f0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f134610>, <__main__.Case object at 0x794a0f1369e0>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f1355a0>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a0f134c10>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135600>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f136020>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a0f135d80>, <__main__.Case object at 0x794a0f1359c0>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f1367a0>, <__main__.Case object at 0x794a0f134d90>, <__main__.Case object at 0x794a0f134490>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "Episode: 7, Total Steps: 49, Total Rewards: [-148, -112], Status Episode: False\n",
      "------------------------------------------End of episode 7 loop--------------------\n",
      "----- starting point of Episode 8 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((4, 0), 4, 0.5, 28)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 28)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((4, 0), 4, 0.5, 28)]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((5, 0), 2, 0.5, 36)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 36)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((5, 0), 2, 0.5, 36)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.5, 37)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.5, 37)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.5, 37)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.5, 39)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 39)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.5, 39)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 2), 4, 0.5, 41)]\n",
      "comm next state for agent 1: ((6, 2), 4, 0.5, 41)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'agent'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 2), 4, 0.5, 41)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 2), 2, 0.6, 38)]\n",
      "comm next state for agent 1: ((7, 2), 2, 0.6, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'agent'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 2), 2, 0.6, 38)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.6, 39)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.6, 39)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.6, 39)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.6, 45)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.6, 45)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.6, 45)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.6, 54)]\n",
      "comm next state for agent 1: ((7, 5), 3, 0.6, 54)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.6, 54)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((6, 5), 1, 0.5, 54)]\n",
      "comm next state for agent 1: ((6, 5), 1, 0.5, 54)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((6, 5), 1, 0.5, 54)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.5, 54)]\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 54)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.5, 54)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 4), 2, 0.5, 53)]\n",
      "comm next state for agent 1: ((5, 4), 2, 0.5, 53)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 4), 2, 0.5, 53)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 8 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 hit an obstacle! Next state: [262.5, 112.5, 287.5, 137.5]\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "next state for agent 1: [[5, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f1366e0>, <__main__.Case object at 0x794a0f134d30>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f134c10>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f135d80>, <__main__.Case object at 0x794a0f1362c0>]\n",
      "agent0 comm temp case base: []\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.7, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.7, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.7, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 2, tv: 0.3, time steps: 49\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.6, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 39\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.6, time steps: 37\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.6, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.3, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (0, 3), solution: 1, tv: 0.3, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.39999999999999997, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.39999999999999997, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.39999999999999997, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.39999999999999997, time steps: 21\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 2, 0.5, 38)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 38)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 38)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 38)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 38)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 38)\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 4, 0.5, 38)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 38)\n",
      "Episode succeeded, case (5, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 1, 0.5)\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7, time steps: 38\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.6, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.6, time steps: 36\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 28\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 2\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12e5c0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f1345e0>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f136680>, <__main__.Case object at 0x794a0f1349d0>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f1348e0>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f1347f0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12f970>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12f7f0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f134490>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f134940>, <__main__.Case object at 0x794a0f134430>, <__main__.Case object at 0x794a0f1353f0>, <__main__.Case object at 0x794a0f136410>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f1346a0>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f135630>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.6)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Integrated case process. comm case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 38\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 39\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 28\n",
      "Episode: 8, Total Steps: 38, Total Rewards: [75, -137], Status Episode: False\n",
      "------------------------------------------End of episode 8 loop--------------------\n",
      "----- starting point of Episode 9 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 2, 0.5, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 2)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 2, 0.5, 2)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 1), 4, 0.5, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.5, 4)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 1), 4, 0.5, 4)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((1, 1), 1, 0.5, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.5, 6)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((1, 1), 1, 0.5, 6)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 0), 4, 0.5, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 0), 4, 0.5, 9)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((2, 0), 2, 0.5, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 0.5, 10)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((2, 0), 2, 0.5, 10)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], ((2, 1), 4, 0.5, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.5, 11)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], ((2, 1), 4, 0.5, 11)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.5, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 0.5, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.5, 12)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.5, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.5, 13)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], ((4, 0), 4, 0.6, 28)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 28)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], ((4, 0), 4, 0.6, 28)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 0), 2, 0.6, 36)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.6, 36)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 0), 2, 0.6, 36)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 1), 4, 0.6, 37)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.6, 37)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 1), 4, 0.6, 37)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((6, 1), 2, 0.5, 38)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((6, 1), 2, 0.5, 38)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 4, 0.5, 38)]\n",
      "comm next state for agent 1: ((6, 2), 4, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 4, 0.5, 38)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.7, 38)]\n",
      "comm next state for agent 1: ((7, 2), 2, 0.7, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.7, 38)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 38)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 38)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5, 38)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5, 38)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 5), 3, 0.5, 38)]\n",
      "comm next state for agent 1: ((7, 5), 3, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 5), 3, 0.5, 38)]\n",
      "next state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.5, 38)]\n",
      "comm next state for agent 1: ((6, 5), 1, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.5, 38)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.5, 38)]\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.5, 38)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.5, 38)]\n",
      "comm next state for agent 1: ((5, 4), 2, 0.5, 38)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.5, 38)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 9 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f1344c0>, <__main__.Case object at 0x794a0f135600>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f135d80>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f135810>]\n",
      "agent0 comm temp case base: []\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.7, time steps: 37\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.7, time steps: 36\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 2\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 4, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 1), 4, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 2, 0.5, 33)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7, time steps: 28\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.6, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.6, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12e1d0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12f1c0>, <__main__.Case object at 0x794a0f12cc70>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12f4f0>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f1367a0>, <__main__.Case object at 0x794a0f134940>, <__main__.Case object at 0x794a0f136410>, <__main__.Case object at 0x794a0f1346a0>, <__main__.Case object at 0x794a0f1344f0>, <__main__.Case object at 0x794a0f135090>, <__main__.Case object at 0x794a0f134610>, <__main__.Case object at 0x794a0f136020>, <__main__.Case object at 0x794a0f1345e0>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f1348e0>, <__main__.Case object at 0x794a0f135c60>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f136230>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12fc10>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f1347f0>, <__main__.Case object at 0x794a0f135f30>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f1353f0>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f1342b0>, <__main__.Case object at 0x794a0f1354b0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f1349d0>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f135e40>, <__main__.Case object at 0x794a0f135a20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.7, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.7, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.39999999999999997, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.3, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.3, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.3, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.3, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.3, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.3, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7, time steps: 38\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 4, tv: 0.3, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.3, time steps: 39\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 4, tv: 0.3, time steps: 37\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.3, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.3, time steps: 28\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 33)\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 4, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 1, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 4, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (5, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 1), 4, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 1, 0.5)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 2, 0.5)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.5)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 0), solution: 4, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 1), solution: 1, tv: 0.5, time steps: 5\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.5, time steps: 11\n",
      "Episode: 9, Total Steps: 33, Total Rewards: [80, 68], Status Episode: True\n",
      "------------------------------------------End of episode 9 loop--------------------\n",
      "----- starting point of Episode 10 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.5, 23)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.5, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 2)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.6, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.6, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.5, 23)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.5, 24)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.5, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.6, 4)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.6, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.5, 24)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.5, 25)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 25)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 6)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.5, 25)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 9)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.6, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 0.6, 10)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.6, 10)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.6, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 11)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.6, 11)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.6, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 0.6, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.6, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.6, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.7, 21)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.6, 13)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.7, 28)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7, 28)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.7, 21)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.7, 21)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.7, 28)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 33)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.7, 21)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 33)]\n",
      "next state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.5, 33)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.5, 33)]\n",
      "next state for agent 1: [[8, 6], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.5, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 6], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.5, 33)]\n",
      "next state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 4, 0.5, 33)]\n",
      "comm next state for agent 1: ((6, 2), 4, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 4, 0.5, 33)]\n",
      "next state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.5, 33)]\n",
      "comm next state for agent 1: ((7, 2), 2, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.5, 33)]\n",
      "next state for agent 1: [[8, 6], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 33)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 6], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 33)]\n",
      "next state for agent 1: [[8, 6], False, [['agent', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5, 33)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 6], False, [['agent', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5, 33)]\n",
      "next state for agent 1: [[8, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.5, 33)]\n",
      "comm next state for agent 1: ((7, 5), 3, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.5, 33)]\n",
      "next state for agent 1: [[9, 7], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((6, 5), 1, 0.5, 33)]\n",
      "comm next state for agent 1: ((6, 5), 1, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 7], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((6, 5), 1, 0.5, 33)]\n",
      "next state for agent 1: [[9, 8], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((6, 4), 3, 0.5, 33)]\n",
      "comm next state for agent 1: ((6, 4), 3, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 8], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((6, 4), 3, 0.5, 33)]\n",
      "next state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.5, 33)]\n",
      "comm next state for agent 1: ((5, 4), 2, 0.5, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.5, 33)]\n",
      "next state for agent 1: [[8, 9], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 9], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 9], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 9], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 9], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 9], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 8], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[6, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[6, 7], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 7], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 8], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 8], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[8, 8], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 8], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 8], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 8], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 9], False, [['empty', 'agent', None], ['empty', 'empty', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 9], False, [['empty', 'agent', None], ['empty', 'empty', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['empty', 'agent', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['empty', 'agent', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['empty', 'agent', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['empty', 'agent', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['empty', 'agent', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['empty', 'agent', None], [None, None, None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 8], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 8], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 8], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 8], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[9, 7], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 7], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 7], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 6], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 10 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12f4f0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f730>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f135630>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f136410>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f136350>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f1343a0>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f1356c0>, <__main__.Case object at 0x794a0f1352a0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f1352d0>, <__main__.Case object at 0x794a0f136080>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f135990>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f136560>, <__main__.Case object at 0x794a0f1368f0>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f136f50>, <__main__.Case object at 0x794a0f136e90>, <__main__.Case object at 0x794a0f136d70>, <__main__.Case object at 0x794a0f136cb0>, <__main__.Case object at 0x794a0f136bf0>, <__main__.Case object at 0x794a0f136b30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12faf0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.6, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.7, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 0.7, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.7, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.7, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.6, time steps: 33\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 28\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.7, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.7, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.7, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.7, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7, time steps: 2\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12fc10>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f136470>, <__main__.Case object at 0x794a0f1342b0>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f1344c0>, <__main__.Case object at 0x794a0f1366e0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f135210>, <__main__.Case object at 0x794a0f135720>, <__main__.Case object at 0x794a0f1345e0>, <__main__.Case object at 0x794a0f1348e0>, <__main__.Case object at 0x794a0f135cc0>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f134e50>, <__main__.Case object at 0x794a0f1364a0>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f134970>, <__main__.Case object at 0x794a0f1360b0>, <__main__.Case object at 0x794a0f135e70>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f136140>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f136890>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f136fe0>, <__main__.Case object at 0x794a0f136ec0>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f136e00>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f136b00>, <__main__.Case object at 0x794a0f136a10>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12fdc0>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f135900>, <__main__.Case object at 0x794a0f1346a0>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f134df0>, <__main__.Case object at 0x794a0f134b50>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f135b40>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f1363e0>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f136b60>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.3, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 4, tv: 0.3, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.3, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 1, tv: 0.3, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 1, tv: 0.3, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.3, time steps: 11\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 7) is empty. Temporary case base stored to the case base: ((7, 7), 1, 0.5)\n",
      "Episode succeeded, case (7, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 7) is empty. Temporary case base stored to the case base: ((8, 7), 3, 0.5)\n",
      "Episode succeeded, case (9, 7) is empty. Temporary case base stored to the case base: ((9, 7), 3, 0.5)\n",
      "Episode succeeded, case (9, 8) is empty. Temporary case base stored to the case base: ((9, 8), 1, 0.5)\n",
      "Episode succeeded, case (9, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 9) is empty. Temporary case base stored to the case base: ((9, 9), 1, 0.5)\n",
      "Episode succeeded, case (9, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 8) is empty. Temporary case base stored to the case base: ((8, 8), 4, 0.5)\n",
      "Episode succeeded, case (8, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 8) is empty. Temporary case base stored to the case base: ((7, 8), 4, 0.5)\n",
      "Episode succeeded, case (7, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 7) is empty. Temporary case base stored to the case base: ((6, 7), 4, 0.5)\n",
      "Episode succeeded, case (6, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 9) is empty. Temporary case base stored to the case base: ((7, 9), 1, 0.5)\n",
      "Episode succeeded, case (7, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 9) is empty. Temporary case base stored to the case base: ((8, 9), 1, 0.5)\n",
      "Episode succeeded, case (8, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 2, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 2, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 3, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Integrated case process. comm case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.7)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 1, 0.6)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.6)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 2, 0.6)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.6)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.6)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 25\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6, time steps: 24\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 6), solution: 1, tv: 0.5, time steps: 48\n",
      "cases content after RETAIN, problem: (7, 7), solution: 1, tv: 0.5, time steps: 47\n",
      "cases content after RETAIN, problem: (8, 7), solution: 3, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (9, 7), solution: 3, tv: 0.5, time steps: 44\n",
      "cases content after RETAIN, problem: (9, 8), solution: 1, tv: 0.5, time steps: 43\n",
      "cases content after RETAIN, problem: (9, 9), solution: 1, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (8, 8), solution: 4, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (7, 8), solution: 4, tv: 0.5, time steps: 31\n",
      "cases content after RETAIN, problem: (6, 7), solution: 4, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (7, 9), solution: 1, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (8, 9), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 6), solution: 2, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (8, 5), solution: 2, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 3), solution: 3, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5, time steps: 5\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7, time steps: 28\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.6, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.6, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 2\n",
      "Episode: 10, Total Steps: 53, Total Rewards: [81, 48], Status Episode: True\n",
      "------------------------------------------End of episode 10 loop--------------------\n",
      "----- starting point of Episode 11 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.6, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.7, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.7, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.6, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.6, 23)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.6, 23)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.7, 2)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.6, 23)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 1), 3, 0.6, 24)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.6, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.7, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.7, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 1), 3, 0.6, 24)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 1), 2, 0.6, 25)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 25)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.7, 4)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.7, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.7, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 1), 2, 0.6, 25)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.7, 6)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((1, 0), 4, 0.7, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 11 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 0.6, 25)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 25)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((1, 0), 4, 0.7, 9)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.7, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 0.7, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 0.6, 25)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.7, 10)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.7, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.7, 11)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 11 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 3), 2, 0.7999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.7, 11)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 3), 2, 0.7999999999999999, 21)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.7999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.7, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.7, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.7999999999999999, 21)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.7999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.7, 13)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.7999999999999999, 28)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7999999999999999, 28)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.7999999999999999, 21)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 0.7999999999999999, 21)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.7999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.7999999999999999, 28)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 0), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 0.7999999999999999, 21)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 4), 3, 0.7999999999999999, 21)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 0), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((5, 1), 4, 0.6, 33)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 4), 3, 0.7999999999999999, 21)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.7999999999999999, 21)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.7999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((5, 1), 4, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.7999999999999999, 21)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 11 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 11 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.6, 33)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 33)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f134520>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f136e60>, <__main__.Case object at 0x794a0f135ba0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f134310>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.7, time steps: 33\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.8999999999999999, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.5999999999999999, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.7999999999999999, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 0.7999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.7999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.7999999999999999, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.3, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.3, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.3, time steps: 13\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 2, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 4, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 23)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 1), 4, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 2, 0.5, 23)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 23)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5999999999999999, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.7999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.7999999999999999, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.7999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.7999999999999999, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 2\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12fdc0>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12e1d0>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f1371f0>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f134df0>, <__main__.Case object at 0x794a0f135b40>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f135a20>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f135fc0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12e5c0>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f135ab0>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f135c90>, <__main__.Case object at 0x794a0f1367d0>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f135630>, <__main__.Case object at 0x794a0f135930>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.8999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.8999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 1, tv: 0.3, time steps: 48\n",
      "case content after REVISE for agent 1, problem: (7, 7), solution: 1, tv: 0.3, time steps: 47\n",
      "case content after REVISE for agent 1, problem: (8, 7), solution: 3, tv: 0.3, time steps: 45\n",
      "case content after REVISE for agent 1, problem: (9, 7), solution: 3, tv: 0.3, time steps: 44\n",
      "case content after REVISE for agent 1, problem: (9, 8), solution: 1, tv: 0.3, time steps: 43\n",
      "case content after REVISE for agent 1, problem: (9, 9), solution: 1, tv: 0.3, time steps: 41\n",
      "case content after REVISE for agent 1, problem: (8, 8), solution: 4, tv: 0.3, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (7, 8), solution: 4, tv: 0.3, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (6, 7), solution: 4, tv: 0.3, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (7, 9), solution: 1, tv: 0.3, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (8, 9), solution: 1, tv: 0.3, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 2, tv: 0.3, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 2, tv: 0.3, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 3, tv: 0.3, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 0.3, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 4, tv: 0.3, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.3, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 4, tv: 0.3, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.3, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.49999999999999994, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.39999999999999997, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 2, tv: 0.39999999999999997, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.39999999999999997, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.39999999999999997, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.39999999999999997, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.39999999999999997, time steps: 2\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.5)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 23)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 1), 3, 0.5, 23)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (5, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 1), 4, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.7)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 1, 0.7)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.7)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 2, 0.7)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.7)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.7)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.7)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.8999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.8999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 23\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 20\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.49999999999999994, time steps: 28\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.7, time steps: 11\n",
      "Episode: 11, Total Steps: 23, Total Rewards: [78, 88], Status Episode: True\n",
      "------------------------------------------End of episode 11 loop--------------------\n",
      "----- starting point of Episode 12 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.7, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.7999999999999999, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.7999999999999999, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.7, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.7, 23)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.7999999999999999, 2)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.7999999999999999, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.7999999999999999, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.7, 23)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.7999999999999999, 4)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.7999999999999999, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.7999999999999999, 6)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 12 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.7999999999999999, 6)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((1, 0), 4, 0.7999999999999999, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7999999999999999, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 12 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 0), 2, 0.7, 23)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((1, 0), 4, 0.7999999999999999, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.7999999999999999, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 0.7999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 0), 2, 0.7, 23)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.5, 23)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.5, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.7999999999999999, 10)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.7999999999999999, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.7999999999999999, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.5, 23)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 1), 2, 0.5, 23)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.7999999999999999, 11)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 1), 2, 0.5, 23)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.5, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.5, 6)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.8999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.5, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 21)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.5, 23)]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 0), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 23)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 12 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 0), 2, 0.5, 23)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.5, 23)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.5, 23)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 12 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.8999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.8999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.5, 23)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.8999999999999999, 21)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.8999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.8999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 1), 2, 0.5, 23)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 4, 0.5, 23)]\n",
      "comm next state for agent 1: ((6, 2), 4, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.8999999999999999, 21)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.8999999999999999, 21)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.8999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 4, 0.5, 23)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'agent'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((7, 2), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 2), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.8999999999999999, 21)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['agent', 'empty', 'empty']], ((6, 4), 3, 0.8999999999999999, 21)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8999999999999999, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'agent'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((7, 2), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['agent', 'empty', 'empty']], ((6, 4), 3, 0.8999999999999999, 21)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.8999999999999999, 21)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.8999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.8999999999999999, 21)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "next state for agent 0: [[6, 6], False, [['agent', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'agent']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 12 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 6], False, [['agent', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'agent']], ((7, 3), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 23)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f135630>, <__main__.Case object at 0x794a0f1346a0>, <__main__.Case object at 0x794a0f134790>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f135c90>, <__main__.Case object at 0x794a0f134b20>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.39999999999999986, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.8999999999999999, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 0.8999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.8999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.8999999999999999, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.8999999999999999, time steps: 2\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) is empty. Temporary case base stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.7)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.7)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.7)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.8999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.8999999999999999, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.8999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.8999999999999999, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.8999999999999999, time steps: 2\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 23\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f135d50>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12fc10>, <__main__.Case object at 0x794a0f12f730>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12fdc0>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f134520>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.9999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.9999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8999999999999999, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.29999999999999993, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 1, tv: 0.49999999999999994, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.49999999999999994, time steps: 11\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 3, 0.5)\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Integrated case process. comm case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.9999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.9999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 23\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8999999999999999, time steps: 20\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.49999999999999994, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.49999999999999994, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 4), solution: 3, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.7999999999999999, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.7999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.7999999999999999, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 2\n",
      "Episode: 12, Total Steps: 23, Total Rewards: [78, 85], Status Episode: True\n",
      "------------------------------------------End of episode 12 loop--------------------\n",
      "----- starting point of Episode 13 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.8999999999999999, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.8999999999999999, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.7999999999999999, 23)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7999999999999999, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.8999999999999999, 2)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.8999999999999999, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.8999999999999999, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.7999999999999999, 23)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.6, 23)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.6, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.8999999999999999, 4)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.8999999999999999, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.8999999999999999, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.6, 23)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.6, 23)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.8999999999999999, 6)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.6, 23)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.6, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 9)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.8999999999999999, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 0.8999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.6, 6)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.9999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.9999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.8999999999999999, 10)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.8999999999999999, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.8999999999999999, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.9999999999999999, 21)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.9999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.9999999999999999, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.8999999999999999, 11)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.9999999999999999, 21)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.9999999999999999, 21)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.9999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.8999999999999999, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.8999999999999999, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.9999999999999999, 21)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((6, 5), 1, 0.9999999999999999, 21)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.9999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.8999999999999999, 11)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((3, 1), 1, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((6, 5), 1, 0.9999999999999999, 21)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 4), 3, 0.9999999999999999, 21)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.9999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((3, 1), 1, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((3, 0), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 4), 3, 0.9999999999999999, 21)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 4), 2, 0.9999999999999999, 21)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.9999999999999999, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((3, 0), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 4), 2, 0.9999999999999999, 21)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 13 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f136380>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f136da0>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f136ef0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f135900>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f1349a0>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f136890>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.9999999999999999, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 0.9999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.9999999999999999, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.9999999999999999, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 1, tv: 0.3, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.49999999999999994, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.49999999999999994, time steps: 13\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 2, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 2, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 4, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 1), 4, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 2, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 22)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.9999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.9999999999999999, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.9999999999999999, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.9999999999999999, time steps: 2\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.49999999999999994, time steps: 23\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.49999999999999994, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8999999999999999, time steps: 20\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f135ab0>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f136fb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f135c90>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f1364d0>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f135a20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.8999999999999999, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.9999999999999999, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 1, tv: 0.29999999999999993, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.29999999999999993, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 3, tv: 0.3, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 4, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 4, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 2, tv: 0.5999999999999999, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5999999999999999, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.5999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.5999999999999999, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.5999999999999999, time steps: 2\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 1), 3, 0.5, 22)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 2, 0.5, 22)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9999999999999999, time steps: 20\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.5999999999999999, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5999999999999999, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5999999999999999, time steps: 2\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 23\n",
      "Episode: 13, Total Steps: 22, Total Rewards: [79, 90], Status Episode: True\n",
      "------------------------------------------End of episode 13 loop--------------------\n",
      "----- starting point of Episode 14 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.9999999999999999, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.9999999999999999, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.5, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.9999999999999999, 2)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.9999999999999999, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.9999999999999999, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.5, 22)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.5, 22)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.5, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.9999999999999999, 4)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.9999999999999999, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.9999999999999999, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.5, 22)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.5, 22)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.9999999999999999, 6)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.9999999999999999, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.9999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.5, 22)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.7, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.9999999999999999, 9)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.7, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 21)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 21)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 1, 21)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.9999999999999999, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 0.9999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 1, 21)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.9999999999999999, 10)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.9999999999999999, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.9999999999999999, 11)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 14 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 5), 3, 1, 21)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.9999999999999999, 11)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.9999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 0.9999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 5), 3, 1, 21)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 1, 21)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.9999999999999999, 12)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((3, 0), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 1, 1, 21)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 4), 3, 1, 21)]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((3, 0), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((4, 0), 4, 0.5, 22)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 4), 3, 1, 21)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 1, 21)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((4, 0), 4, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 1, 21)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[6, 3], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 14 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 0), 2, 0.5, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.5, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f134520>, <__main__.Case object at 0x794a0f135f30>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f134910>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f135ae0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f135480>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.3, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.3, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 1, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.29999999999999993, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.29999999999999993, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 4, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 1, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 1, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 2\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f136380>, <__main__.Case object at 0x794a0f136da0>, <__main__.Case object at 0x794a0f1347c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f135c30>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.39999999999999986, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 2, tv: 0.39999999999999986, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.39999999999999986, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.39999999999999986, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.39999999999999986, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.39999999999999986, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.39999999999999997, time steps: 23\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 21\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 20\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.9999999999999999, time steps: 11\n",
      "Episode: 14, Total Steps: 22, Total Rewards: [79, 89], Status Episode: True\n",
      "------------------------------------------End of episode 14 loop--------------------\n",
      "----- starting point of Episode 15 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 1, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.6, 22)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.6, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 1, 2)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 1, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 1, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.6, 22)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.6, 22)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.6, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 1, 4)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.6, 22)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.6, 22)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 6)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.6, 22)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.7999999999999999, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 1, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.7999999999999999, 6)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 21)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 1, 10)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 1, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 21)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 4), 2, 1, 21)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 1, 11)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 1, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 4), 2, 1, 21)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 21)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 1, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 1, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 21)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 1, 21)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 1, 13)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((4, 0), 4, 0.6, 22)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 1, 21)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((4, 0), 4, 0.6, 22)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((5, 0), 2, 0.6, 22)]\n",
      "comm next state for agent 1: ((5, 0), 2, 0.6, 22)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 15 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 4), 2, 1, 21)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((5, 0), 2, 0.6, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.6, 22)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.6, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 4), 2, 1, 21)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.6, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.6, 22)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.6, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [262.5, 112.5, 287.5, 137.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 20)]\n",
      "next state for agent 0: [[5, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 20)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.6, 22)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 1), 4, 0.6, 22)]\n",
      "comm next state for agent 1: ((5, 1), 4, 0.6, 22)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12fc70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12e830>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 4, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 2, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.6)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.6, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.6, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 2\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12eec0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12eb60>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.3, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 1, tv: 0.7999999999999998, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.7999999999999998, time steps: 11\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 5), 4, 0.5, 13)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 13)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 1), 3, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 2, 0.5, 13)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.6)\n",
      "Integrated case process. comm case (5, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 1), 4, 0.6)\n",
      "Integrated case process. comm case (5, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 1), 4, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 2, 0.6)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 1, 1)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 2, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 1)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.7999999999999998, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.7999999999999998, time steps: 11\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 1, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 1, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 2\n",
      "Episode: 15, Total Steps: 13, Total Rewards: [-112, 90], Status Episode: False\n",
      "------------------------------------------End of episode 15 loop--------------------\n",
      "----- starting point of Episode 16 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 2)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.5, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 2)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.6, 4)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.6, 4)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.5, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.5, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.6, 4)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 6)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.6, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((8, 1), 3, 0.5, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.5, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 6)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 9)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.5, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.8999999999999999, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.8999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 9)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.6, 10)]\n",
      "comm next state for agent 1: ((2, 0), 2, 0.6, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.8999999999999999, 6)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.5, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 2, 0.6, 10)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.6, 11)]\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.5, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 4), 2, 0.5, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 1), 4, 0.6, 11)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.6, 12)]\n",
      "comm next state for agent 1: ((3, 1), 1, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 4), 2, 0.5, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 0.5, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 1, 0.6, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.6, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 0.5, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((6, 5), 1, 0.5, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.6, 13)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((6, 5), 1, 0.5, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 4), 3, 0.5, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((3, 0), 4, 0.6, 13)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 4), 3, 0.5, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((3, 0), 4, 0.6, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.5, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 16 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 16 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [212.5, 62.5, 237.5, 87.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.5, 13)]\n",
      "next state for agent 0: [[4, 1], False, [['empty', 'empty', 'empty'], ['empty', 'obstacle', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.5, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12f4c0>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f136da0>, <__main__.Case object at 0x794a0f136e60>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f1346d0>, <__main__.Case object at 0x794a0f137160>, <__main__.Case object at 0x794a0f135d80>, <__main__.Case object at 0x794a0f135a20>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f136050>, <__main__.Case object at 0x794a0f136080>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12fc10>, <__main__.Case object at 0x794a0f12e5c0>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f1363e0>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f136440>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f135bd0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 1, tv: 0.19999999999999996, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 2, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.19999999999999996, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.19999999999999996, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.09999999999999998, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.3999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f135630>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f1360e0>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f1367d0>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f136350>, <__main__.Case object at 0x794a0f1351e0>, <__main__.Case object at 0x794a0f1348e0>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f135ab0>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f135300>, <__main__.Case object at 0x794a0f135b40>, <__main__.Case object at 0x794a0f134d00>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f136560>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12e230>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 1, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.5999999999999999, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 4, tv: 0.39999999999999997, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.39999999999999997, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 2, tv: 0.8, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.8, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.8, time steps: 2\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 1, 0.6)\n",
      "Integrated case process. comm case (2, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.6)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 2, 0.6)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.6)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.6)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 1, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.5999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.8, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.8, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.8, time steps: 2\n",
      "Episode: 16, Total Steps: 37, Total Rewards: [-136, 90], Status Episode: False\n",
      "------------------------------------------End of episode 16 loop--------------------\n",
      "----- starting point of Episode 17 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((8, 0), 2, 0.6, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((8, 0), 2, 0.6, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.6, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.6, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((7, 1), 2, 0.6, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((7, 1), 2, 0.6, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((7, 2), 2, 0.9999999999999999, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.9999999999999999, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((7, 2), 2, 0.9999999999999999, 6)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((7, 3), 2, 0.6, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((7, 3), 2, 0.6, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 4), 2, 0.6, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 4), 2, 0.6, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 0.6, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 0.6, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.6, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.6, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 0.6, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 0.6, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.6, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [62.5, 162.5, 87.5, 187.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['agent', 'obstacle', 'empty'], ['empty', 'obstacle', 'obstacle']], ((5, 5), 4, 0.6, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f136560>, <__main__.Case object at 0x794a0f1358a0>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f134190>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12fc10>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f135540>, <__main__.Case object at 0x794a0f135ff0>, <__main__.Case object at 0x794a0f135c90>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f134fa0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.6)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.6)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.6)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.6)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12f7f0>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12dff0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a0f1363e0>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f134cd0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 1, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.39999999999999986, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 2, tv: 0.6000000000000001, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.6000000000000001, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.6000000000000001, time steps: 2\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (2, 0), solution: 2, tv: 0.6000000000000001, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.6000000000000001, time steps: 4\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6000000000000001, time steps: 2\n",
      "Episode: 17, Total Steps: 29, Total Rewards: [-128, 90], Status Episode: False\n",
      "------------------------------------------End of episode 17 loop--------------------\n",
      "----- starting point of Episode 18 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.7, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.7, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.7, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.7, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.7, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.7, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.7, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.7, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 0.7, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 0.7, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 4), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 4), 2, 0.7, 13)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 13)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'empty', 'obstacle'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 112.5, 137.5, 137.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 2], False, [['empty', 'empty', 'obstacle'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 13)]\n",
      "next state for agent 0: [[2, 2], False, [['empty', 'empty', 'empty'], ['empty', 'obstacle', 'agent'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12f6d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12f220>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.7)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.7)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.7)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.7)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.7)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.7)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12fcd0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12f7f0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 2, tv: 0.4000000000000001, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.4000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.4000000000000001, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.4000000000000001, time steps: 2\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 18, Total Steps: 14, Total Rewards: [-113, 90], Status Episode: False\n",
      "------------------------------------------End of episode 18 loop--------------------\n",
      "----- starting point of Episode 19 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((7, 5), 3, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((7, 5), 3, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 19 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 19 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 62.5, 137.5, 87.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f135fc0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f12f850>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f135c90>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 19, Total Steps: 25, Total Rewards: [-124, 92], Status Episode: False\n",
      "------------------------------------------End of episode 19 loop--------------------\n",
      "----- starting point of Episode 20 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((8, 0), 2, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((8, 0), 2, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((7, 5), 3, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((7, 5), 3, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((6, 5), 1, 0.5999999999999999, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((6, 5), 1, 0.5999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 4), 3, 0.5999999999999999, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 4), 3, 0.5999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 4), 2, 0.5999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 4), 2, 0.5999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 62.5, 137.5, 87.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12dff0>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f137100>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f1347c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5999999999999999)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12f7f0>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12f730>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f136080>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f134850>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 20, Total Steps: 30, Total Rewards: [-129, 90], Status Episode: False\n",
      "------------------------------------------End of episode 20 loop--------------------\n",
      "----- starting point of Episode 21 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((6, 5), 1, 0.6999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((6, 5), 1, 0.6999999999999998, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((6, 4), 3, 0.6999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((6, 4), 3, 0.6999999999999998, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 4), 2, 0.6999999999999998, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 4), 2, 0.6999999999999998, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 21 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 6], False, [[None, 'agent', 'obstacle'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 6], False, [[None, 'agent', 'obstacle'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 7], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 7], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 7], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 7], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 7], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 7], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[0, 6], False, [[None, 'empty', 'obstacle'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 6], False, [[None, 'empty', 'obstacle'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 6], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 6], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[2, 6], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 6], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[2, 5], False, [['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 5], False, [['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[2, 6], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 6], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 6], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [62.5, 262.5, 87.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 6], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "next state for agent 0: [[1, 5], False, [['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.9999999999999999, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12f730>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f1362f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f12dc00>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f135180>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.6999999999999998)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.6999999999999998)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.6999999999999998)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12dff0>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f137100>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 21, Total Steps: 27, Total Rewards: [-126, 90], Status Episode: False\n",
      "------------------------------------------End of episode 21 loop--------------------\n",
      "----- starting point of Episode 22 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 22 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 0.7999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.7999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 0.7999999999999998, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((6, 4), 3, 0.7999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((6, 4), 3, 0.7999999999999998, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 4), 2, 0.7999999999999998, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 4), 2, 0.7999999999999998, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 22 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 62.5, 137.5, 87.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f136440>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f135ff0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135000>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.7999999999999998)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f134700>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 22, Total Steps: 29, Total Rewards: [-128, 88], Status Episode: False\n",
      "------------------------------------------End of episode 22 loop--------------------\n",
      "----- starting point of Episode 23 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 23 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 23 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.8999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.8999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.8999999999999998, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((6, 4), 3, 0.8999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((6, 4), 3, 0.8999999999999998, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 4), 2, 0.8999999999999998, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.8999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 4), 2, 0.8999999999999998, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 62.5, 137.5, 87.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f1347c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f136440>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f134850>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.8999999999999998)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.8999999999999998)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.8999999999999998)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f135ea0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 1\n",
      "Episode: 23, Total Steps: 29, Total Rewards: [-128, 90], Status Episode: False\n",
      "------------------------------------------End of episode 23 loop--------------------\n",
      "----- starting point of Episode 24 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 0.8, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 0.8, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.8, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 0.8, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 24 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((6, 5), 1, 0.9999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.9999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((6, 5), 1, 0.9999999999999998, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((6, 4), 3, 0.9999999999999998, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.9999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((6, 4), 3, 0.9999999999999998, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.9999999999999998, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.9999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 0.9999999999999998, 13)]\n",
      "next state for agent 0: [[2, 4], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 24 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 4], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 5], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 5], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [162.5, 212.5, 187.5, 237.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 4], False, [['obstacle', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12fb50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12ffd0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.9999999999999998)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.9999999999999998)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.9999999999999998)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.8)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.8)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12f730>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12e7d0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.3, time steps: 1\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 24, Total Steps: 15, Total Rewards: [-114, 90], Status Episode: False\n",
      "------------------------------------------End of episode 24 loop--------------------\n",
      "----- starting point of Episode 25 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.9, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.9, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.9, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.9, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 4], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 4], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], 0]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 62.5, 137.5, 87.5]\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'obstacle', 'agent'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'obstacle', 'agent'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 25 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: False\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f137130>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f135f00>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.9)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.9)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f136440>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135ff0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 1, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 4, 0.5)\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 1, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 3), solution: 1, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (9, 4), solution: 1, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (8, 4), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.5, time steps: 9\n",
      "Episode: 25, Total Steps: 33, Total Rewards: [-128, 68], Status Episode: False\n",
      "------------------------------------------End of episode 25 loop--------------------\n",
      "----- starting point of Episode 26 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1.0, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 1.0, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 26 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 26 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 26 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 26 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [62.5, 262.5, 87.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[1, 5], False, [['empty', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f1364d0>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f1343a0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f1371f0>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f136110>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f135a20>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f1356f0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1.0)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1.0)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f135ff0>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f134760>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f135d80>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f136710>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 1, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.3, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 1, tv: 0.3, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 1, tv: 0.3, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 4, tv: 0.3, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.3, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.3, time steps: 9\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 26, Total Steps: 34, Total Rewards: [-133, 91], Status Episode: False\n",
      "------------------------------------------End of episode 26 loop--------------------\n",
      "----- starting point of Episode 27 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 27 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 27 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 27 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.8, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.8, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 4), 3, 0.8, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((6, 4), 3, 0.8, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.8, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 27 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [262.5, 62.5, 287.5, 87.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f1356f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12fca0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.8)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.8)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.8)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12dc00>, <__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f135f00>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Episode succeeded, case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 5\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 4\n",
      "Episode: 27, Total Steps: 24, Total Rewards: [-123, 88], Status Episode: False\n",
      "------------------------------------------End of episode 27 loop--------------------\n",
      "----- starting point of Episode 28 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 1), 2, 0.8, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 1), 2, 0.8, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.9, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((6, 5), 1, 0.9, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((6, 4), 3, 0.9, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((6, 4), 3, 0.9, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 4), 2, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 4), 2, 0.9, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 28 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 28 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 28 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 28 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 28 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 28 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [62.5, 262.5, 87.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[1, 5], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f136e60>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f136350>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f136560>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f1352d0>, <__main__.Case object at 0x794a0f134c10>, <__main__.Case object at 0x794a0f1354b0>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f136bc0>, <__main__.Case object at 0x794a0f136d70>, <__main__.Case object at 0x794a0f137070>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12dc00>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f1343a0>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f1346d0>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f135ab0>, <__main__.Case object at 0x794a0f135b40>, <__main__.Case object at 0x794a0f135720>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a0f136cb0>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f1366b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.9)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.9)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.9)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.8)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f1358a0>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f1364d0>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f1363e0>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f135060>, <__main__.Case object at 0x794a0f137160>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f136680>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f1356c0>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f134520>, <__main__.Case object at 0x794a0f135540>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f134df0>, <__main__.Case object at 0x794a0f135990>, <__main__.Case object at 0x794a0f135600>, <__main__.Case object at 0x794a0f1345e0>, <__main__.Case object at 0x794a0f136b30>, <__main__.Case object at 0x794a0f136470>, <__main__.Case object at 0x794a0f136e90>, <__main__.Case object at 0x794a0f1370a0>, <__main__.Case object at 0x794a0f134c40>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 4, tv: 0.3, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.3, time steps: 4\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 28, Total Steps: 52, Total Rewards: [-151, 90], Status Episode: False\n",
      "------------------------------------------End of episode 28 loop--------------------\n",
      "----- starting point of Episode 29 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.9, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.9, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((6, 5), 1, 1.0, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((6, 5), 1, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((6, 4), 3, 1.0, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((6, 4), 3, 1.0, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 4), 2, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 4), 2, 1.0, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 29 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 29 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 29 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [112.5, 162.5, 137.5, 187.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 3], False, [['empty', 'empty', 'empty'], ['empty', 'obstacle', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12eec0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f136590>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f134850>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f134a00>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f134760>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f135ae0>, <__main__.Case object at 0x794a0f135b40>, <__main__.Case object at 0x794a0f136d40>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f136170>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1.0)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1.0)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1.0)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.9)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (6, 3), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f1343a0>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f136cb0>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135270>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 29, Total Steps: 35, Total Rewards: [-134, 90], Status Episode: False\n",
      "------------------------------------------End of episode 29 loop--------------------\n",
      "----- starting point of Episode 30 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 1.0, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 1.0, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 5), 1, 1, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 5), 1, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 1, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 30 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 30 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 30 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12dfc0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f136d40>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f134d00>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12f760>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f134760>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135ab0>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f136710>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.49999999999999994, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 4, tv: 0.3, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.3, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.39999999999999997, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 0.5)\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.5)\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 1, 0.5)\n",
      "Episode succeeded, case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 3, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1.0)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.49999999999999994, time steps: 20\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 26\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 2), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f134a00>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f135ae0>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f136590>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f135a20>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode: 30, Total Steps: 35, Total Rewards: [66, 90], Status Episode: True\n",
      "------------------------------------------End of episode 30 loop--------------------\n",
      "----- starting point of Episode 31 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.5, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 3)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 31 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.5, 3)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.5, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.5, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 31 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.5, 16)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.5, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.5, 17)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 19)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 19)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.5, 20)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.5, 20)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.5, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.5, 23)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 0.5, 24)]\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 24)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 0.5, 24)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 31 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[8, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.5, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 25)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f135840>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f134160>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.29999999999999993, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.6, time steps: 26\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.6, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 1, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 3, tv: 0.3, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 4, 0.5)\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 1, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 26\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.6, time steps: 25\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6, time steps: 24\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "cases content after RETAIN, problem: (6, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 3), solution: 1, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f1366b0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f136cb0>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f135b10>, <__main__.Case object at 0x794a0f135ba0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f135d80>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.5)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "Episode: 31, Total Steps: 26, Total Rewards: [75, 91], Status Episode: True\n",
      "------------------------------------------End of episode 31 loop--------------------\n",
      "----- starting point of Episode 32 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.6, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 3)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.6, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.6, 16)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 17)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.6, 19)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.6, 19)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.6, 20)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.6, 20)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 23)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.6, 23)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 0.6, 24)]\n",
      "comm next state for agent 1: ((5, 0), 4, 0.6, 24)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.8, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 0.6, 24)]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((6, 0), 4, 0.6, 25)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.6, 25)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 0.8, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.8, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((6, 0), 4, 0.6, 25)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((7, 0), 2, 0.6, 26)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.6, 26)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.8, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((7, 0), 2, 0.6, 26)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.8, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a0f1340a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f134850>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.8999999999999999, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.39999999999999986, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.7, time steps: 26\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.7, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 1, tv: 0.3, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.3, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.8, time steps: 13\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 1, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 18)\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 18)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.8)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.8)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.8)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f136590>, <__main__.Case object at 0x794a0f136530>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f1346d0>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f134340>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.3, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.3, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.3, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.3, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.3, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.3, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.3, time steps: 3\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.6)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.6)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.6)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 26\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "Episode: 32, Total Steps: 18, Total Rewards: [83, 90], Status Episode: True\n",
      "------------------------------------------End of episode 32 loop--------------------\n",
      "----- starting point of Episode 33 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.7, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.7, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.7, 3)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.7, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.7, 16)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.7, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.7, 17)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.7, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.7, 18)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 18)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.5, 18)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 0.9, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.5, 18)]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 0), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 0.9, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.9, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 0), 4, 0.5, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((6, 0), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.9, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((6, 0), 4, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 0.9, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f134be0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f1340a0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.9)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.9)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.9)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a0f134160>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f12f9a0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12f730>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f135330>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.39999999999999997, time steps: 26\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.39999999999999997, time steps: 18\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.7)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.7)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "Episode: 33, Total Steps: 19, Total Rewards: [82, 90], Status Episode: True\n",
      "------------------------------------------End of episode 33 loop--------------------\n",
      "----- starting point of Episode 34 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 0.7999999999999999, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.7999999999999999, 3)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 0.7999999999999999, 3)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 1), 4, 0.7999999999999999, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.7999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 1), 4, 0.7999999999999999, 16)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 1), 1, 0.7999999999999999, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.7999999999999999, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 1), 1, 0.7999999999999999, 17)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 0), 4, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 0), 4, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((2, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((2, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((3, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 3], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((3, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((4, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((4, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((5, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.6, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.6, 18)]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.9999999999999999, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.9999999999999999, 13)]\n",
      "next state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 4], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.6, 18)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'agent'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.6, 18)]\n",
      "next state for agent 1: [[8, 5], False, [['empty', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.6, 18)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'agent'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 5], False, [['empty', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.6, 18)]\n",
      "next state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.6, 18)]\n",
      "comm next state for agent 1: ((7, 5), 3, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.6, 18)]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 5), 1, 0.6, 18)]\n",
      "comm next state for agent 1: ((6, 5), 1, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((6, 5), 1, 0.6, 18)]\n",
      "next state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.6, 18)]\n",
      "comm next state for agent 1: ((6, 4), 3, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 0.6, 18)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 4), 2, 0.6, 18)]\n",
      "comm next state for agent 1: ((5, 4), 2, 0.6, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 4), 2, 0.6, 18)]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 3], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 34 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12e4a0>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f136cb0>, <__main__.Case object at 0x794a0f134700>]\n",
      "agent0 comm temp case base: []\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.8999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.8, time steps: 13\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.8999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.8999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.8999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f134760>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f136e60>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f136da0>, <__main__.Case object at 0x794a0f135ae0>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f136350>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12f010>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12efe0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f1347c0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f136a40>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.49999999999999994, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.49999999999999994, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.49999999999999994, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 4, 0.5)\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 1, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 1, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 0.6)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.6)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.6)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.6)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.6)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.49999999999999994, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.49999999999999994, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.49999999999999994, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (9, 3), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 3), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (8, 4), solution: 1, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (8, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 4), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.5, time steps: 4\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "Episode: 34, Total Steps: 33, Total Rewards: [83, 68], Status Episode: True\n",
      "------------------------------------------End of episode 34 loop--------------------\n",
      "----- starting point of Episode 35 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.8, 13)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.8999999999999999, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.8999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.8, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.8, 13)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.8999999999999999, 3)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 2, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.8, 13)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 1), 3, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.8, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.8, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 3), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 6], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 6], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 6], False, [['target', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 6], False, [['target', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 6], False, [['target', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.8999999999999999, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.8999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 6], False, [['target', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((0, 1), 4, 0.8999999999999999, 16)]\n",
      "next state for agent 1: [[6, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((1, 1), 1, 0.8999999999999999, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.8999999999999999, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 hit an obstacle! Next state: [312.5, 412.5, 337.5, 437.5]\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((1, 1), 1, 0.8999999999999999, 17)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 35 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 8], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f430>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f134310>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12f8b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.9999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.9999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 13\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 1, 0.5)\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 3) is empty. Temporary case base stored to the case base: ((0, 3), 1, 0.5)\n",
      "Episode succeeded, case (0, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.8)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.8)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.8)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.8)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.9999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.9999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.9999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (0, 2), solution: 1, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (0, 3), solution: 1, tv: 0.5, time steps: 5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8, time steps: 13\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12df90>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f137100>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f1362f0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.49999999999999994, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.49999999999999994, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.49999999999999994, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.5, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 1, tv: 0.5, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 1, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.5, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.49999999999999994, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.49999999999999994, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.49999999999999994, time steps: 3\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (9, 3), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 3), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (8, 4), solution: 1, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (8, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 4), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.5, time steps: 4\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "Episode: 35, Total Steps: 26, Total Rewards: [75, -111], Status Episode: False\n",
      "------------------------------------------End of episode 35 loop--------------------\n",
      "----- starting point of Episode 36 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 2, 0.9999999999999999, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.9999999999999999, 3)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 36 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 2, 0.9999999999999999, 3)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 1), 4, 0.9999999999999999, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.9999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 36 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 1), 4, 0.9999999999999999, 16)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 1), 1, 0.9999999999999999, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.9999999999999999, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 36 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 1), 1, 0.9999999999999999, 17)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((1, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 36 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((9, 1), 2, 0.5, 4)]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.5, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((1, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((2, 0), 4, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((9, 1), 2, 0.5, 4)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 2), 3, 0.5, 24)]\n",
      "comm next state for agent 0: ((9, 2), 3, 0.5, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((2, 0), 4, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 2), 3, 0.5, 24)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 2), 3, 0.5, 25)]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.5, 25)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 2), 3, 0.5, 25)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.6, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.6, 6)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.6, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.6, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.6, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.6, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.6, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 0), 4, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.6, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "comm next state for agent 1: ((7, 1), 2, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 1, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((7, 1), 2, 1, 13)]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 1: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 1, 1, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 1, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((7, 3), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 3), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 1, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((7, 3), 2, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 4), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12ffd0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f135bd0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12fb50>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f1345b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.49999999999999994, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 1, tv: 0.3, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (0, 3), solution: 1, tv: 0.3, time steps: 5\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.6)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 3, 0.5)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.5)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.49999999999999994, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.5, time steps: 4\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f135c60>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12fee0>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12fd30>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f134310>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.29999999999999993, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.29999999999999993, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.29999999999999993, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.6, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 1, tv: 0.3, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 1, tv: 0.3, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 1, tv: 0.3, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 3, tv: 0.3, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.39999999999999997, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 2), 3, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 2), 3, 0.5, 18)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.6, time steps: 4\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "Episode: 36, Total Steps: 18, Total Rewards: [83, 87], Status Episode: True\n",
      "------------------------------------------End of episode 36 loop--------------------\n",
      "----- starting point of Episode 37 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 2, 0.5, 3)]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.5, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 1, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 2, 0.5, 3)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((9, 1), 2, 0.6, 4)]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.6, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 1, 3)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 1), 4, 1, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((9, 1), 2, 0.6, 4)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((9, 2), 3, 0.5, 18)]\n",
      "comm next state for agent 0: ((9, 2), 3, 0.5, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 1), 4, 1, 16)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((9, 2), 3, 0.5, 18)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 2), 3, 0.5, 18)]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.5, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((8, 2), 3, 0.5, 18)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.7, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.7, 6)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.7, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.7, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 1, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((5, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 1, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 1, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((6, 0), 4, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((6, 0), 4, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 1, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], ((6, 0), 4, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 37 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f1364d0>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f135ae0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f135060>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f136710>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.29999999999999993, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.3, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 3, tv: 0.3, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.3, time steps: 4\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7)\n",
      "Integrated case process. comm case (8, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 2), 3, 0.5)\n",
      "Integrated case process. comm case (9, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 2), 3, 0.5)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.6)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5, time steps: 3\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f1351e0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f11bdf0>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12e230>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f136680>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.39999999999999986, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 1)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 3\n",
      "Episode: 37, Total Steps: 19, Total Rewards: [82, 90], Status Episode: True\n",
      "------------------------------------------End of episode 37 loop--------------------\n",
      "----- starting point of Episode 38 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 2, 0.6, 3)]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.6, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 1, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 2, 0.6, 3)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((9, 1), 2, 0.7, 4)]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.7, 4)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 1, 3)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 1), 4, 1, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((9, 1), 2, 0.7, 4)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((9, 2), 3, 0.6, 18)]\n",
      "comm next state for agent 0: ((9, 2), 3, 0.6, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 1), 4, 1, 16)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((9, 2), 3, 0.6, 18)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 2), 3, 0.6, 18)]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.6, 18)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "next state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((8, 2), 3, 0.6, 18)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 2), 2, 0.7999999999999999, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((7, 2), 2, 0.7999999999999999, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 3), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 3), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 5), 1, 1, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['empty', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((3, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((3, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 5), 1, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 1, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['target', 'agent', 'empty']], ((3, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 4), 3, 1, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'target', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'target', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 38 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 38 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 38 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'empty', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 4], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 0.9999999999999999, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f134a00>, <__main__.Case object at 0x794a0f135fc0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f1341c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f1197b0>, <__main__.Case object at 0x794a0f12f5b0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12fd30>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f135c60>, <__main__.Case object at 0x794a0f136590>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f135300>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.3, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.6)\n",
      "Integrated case process. comm case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 3, 0.6)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.7)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12f1c0>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f1351e0>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f1371c0>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f134fa0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12eb00>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12dcc0>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f135ae0>, <__main__.Case object at 0x794a0f1364d0>, <__main__.Case object at 0x794a0f136ec0>, <__main__.Case object at 0x794a0f1346d0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f1355d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.7999999999999999, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.39999999999999986, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.39999999999999986, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.39999999999999986, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.39999999999999986, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.8, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.8, time steps: 3\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 1)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 1)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7999999999999999, time steps: 4\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.8, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.8, time steps: 3\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "Episode: 38, Total Steps: 25, Total Rewards: [76, 90], Status Episode: True\n",
      "------------------------------------------End of episode 38 loop--------------------\n",
      "----- starting point of Episode 39 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 2, 0.7, 3)]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.7, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 1, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 2, 0.7, 3)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 1), 2, 0.7999999999999999, 4)]\n",
      "comm next state for agent 0: ((9, 1), 2, 0.7999999999999999, 4)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((0, 0), 2, 1, 3)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 1), 2, 0.7999999999999999, 4)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 2), 3, 0.7, 18)]\n",
      "comm next state for agent 0: ((9, 2), 3, 0.7, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 1), 4, 1, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((9, 2), 3, 0.7, 18)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 1), 4, 1, 16)]\n",
      "next state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 1, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 39 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 2), 3, 0.7, 18)]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.7, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 1, 17)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 2), 3, 0.7, 18)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.8999999999999999, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.8999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.8999999999999999, 6)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 1, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 1, 13)]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'agent', 'empty']], ((6, 0), 4, 1, 18)]\n",
      "comm next state for agent 1: ((6, 0), 4, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 hit an obstacle! Next state: [262.5, 212.5, 287.5, 237.5]\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 1, 1, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 1, 13)]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'agent', 'empty']], ((6, 0), 4, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'obstacle', 'agent'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 3, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'obstacle', 'agent'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 39 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['agent', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'agent']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[6, 5], False, [['agent', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'agent']], ((7, 0), 2, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'agent'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 hit an obstacle! Next state: [262.5, 212.5, 287.5, 237.5]\n",
      "Agent 1 is locked. Done status: True, win status: False\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['target', 'agent', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'agent'], ['empty', 'target', 'empty']], ((5, 4), 2, 1, 13)]\n",
      "comm next state for agent 0: ((5, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'agent'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'target', 'empty']], ((7, 0), 2, 1, 18)]\n",
      "comm next state for agent 1: ((7, 0), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f136710>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f12fc40>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12feb0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f135930>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (5, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 2), 3, 0.7)\n",
      "Integrated case process. comm case (9, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 2), 3, 0.7)\n",
      "Integrated case process. comm case (9, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12f8b0>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12f910>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12ed10>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a0f136590>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f136ec0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12e2c0>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12fe50>, <__main__.Case object at 0x794a0f12e380>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f136770>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.4999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.29999999999999993, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.29999999999999993, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.3999999999999999, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.29999999999999993, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.8, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.8, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 1)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 4, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 1)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.4999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.8, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.8, time steps: 3\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 18\n",
      "Episode: 39, Total Steps: 18, Total Rewards: [-117, -110], Status Episode: False\n",
      "------------------------------------------End of episode 39 loop--------------------\n",
      "----- starting point of Episode 40 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 3)]\n",
      "comm next state for agent 1: ((0, 0), 2, 0.6, 3)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 40 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 2, 0.6, 3)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((0, 1), 4, 0.6, 16)]\n",
      "comm next state for agent 1: ((0, 1), 4, 0.6, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 40 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 0), 2, 0.7, 18)]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.7, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((0, 1), 4, 0.6, 16)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 17)]\n",
      "comm next state for agent 1: ((1, 1), 1, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 0), 2, 0.7, 18)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.6000000000000001, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6000000000000001, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 1), 1, 0.6, 17)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.6000000000000001, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.4999999999999999, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.4999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.4999999999999999, 6)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.4999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.6, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.4999999999999999, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 0.4999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.6, 18)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 0.4999999999999999, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.6, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.6, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 40 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 40 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 hit an obstacle! Next state: [262.5, 212.5, 287.5, 237.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12e9b0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12fca0>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f136da0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12f340>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f1355d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.19999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 4, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.6)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6000000000000001)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f1199c0>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12f8e0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f134e80>, <__main__.Case object at 0x794a0f134c40>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12efe0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.39999999999999997, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.39999999999999997, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.39999999999999997, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.5999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.49999999999999994, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.6000000000000001, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.6000000000000001, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 1), 1, 0.6)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 4, 0.6)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.49999999999999994, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.6000000000000001, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6000000000000001, time steps: 3\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8, time steps: 18\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 0\n",
      "Episode: 40, Total Steps: 22, Total Rewards: [-121, 92], Status Episode: False\n",
      "------------------------------------------End of episode 40 loop--------------------\n",
      "----- starting point of Episode 41 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 0)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.5, 0)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 3, 0.5, 1)]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 1)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 3, 0.5, 1)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.7999999999999999, 18)]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.7999999999999999, 18)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 41 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.5999999999999999, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5999999999999999, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.5999999999999999, 6)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 0.5999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 0.5999999999999999, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((7, 5), 3, 0.7, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 41 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 3], False, [['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 3], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 3], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 4], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 41 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 4], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 4], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [162.5, 162.5, 187.5, 187.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 3], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 3], False, [['obstacle', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12fd30>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12de10>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12ff70>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12eb90>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12f880>, <__main__.Case object at 0x794a0f12fc10>, <__main__.Case object at 0x794a0f12e5c0>, <__main__.Case object at 0x794a0f12e7d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f1199f0>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f12f850>, <__main__.Case object at 0x794a0f12e980>, <__main__.Case object at 0x794a0f12e440>, <__main__.Case object at 0x794a0f12f070>, <__main__.Case object at 0x794a0f12ccd0>, <__main__.Case object at 0x794a0f12fa00>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12e950>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12f7c0>, <__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f12f280>, <__main__.Case object at 0x794a0f12fd60>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.7)\n",
      "Integrated case process. comm case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12df30>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12e3b0>, <__main__.Case object at 0x794a0f12e7a0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12fe80>, <__main__.Case object at 0x794a0f12eef0>, <__main__.Case object at 0x794a0f12e020>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12fc70>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6999999999999998, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.29999999999999993, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 1, tv: 0.4000000000000001, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.4000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.4000000000000001, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 0\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6999999999999998, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 18\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 0\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.5, time steps: 8\n",
      "Episode: 41, Total Steps: 21, Total Rewards: [-120, 92], Status Episode: False\n",
      "------------------------------------------End of episode 41 loop--------------------\n",
      "----- starting point of Episode 42 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.6, 0)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.6, 0)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 3, 0.6, 1)]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 1)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((8, 0), 3, 0.6, 1)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 0), 2, 0.8999999999999999, 18)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.8, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.8, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 4], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 1), 2, 0.8, 13)]\n",
      "next state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.6999999999999998, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6999999999999998, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 4], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 0.6999999999999998, 6)]\n",
      "next state for agent 0: [[1, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 3), 2, 0.6999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 3), 2, 0.6999999999999998, 13)]\n",
      "next state for agent 0: [[1, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.6999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 5], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 4), 2, 0.6999999999999998, 13)]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], ((7, 5), 3, 0.7999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'obstacle']], ((7, 5), 3, 0.7999999999999999, 13)]\n",
      "next state for agent 0: [[0, 6], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((6, 5), 3, 0.5, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 0.5, 8)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 6], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((6, 5), 3, 0.5, 8)]\n",
      "next state for agent 0: [[0, 6], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 6], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 5], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 42 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 4], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 5], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 6], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 6], False, [[None, 'agent', 'empty'], [None, 'empty', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 6], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 hit an obstacle! Next state: [62.5, 312.5, 87.5, 337.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 6], False, [[None, 'empty', 'empty'], [None, 'agent', 'obstacle'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 6], False, [['empty', 'empty', 'empty'], ['agent', 'obstacle', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12ebc0>, <__main__.Case object at 0x794a0f12f3a0>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12eaa0>, <__main__.Case object at 0x794a0f12fdf0>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12f0d0>, <__main__.Case object at 0x794a0f12dde0>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12fac0>, <__main__.Case object at 0x794a0f12f160>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12e6b0>, <__main__.Case object at 0x794a0f12fb80>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12f5e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12e050>, <__main__.Case object at 0x794a0f12e5f0>, <__main__.Case object at 0x794a0f12ead0>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12f220>, <__main__.Case object at 0x794a0f12e6e0>, <__main__.Case object at 0x794a0f12dc30>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12e5c0>, <__main__.Case object at 0x794a0f12e260>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12e140>, <__main__.Case object at 0x794a0f12f490>, <__main__.Case object at 0x794a0f12eef0>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 1\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 3, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.6999999999999998)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.6999999999999998)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.6999999999999998)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.8)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 1, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.7, time steps: 4\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f12fc70>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12e770>, <__main__.Case object at 0x794a0f12fa30>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12f310>, <__main__.Case object at 0x794a0f12f190>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12f550>, <__main__.Case object at 0x794a0f12df00>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12ec50>, <__main__.Case object at 0x794a0f12f1c0>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12ece0>, <__main__.Case object at 0x794a0f12fbe0>, <__main__.Case object at 0x794a0f12f460>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7999999999999998, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 4, tv: 0.4000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4000000000000001, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.6, time steps: 8\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 2, 0.5, 17)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999998, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7, time steps: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 0\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.6, time steps: 8\n",
      "Episode: 42, Total Steps: 17, Total Rewards: [-116, 92], Status Episode: False\n",
      "------------------------------------------End of episode 42 loop--------------------\n",
      "----- starting point of Episode 43 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 43 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.7, 0)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7, 0)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((9, 0), 3, 0.7, 0)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 3, 0.7, 1)]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7, 1)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((8, 0), 3, 0.7, 1)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 17)]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.5, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.5, 17)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 1), 2, 0.9, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.9, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 1), 2, 0.9, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 0.7999999999999998, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7999999999999998, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[0, 1], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((7, 2), 2, 0.7999999999999998, 6)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 0.7999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 3), 2, 0.7999999999999998, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 0.7999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((7, 4), 2, 0.7999999999999998, 13)]\n",
      "next state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 5), 3, 0.8999999999999999, 13)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((7, 5), 3, 0.8999999999999999, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 3, 0.6, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 0.6, 8)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 3, 0.6, 8)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 43 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'agent', 'empty'], [None, 'empty', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 3], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'agent', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 2], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'empty', 'agent'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 43 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 2], False, [[None, 'empty', 'empty'], [None, 'agent', 'empty'], [None, 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[0, 1], False, [[None, 'empty', 'empty'], [None, 'empty', 'empty'], [None, 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle'], ['empty', 'agent', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 2], False, [['empty', 'agent', 'obstacle'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f119990>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f12f460>, <__main__.Case object at 0x794a0f12f6d0>, <__main__.Case object at 0x794a0f12f790>, <__main__.Case object at 0x794a0f12eda0>, <__main__.Case object at 0x794a0f12e7d0>, <__main__.Case object at 0x794a0f12e920>, <__main__.Case object at 0x794a0f12f520>, <__main__.Case object at 0x794a0f12fd30>, <__main__.Case object at 0x794a0f12e800>, <__main__.Case object at 0x794a0f12fbb0>, <__main__.Case object at 0x794a0f12eb60>, <__main__.Case object at 0x794a0f12ff10>, <__main__.Case object at 0x794a0f12f7c0>, <__main__.Case object at 0x794a0f12ff40>, <__main__.Case object at 0x794a0f12ea40>, <__main__.Case object at 0x794a0f12e4d0>, <__main__.Case object at 0x794a0f12fe20>, <__main__.Case object at 0x794a0f12e080>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f1340d0>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f135c30>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f1366b0>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f1346d0>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f1358a0>, <__main__.Case object at 0x794a0f136e30>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f134610>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f135900>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f135600>, <__main__.Case object at 0x794a0f136470>, <__main__.Case object at 0x794a0f1374c0>, <__main__.Case object at 0x794a0f1375b0>, <__main__.Case object at 0x794a0f137640>, <__main__.Case object at 0x794a0f1377f0>, <__main__.Case object at 0x794a0f1378e0>, <__main__.Case object at 0x794a0f136ad0>, <__main__.Case object at 0x794a0f136fe0>, <__main__.Case object at 0x794a0f135f60>, <__main__.Case object at 0x794a0f1364a0>, <__main__.Case object at 0x794a0f134670>, <__main__.Case object at 0x794a0f137400>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f136950>, <__main__.Case object at 0x794a0f137a90>, <__main__.Case object at 0x794a0f137bb0>, <__main__.Case object at 0x794a0f137cd0>, <__main__.Case object at 0x794a0f137df0>, <__main__.Case object at 0x794a0f14c070>, <__main__.Case object at 0x794a0f14c100>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f10e0b0>, <__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f12f610>, <__main__.Case object at 0x794a0f12ef20>, <__main__.Case object at 0x794a0f12dd20>, <__main__.Case object at 0x794a0f12fd60>, <__main__.Case object at 0x794a0f12dc90>, <__main__.Case object at 0x794a0f12e0e0>, <__main__.Case object at 0x794a0f12e830>, <__main__.Case object at 0x794a0f12faf0>, <__main__.Case object at 0x794a0f12f100>, <__main__.Case object at 0x794a0f12cc40>, <__main__.Case object at 0x794a0f12ea70>, <__main__.Case object at 0x794a0f12e710>, <__main__.Case object at 0x794a0f12fd00>, <__main__.Case object at 0x794a0f12f130>, <__main__.Case object at 0x794a0f12ee30>, <__main__.Case object at 0x794a0f12f820>, <__main__.Case object at 0x794a0f12ea10>, <__main__.Case object at 0x794a0f12e110>, <__main__.Case object at 0x794a0f136da0>, <__main__.Case object at 0x794a0f135b10>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f135a20>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f135660>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f134910>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f1347f0>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f136f50>, <__main__.Case object at 0x794a0f1376a0>, <__main__.Case object at 0x794a0f137760>, <__main__.Case object at 0x794a0f137820>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f136140>, <__main__.Case object at 0x794a0f134bb0>, <__main__.Case object at 0x794a0f134e50>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f137340>, <__main__.Case object at 0x794a0f134430>, <__main__.Case object at 0x794a0f1372b0>, <__main__.Case object at 0x794a0f137b20>, <__main__.Case object at 0x794a0f137c40>, <__main__.Case object at 0x794a0f137d60>, <__main__.Case object at 0x794a0f137e80>, <__main__.Case object at 0x794a0f137fa0>, <__main__.Case object at 0x794a0f137f10>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.8, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 3, tv: 0.39999999999999997, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 2), solution: 3, tv: 0.39999999999999997, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.49999999999999994, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.49999999999999994, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.49999999999999994, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.39999999999999986, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.39999999999999986, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.39999999999999986, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.5999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.3, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 0.5)\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 2, 0.5)\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 1, 0.5)\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 1, 0.5)\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 3, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 3) is empty. Temporary case base stored to the case base: ((0, 3), 1, 0.5)\n",
      "Episode succeeded, case (0, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 3, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 3, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.6)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.9)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.8, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.49999999999999994, time steps: 4\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.49999999999999994, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.49999999999999994, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.6, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 67\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 64\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 61\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 60\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 58\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 57\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 56\n",
      "cases content after RETAIN, problem: (1, 1), solution: 1, tv: 0.5, time steps: 55\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (0, 2), solution: 1, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (1, 2), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (0, 3), solution: 1, tv: 0.5, time steps: 46\n",
      "cases content after RETAIN, problem: (2, 1), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 9\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f12e500>, <__main__.Case object at 0x794a0f12f640>, <__main__.Case object at 0x794a0f12f700>, <__main__.Case object at 0x794a0f12e470>, <__main__.Case object at 0x794a0f12f6a0>, <__main__.Case object at 0x794a0f12fb20>, <__main__.Case object at 0x794a0f12e9e0>, <__main__.Case object at 0x794a0f12f370>, <__main__.Case object at 0x794a0f12da80>, <__main__.Case object at 0x794a0f12ffa0>, <__main__.Case object at 0x794a0f12f250>, <__main__.Case object at 0x794a0f12de70>, <__main__.Case object at 0x794a0f12f040>, <__main__.Case object at 0x794a0f12e290>, <__main__.Case object at 0x794a0f12edd0>, <__main__.Case object at 0x794a0f12ed40>, <__main__.Case object at 0x794a0f12f2b0>, <__main__.Case object at 0x794a0f12ec80>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f1364d0>, <__main__.Case object at 0x794a0f136a40>, <__main__.Case object at 0x794a0f134760>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f136ec0>, <__main__.Case object at 0x794a0f135720>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f134e20>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f137160>, <__main__.Case object at 0x794a0f136e60>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f136bc0>, <__main__.Case object at 0x794a0f135a50>, <__main__.Case object at 0x794a0f135ab0>, <__main__.Case object at 0x794a0f1359c0>, <__main__.Case object at 0x794a0f134ca0>, <__main__.Case object at 0x794a0f1348e0>, <__main__.Case object at 0x794a0f1356f0>, <__main__.Case object at 0x794a0f134520>, <__main__.Case object at 0x794a0f134df0>, <__main__.Case object at 0x794a0f1345e0>, <__main__.Case object at 0x794a0f136e90>, <__main__.Case object at 0x794a0f137550>, <__main__.Case object at 0x794a0f137520>, <__main__.Case object at 0x794a0f137730>, <__main__.Case object at 0x794a0f1378b0>, <__main__.Case object at 0x794a0f137a30>, <__main__.Case object at 0x794a0f136c20>, <__main__.Case object at 0x794a0f136290>, <__main__.Case object at 0x794a0f134970>, <__main__.Case object at 0x794a0f136740>, <__main__.Case object at 0x794a0f137490>, <__main__.Case object at 0x794a0f1373a0>, <__main__.Case object at 0x794a0f137250>, <__main__.Case object at 0x794a0f137190>, <__main__.Case object at 0x794a0f137af0>, <__main__.Case object at 0x794a0f137c10>, <__main__.Case object at 0x794a0f137d30>, <__main__.Case object at 0x794a0f137e50>, <__main__.Case object at 0x794a0f137f70>, <__main__.Case object at 0x794a0f14c0d0>, <__main__.Case object at 0x794a0f14c1f0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.8999999999999998, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.7, time steps: 8\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8999999999999998, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 0\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.7, time steps: 8\n",
      "Episode: 43, Total Steps: 70, Total Rewards: [31, 91], Status Episode: True\n",
      "------------------------------------------End of episode 43 loop--------------------\n",
      "----- starting point of Episode 44 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.7999999999999999, 0)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999999, 0)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.5, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.7999999999999999, 0)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 3, 0.7999999999999999, 1)]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999999, 1)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.5, 9)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((1, 0), 4, 0.5, 56)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 56)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 3, 0.7999999999999999, 1)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.6, 17)]\n",
      "comm next state for agent 0: ((7, 0), 2, 0.6, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['obstacle', 'empty', 'empty']], ((1, 0), 4, 0.5, 56)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 0), 2, 0.6, 17)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 1.0, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 57)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 57)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 1.0, 13)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.8999999999999998, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.8999999999999998, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 57)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.5, 58)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.5, 58)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.8999999999999998, 6)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.5, 58)]\n",
      "next state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.5, 59)]\n",
      "comm next state for agent 1: ((3, 1), 2, 0.5, 59)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.5, 59)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.5, 60)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.5, 60)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999998, 13)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.8999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.5, 60)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 61)]\n",
      "comm next state for agent 1: ((4, 2), 4, 0.5, 61)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.8999999999999998, 13)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 61)]\n",
      "next state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 65)]\n",
      "comm next state for agent 1: ((5, 2), 4, 0.5, 65)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 5], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 65)]\n",
      "next state for agent 1: [[8, 6], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 2), 2, 0.5, 66)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 66)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 6], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((6, 2), 2, 0.5, 66)]\n",
      "next state for agent 1: [[9, 6], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'obstacle', None]], ((6, 3), 2, 0.5, 67)]\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 67)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 6], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'obstacle', None]], ((6, 3), 2, 0.5, 67)]\n",
      "next state for agent 1: [[9, 6], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'obstacle', None]], ((6, 4), 2, 0.5, 68)]\n",
      "comm next state for agent 1: ((6, 4), 2, 0.5, 68)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 6], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'obstacle', None]], ((6, 4), 2, 0.5, 68)]\n",
      "next state for agent 1: [[9, 5], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((6, 5), 3, 0.6, 8)]\n",
      "comm next state for agent 1: ((6, 5), 3, 0.6, 8)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 5], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((6, 5), 3, 0.6, 8)]\n",
      "next state for agent 1: [[9, 6], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'obstacle', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 6], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'obstacle', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[8, 6], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 6], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[8, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 7], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[8, 8], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 8], False, [['empty', 'agent', 'obstacle'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 8], False, [['empty', 'obstacle', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 8], False, [['empty', 'obstacle', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 8], False, [['empty', 'obstacle', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 8], False, [['empty', 'obstacle', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 9], False, [['empty', 'agent', None], ['empty', 'empty', None], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 9], False, [['empty', 'agent', None], ['empty', 'empty', None], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[8, 9], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 9], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[8, 9], False, [['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 9], False, [['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['agent', 'empty', None], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 9], False, [['empty', 'empty', None], ['agent', 'empty', None], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[8, 9], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 9], False, [['obstacle', 'empty', 'empty'], ['empty', 'empty', 'agent'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[7, 9], False, [['empty', 'obstacle', 'empty'], ['obstacle', 'empty', 'agent'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 44 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 hit an obstacle! Next state: [312.5, 462.5, 337.5, 487.5]\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 9], False, [['empty', 'obstacle', 'empty'], ['obstacle', 'empty', 'agent'], [None, None, None]], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 1: [[6, 9], False, [['empty', 'empty', 'obstacle'], ['empty', 'obstacle', 'agent'], [None, None, None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f135ae0>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f135a20>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f134a00>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f136200>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f137760>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f1344f0>, <__main__.Case object at 0x794a0f137ca0>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f136110>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f1366b0>, <__main__.Case object at 0x794a0f1355a0>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f136bf0>, <__main__.Case object at 0x794a0f137220>, <__main__.Case object at 0x794a0f134eb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f134fd0>, <__main__.Case object at 0x794a0f136c80>, <__main__.Case object at 0x794a0f134d60>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6000000000000001, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 2, tv: 0.29999999999999993, time steps: 4\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.29999999999999993, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.29999999999999993, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 2, tv: 0.39999999999999986, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 0.7, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6, time steps: 68\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 67\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 66\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.3, time steps: 64\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.6, time steps: 61\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.6, time steps: 60\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.6, time steps: 59\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.6, time steps: 58\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 57\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 56\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 1, tv: 0.3, time steps: 55\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.3, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 1, tv: 0.3, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 3, tv: 0.3, time steps: 50\n",
      "case content after REVISE for agent 0, problem: (0, 3), solution: 1, tv: 0.3, time steps: 46\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 3, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 3, tv: 0.3, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 2), 4, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 2), 4, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 2), 4, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 1), 2, 0.5, 27)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 2, 0.5, 27)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 27)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 0), 4, 0.5, 27)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.8999999999999998)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 0.8999999999999998)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.8999999999999998)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1.0)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 2, 0.6)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6000000000000001, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.7, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8999999999999998, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 1\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f137700>, <__main__.Case object at 0x794a0f1379d0>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f134430>, <__main__.Case object at 0x794a0f137c40>, <__main__.Case object at 0x794a0f137fa0>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f136560>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f135600>, <__main__.Case object at 0x794a0f135d50>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f136770>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f136da0>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f136050>, <__main__.Case object at 0x794a0f1352a0>, <__main__.Case object at 0x794a0f1368f0>, <__main__.Case object at 0x794a0f134490>, <__main__.Case object at 0x794a0f137940>, <__main__.Case object at 0x794a0f135e70>, <__main__.Case object at 0x794a0f137340>, <__main__.Case object at 0x794a0f137b20>, <__main__.Case object at 0x794a0f137e80>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f136d40>, <__main__.Case object at 0x794a0f1352d0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f1375b0>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.4999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.4999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.4999999999999998, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.3999999999999999, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.3999999999999999, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.7, time steps: 8\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.6)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.4999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.4999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.4999999999999998, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.7, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 68\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 67\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 66\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 61\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 60\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 58\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 57\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 56\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 9\n",
      "Episode: 44, Total Steps: 27, Total Rewards: [87, -126], Status Episode: False\n",
      "------------------------------------------End of episode 44 loop--------------------\n",
      "----- starting point of Episode 45 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 4, 0.6, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 4, 0.6, 9)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 0), 4, 0.5, 27)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((1, 0), 4, 0.5, 27)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 27)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 27)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], ((3, 0), 2, 0.5, 27)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['agent', 'empty', None], ['empty', 'empty', None]], ((3, 0), 2, 0.5, 27)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((3, 1), 2, 0.5, 27)]\n",
      "comm next state for agent 1: ((3, 1), 2, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], ((3, 1), 2, 0.5, 27)]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], ((3, 2), 4, 0.5, 27)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'empty', None], ['empty', 'agent', None]], ((3, 2), 4, 0.5, 27)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 27)]\n",
      "comm next state for agent 1: ((4, 2), 4, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 27)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 27)]\n",
      "comm next state for agent 1: ((5, 2), 4, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 27)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 27)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 27)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((6, 3), 2, 0.5, 27)]\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((6, 3), 2, 0.5, 27)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 4), 2, 0.5, 27)]\n",
      "comm next state for agent 1: ((6, 4), 2, 0.5, 27)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 4), 2, 0.5, 27)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 5), 3, 0.7, 8)]\n",
      "comm next state for agent 1: ((6, 5), 3, 0.7, 8)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 45 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 5), 3, 0.7, 8)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 13)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.9, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f137e80>, <__main__.Case object at 0x794a0f136d40>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f135a20>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f136e30>, <__main__.Case object at 0x794a0f1349d0>, <__main__.Case object at 0x794a0f135c60>, <__main__.Case object at 0x794a0f134040>]\n",
      "agent0 comm temp case base: []\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.4000000000000001, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 0.7999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6999999999999997, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6999999999999997, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6999999999999997, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 1\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 2), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 2), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 2), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 1), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 0), 4, 0.5, 18)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.7999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6999999999999997, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6999999999999997, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6999999999999997, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f137ee0>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f1358a0>, <__main__.Case object at 0x794a0f1344c0>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f135de0>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f136140>, <__main__.Case object at 0x794a0f137dc0>, <__main__.Case object at 0x794a0f137a60>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f134220>, <__main__.Case object at 0x794a0f136230>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f137b80>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f136380>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f137820>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f1355a0>, <__main__.Case object at 0x794a0f137220>, <__main__.Case object at 0x794a0f135ba0>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.5999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.5999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.5999999999999998, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.7999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.3, time steps: 68\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.3, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.3, time steps: 66\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.3, time steps: 65\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.3, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.3, time steps: 60\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.3, time steps: 59\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.3, time steps: 58\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.3, time steps: 57\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.3, time steps: 56\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.3, time steps: 9\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.7)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (4, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.6)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5999999999999998, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.7999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 6\n",
      "Episode: 45, Total Steps: 18, Total Rewards: [89, 83], Status Episode: True\n",
      "------------------------------------------End of episode 45 loop--------------------\n",
      "----- starting point of Episode 46 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.5, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.7, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.5, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.5, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.5, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.7, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.5, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.5, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.5, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.5, 18)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.5, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.7, 13)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 18)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.7, 13)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.5999999999999998, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.5999999999999998, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.5, 18)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.5999999999999998, 6)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.5999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.5999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.5999999999999998, 13)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5999999999999998, 13)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.5999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.5, 18)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((4, 2), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5999999999999998, 13)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 18)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 18)]\n",
      "comm next state for agent 1: ((5, 2), 4, 0.5, 18)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 46 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 0.7999999999999999, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 0.7999999999999999, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 0.7999999999999999, 8)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 13)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 18)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 18)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f125330>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f1344f0>, <__main__.Case object at 0x794a0f137610>, <__main__.Case object at 0x794a0f137ee0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f136e30>, <__main__.Case object at 0x794a0f1375b0>, <__main__.Case object at 0x794a0f135060>]\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 0.8999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.4999999999999997, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.4999999999999997, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.4999999999999997, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.39999999999999986, time steps: 1\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 2, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 2), 4, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 2), 4, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 2), 4, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 1), 2, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 2, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 0), 4, 0.5, 12)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.5999999999999998)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5999999999999998)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.5999999999999998)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.7)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.8999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.4999999999999997, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.4999999999999997, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.4999999999999997, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f137b80>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f137ca0>, <__main__.Case object at 0x794a0f137220>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f1353c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f137b20>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f137ac0>, <__main__.Case object at 0x794a0f1352d0>, <__main__.Case object at 0x794a0f134340>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f137a60>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6999999999999997, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6999999999999997, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.6999999999999997, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.8999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.3, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 6\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 5), 4, 0.5, 12)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 3, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 4), 2, 0.5, 12)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 3), 2, 0.5, 12)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 2, 0.5, 12)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.7)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6999999999999997, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.8999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "Episode: 46, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 46 loop--------------------\n",
      "----- starting point of Episode 47 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.6, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.7999999999999999, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.6, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.6, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.6, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.7999999999999999, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.5, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.6, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.6, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.6, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.5, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.6, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.5, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.5, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.5, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.5, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.5, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.5, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.5, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 0.5, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 47 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.6999999999999997, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.6999999999999997, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.5, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.5, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.6999999999999997, 6)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.5, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.5, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.5, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.5, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.5, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.5, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.5, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.5, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.5, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.5, 12)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.5, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 3), 2, 0.5, 12)]\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 47 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'agent', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 3), 2, 0.5, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 3), 2, 0.5, 12)]\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.5, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.5, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 3), 2, 0.5, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 3), 2, 0.5, 12)]\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f1349d0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f137820>, <__main__.Case object at 0x794a0f135a20>, <__main__.Case object at 0x794a0f136f50>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f1352a0>, <__main__.Case object at 0x794a0f136050>, <__main__.Case object at 0x794a0f1376a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f137610>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f1366e0>, <__main__.Case object at 0x794a0f134bb0>]\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 0.9999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.2999999999999997, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.2999999999999997, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.2999999999999997, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.49999999999999994, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.3, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.3, time steps: 6\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.5)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.5)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.6999999999999997)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.5)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6)\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.9999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.49999999999999994, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f135a80>, <__main__.Case object at 0x794a0f135060>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f1355a0>, <__main__.Case object at 0x794a0f1344f0>, <__main__.Case object at 0x794a0f137b80>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f134d90>, <__main__.Case object at 0x794a0f135e70>, <__main__.Case object at 0x794a0f137700>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f1358a0>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f137340>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f134490>, <__main__.Case object at 0x794a0f1341c0>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.7999999999999997, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 0.9999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.49999999999999994, time steps: 9\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (4, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.5)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999997, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 0.9999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.49999999999999994, time steps: 9\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 12\n",
      "Episode: 47, Total Steps: 12, Total Rewards: [89, 91], Status Episode: True\n",
      "------------------------------------------End of episode 47 loop--------------------\n",
      "----- starting point of Episode 48 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.7, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.8999999999999999, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.7, 6)]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 0.8999999999999999, 9)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 48 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.7, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.7, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.6, 12)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.6, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 48 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.7, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.7, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.6, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.6, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.7, 11)]\n",
      "next state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.6, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 48 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.6, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.6, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.6, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.7999999999999997, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.7999999999999997, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.6, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.7999999999999997, 6)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.6, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.6, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.6, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.6, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.6, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.6, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.6, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.6, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.6, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.6, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.6, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.6, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.6, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.6, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.6, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 0.9999999999999999, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 0.9999999999999999, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.6, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.6, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 0.9999999999999999, 8)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.6, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.6, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.6, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.6, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.6, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.6, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.6, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.6, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.6, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.6, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f136e30>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f1368f0>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f135ae0>, <__main__.Case object at 0x794a0f135660>, <__main__.Case object at 0x794a0f1375b0>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f137940>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f134940>, <__main__.Case object at 0x794a0f135bd0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f137820>, <__main__.Case object at 0x794a0f1352a0>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f137f10>]\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.4000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.29999999999999993, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.3, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.39999999999999997, time steps: 8\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.6)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.6)\n",
      "Integrated case process. comm case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.6)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 0.6)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.7999999999999997)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.6)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.7)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.7)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.7)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.5, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.7999999999999997, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f136ce0>, <__main__.Case object at 0x794a0f137610>, <__main__.Case object at 0x794a0f136aa0>, <__main__.Case object at 0x794a0f134c70>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f136d40>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f135f90>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f134b20>, <__main__.Case object at 0x794a0f137220>, <__main__.Case object at 0x794a0f136980>, <__main__.Case object at 0x794a0f1355a0>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f134220>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.8999999999999997, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.7999999999999999, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.29999999999999993, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.3, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.3, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.6)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.6)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.6)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.6)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.6)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.6)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8999999999999997, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 12\n",
      "Episode: 48, Total Steps: 15, Total Rewards: [86, 89], Status Episode: True\n",
      "------------------------------------------End of episode 48 loop--------------------\n",
      "----- starting point of Episode 49 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 4, 0.9999999999999999, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 0.9999999999999999, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 49 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.7999999999999999, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], ((0, 0), 4, 0.9999999999999999, 9)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.7999999999999999, 6)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.7999999999999999, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.7999999999999999, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.7, 12)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.7999999999999999, 8)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.7999999999999999, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.7999999999999999, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.7, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.7999999999999999, 11)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 1), 2, 0.7, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.7, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((7, 1), 2, 0.7, 12)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.8999999999999997, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.8999999999999997, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 0.8999999999999997, 6)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 3), 2, 0.7, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.7, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 3), 2, 0.7, 12)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 0.7, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.7, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 4), 2, 0.7, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 0.7, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[3, 2], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'obstacle', 'empty']], ((7, 5), 3, 0.7, 12)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "next state for agent 0: [[5, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'obstacle', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['obstacle', 'obstacle', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 49 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f137340>, <__main__.Case object at 0x794a0f136380>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f135e70>, <__main__.Case object at 0x794a0f135210>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f1347f0>, <__main__.Case object at 0x794a0f136890>, <__main__.Case object at 0x794a0f136320>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a18052c50>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f136530>, <__main__.Case object at 0x794a0f1368f0>, <__main__.Case object at 0x794a0f136230>, <__main__.Case object at 0x794a0f134940>, <__main__.Case object at 0x794a0f136aa0>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f136d40>, <__main__.Case object at 0x794a0f136da0>, <__main__.Case object at 0x794a0f1379d0>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f137c40>, <__main__.Case object at 0x794a0f134be0>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.3, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.5999999999999996, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.49999999999999994, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.49999999999999994, time steps: 6\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 4, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.7)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.7)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.7)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.8999999999999997)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.7)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5999999999999996, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.49999999999999994, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.49999999999999994, time steps: 6\n",
      "cases content after RETAIN, problem: (5, 3), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.7999999999999999, time steps: 8\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f1372e0>, <__main__.Case object at 0x794a0f1366b0>, <__main__.Case object at 0x794a0f1366e0>, <__main__.Case object at 0x794a0f1375b0>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f137610>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f134a00>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f134f10>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f1349d0>, <__main__.Case object at 0x794a0f137760>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f136140>, <__main__.Case object at 0x794a0f137ac0>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f137d60>, <__main__.Case object at 0x794a0f134430>, <__main__.Case object at 0x794a0f137040>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.9999999999999997, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.8999999999999999, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.8999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.39999999999999997, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.39999999999999997, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.7)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.7)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.7)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.7)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.9999999999999997, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.8999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.7, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "Episode: 49, Total Steps: 18, Total Rewards: [83, 91], Status Episode: True\n",
      "------------------------------------------End of episode 49 loop--------------------\n",
      "----- starting point of Episode 50 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.8999999999999999, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.8999999999999999, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.8999999999999999, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.8999999999999999, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.8999999999999999, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.8999999999999999, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.8999999999999999, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 0.8999999999999999, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.9999999999999997, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 0.9999999999999997, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 0.9999999999999997, 6)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.7999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 0.7999999999999999, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.7999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.7999999999999999, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.7999999999999999, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f134d00>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f134850>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f136560>, <__main__.Case object at 0x794a0f135660>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f135960>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f137940>, <__main__.Case object at 0x794a0f135b10>, <__main__.Case object at 0x794a0f137fa0>, <__main__.Case object at 0x794a0f136f50>, <__main__.Case object at 0x794a0f1373d0>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f136800>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f135210>, <__main__.Case object at 0x794a0f136320>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.39999999999999963, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.29999999999999993, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.29999999999999993, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 4, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.49999999999999994, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.49999999999999994, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.49999999999999994, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.5999999999999999, time steps: 8\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 0.9999999999999997)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.49999999999999994, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.49999999999999994, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.49999999999999994, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f134fd0>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f135cc0>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f136380>, <__main__.Case object at 0x794a0f135e70>, <__main__.Case object at 0x794a0f136890>, <__main__.Case object at 0x794a0f1366b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f1368f0>, <__main__.Case object at 0x794a0f136d40>, <__main__.Case object at 0x794a0f136770>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f135180>, <__main__.Case object at 0x794a0f134d60>, <__main__.Case object at 0x794a0f136200>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f134b20>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.9999999999999999, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.9999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.49999999999999994, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.7999999999999998, time steps: 9\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.9999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.9999999999999999, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.49999999999999994, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7999999999999998, time steps: 9\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 12\n",
      "Episode: 50, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 50 loop--------------------\n",
      "----- starting point of Episode 51 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.9999999999999999, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9999999999999999, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 0.9999999999999999, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.9999999999999999, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.9999999999999999, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.9999999999999999, 8)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 51 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.9999999999999999, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 0.9999999999999999, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['empty', 'agent', 'empty']], 0]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 0.9999999999999999, 8)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.9999999999999999, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 0.9999999999999999, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((8, 1), 3, 0.9999999999999999, 11)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 0.8999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 0), 2, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 1), 2, 0.8999999999999999, 12)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.8999999999999999, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.8999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 0.8999999999999999, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.8999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 2), 4, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.8999999999999999, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 0.8999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 5), 3, 0.8999999999999999, 12)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'agent', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 0.8999999999999999, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'agent', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f1345b0>, <__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135ba0>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f1379d0>, <__main__.Case object at 0x794a0f134430>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f1348b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a18069900>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f134f10>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f137f10>, <__main__.Case object at 0x794a0f137340>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f135e70>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f137160>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.29999999999999993, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.29999999999999993, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.29999999999999993, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.39999999999999986, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.9999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f135b10>, <__main__.Case object at 0x794a0f137fa0>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f134f70>, <__main__.Case object at 0x794a0f137a60>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f1372e0>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f136380>, <__main__.Case object at 0x794a0f136bc0>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f135330>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f135930>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.29999999999999993, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.5999999999999999, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5999999999999999, time steps: 12\n",
      "Episode: 51, Total Steps: 13, Total Rewards: [88, 90], Status Episode: True\n",
      "------------------------------------------End of episode 51 loop--------------------\n",
      "----- starting point of Episode 52 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 1), 2, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 4), 2, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((7, 5), 3, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'obstacle', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 0], False, [[None, None, None], ['empty', 'agent', 'empty'], ['empty', 'empty', 'obstacle']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 52 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9999999999999999, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9999999999999999, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f135d80>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f134fd0>, <__main__.Case object at 0x794a0f136e60>, <__main__.Case object at 0x794a0f137fa0>, <__main__.Case object at 0x794a0f137a60>, <__main__.Case object at 0x794a0f135510>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134a00>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f1372b0>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f137850>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f136890>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f136560>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f135120>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f134430>, <__main__.Case object at 0x794a0f1366b0>, <__main__.Case object at 0x794a0f137010>, <__main__.Case object at 0x794a0f135660>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f135090>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f137e80>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f134a90>, <__main__.Case object at 0x794a0f134c70>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.7999999999999998, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 6\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 3, 0.5)\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 3, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 3, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.7999999999999998, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.9999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f135cc0>, <__main__.Case object at 0x794a0f1340a0>, <__main__.Case object at 0x794a0f134cd0>, <__main__.Case object at 0x794a0f135930>, <__main__.Case object at 0x794a0f135210>, <__main__.Case object at 0x794a0f136830>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f136a10>, <__main__.Case object at 0x794a0f136320>, <__main__.Case object at 0x794a0f136920>, <__main__.Case object at 0x794a0f136380>, <__main__.Case object at 0x794a0f1370d0>, <__main__.Case object at 0x794a0f136d70>, <__main__.Case object at 0x794a0f134c10>, <__main__.Case object at 0x794a0f1376a0>, <__main__.Case object at 0x794a0f134f40>, <__main__.Case object at 0x794a0f1352d0>, <__main__.Case object at 0x794a0f135300>, <__main__.Case object at 0x794a0f135570>, <__main__.Case object at 0x794a0f136aa0>, <__main__.Case object at 0x794a0f135810>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f136dd0>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.39999999999999986, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.39999999999999986, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "Episode: 52, Total Steps: 23, Total Rewards: [78, 92], Status Episode: True\n",
      "------------------------------------------End of episode 52 loop--------------------\n",
      "----- starting point of Episode 53 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 1, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 1, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f127970>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f135660>, <__main__.Case object at 0x794a0f137070>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f136f20>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f134be0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f134fa0>, <__main__.Case object at 0x794a0f135330>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f134fd0>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f1372b0>, <__main__.Case object at 0x794a0f137340>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.5999999999999999, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 3, tv: 0.3, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.3, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.3, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.7999999999999998, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.7999999999999998, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.7999999999999998, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.8, time steps: 8\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5999999999999999, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.7999999999999998, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999998, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.7999999999999998, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f135a50>, <__main__.Case object at 0x794a0f137e80>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f134e20>, <__main__.Case object at 0x794a0f135960>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f135ed0>, <__main__.Case object at 0x794a0f134430>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f137b80>, <__main__.Case object at 0x794a0f135000>, <__main__.Case object at 0x794a0f134190>, <__main__.Case object at 0x794a0f134eb0>, <__main__.Case object at 0x794a0f1375b0>, <__main__.Case object at 0x794a0f137b20>, <__main__.Case object at 0x794a0f1357e0>, <__main__.Case object at 0x794a0f134cd0>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "Episode: 53, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 53 loop--------------------\n",
      "----- starting point of Episode 54 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((2, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['agent', 'empty', None], ['empty', 'empty', None]], ((2, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((3, 0), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'agent', None], ['empty', 'empty', None]], ((3, 0), 2, 1, 12)]\n",
      "next state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[3, 1], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 2], False, [['empty', 'agent', None], ['empty', 'empty', None], ['empty', 'empty', None]], 0]\n",
      "next state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((3, 1), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 1], False, [['empty', 'empty', None], ['empty', 'empty', None], ['empty', 'agent', None]], ((3, 1), 2, 1, 12)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'agent'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'agent'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 0], False, [[None, None, None], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 54 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 3), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 3), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 4), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 1: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999998, 12)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7999999999999998, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['agent', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999998, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.7999999999999998, 12)]\n",
      "comm next state for agent 1: ((5, 5), 4, 0.7999999999999998, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f135210>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f134610>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f137850>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f135cc0>, <__main__.Case object at 0x794a0f137ee0>, <__main__.Case object at 0x794a0f136080>, <__main__.Case object at 0x794a0f135570>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807bd90>, <__main__.Case object at 0x794a0f1250f0>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f1346d0>, <__main__.Case object at 0x794a0f1367d0>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f1362c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.4000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.39999999999999986, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.39999999999999986, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.8999999999999998, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.8, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8999999999999998, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.5999999999999999, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8, time steps: 12\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f1372b0>, <__main__.Case object at 0x794a0f137b20>, <__main__.Case object at 0x794a0f1375b0>, <__main__.Case object at 0x794a0f135660>, <__main__.Case object at 0x794a0f1361a0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f135870>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f134130>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f135270>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f136d70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a180698d0>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f135db0>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f1348b0>, <__main__.Case object at 0x794a0f134310>, <__main__.Case object at 0x794a0f136ec0>, <__main__.Case object at 0x794a0f135300>, <__main__.Case object at 0x794a0f134c10>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 2, 0.5)\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (4, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 0), solution: 2, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 6\n",
      "cases content after RETAIN, problem: (9, 2), solution: 1, tv: 0.5, time steps: 5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "Episode: 54, Total Steps: 16, Total Rewards: [88, 85], Status Episode: True\n",
      "------------------------------------------End of episode 54 loop--------------------\n",
      "----- starting point of Episode 55 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.8, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.8, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.8, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.8, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f136d70>, <__main__.Case object at 0x794a0f1366e0>, <__main__.Case object at 0x794a0f1367d0>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f135ea0>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f1375b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a180524d0>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f134e20>, <__main__.Case object at 0x794a0f137100>, <__main__.Case object at 0x794a0f134580>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f134610>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f136080>, <__main__.Case object at 0x794a0f137340>, <__main__.Case object at 0x794a0f137010>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.6999999999999997, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.39999999999999986, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.4000000000000001, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.8)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.6999999999999997, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f135bd0>, <__main__.Case object at 0x794a0f136aa0>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f135210>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f137ee0>, <__main__.Case object at 0x794a0f134fd0>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f137be0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134a00>, <__main__.Case object at 0x794a0f1352d0>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f136b90>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f135150>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.9, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 2, tv: 0.3, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.3, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 1, tv: 0.3, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.8, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (4, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.9, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8, time steps: 12\n",
      "Episode: 55, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 55 loop--------------------\n",
      "----- starting point of Episode 56 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 5), 4, 0.9, 12)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.9, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 0.9, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 0.9, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 0.9, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f134c10>, <__main__.Case object at 0x794a0f136170>, <__main__.Case object at 0x794a0f1362f0>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f135720>, <__main__.Case object at 0x794a0f135750>, <__main__.Case object at 0x794a0f136500>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f136aa0>, <__main__.Case object at 0x794a0f135210>, <__main__.Case object at 0x794a0f137ee0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a18069960>, <__main__.Case object at 0x794a0f135f90>, <__main__.Case object at 0x794a0f137340>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f136d70>, <__main__.Case object at 0x794a0f135d20>, <__main__.Case object at 0x794a0f134040>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f134940>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f137b20>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.4999999999999997, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.8, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.8, time steps: 6\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 0.9)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.4999999999999997, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.8, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f135810>, <__main__.Case object at 0x794a0f136080>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f1359f0>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f134fd0>, <__main__.Case object at 0x794a0f1354e0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f10e050>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f135b70>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f134d00>, <__main__.Case object at 0x794a0f1372b0>, <__main__.Case object at 0x794a0f1358d0>, <__main__.Case object at 0x794a0f137850>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1.0, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (4, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1.0, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "Episode: 56, Total Steps: 13, Total Rewards: [88, 92], Status Episode: True\n",
      "------------------------------------------End of episode 56 loop--------------------\n",
      "----- starting point of Episode 57 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "comm next state for agent 0: ((7, 2), 2, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], ((7, 2), 2, 1, 6)]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 3), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "next state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 3), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 4), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 4], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 4), 2, 1, 12)]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 5], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "Agent 1 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((7, 5), 3, 1, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 0: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 5], False, [['obstacle', 'empty', 'empty'], ['target', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 1.0, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((5, 5), 4, 1.0, 12)]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "Agent 1 is locked. Done status: True, win status: True\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 12)]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((5, 5), 4, 1.0, 12)]\n",
      "comm next state for agent 0: ((5, 5), 4, 1.0, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'agent'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e020>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f1366e0>, <__main__.Case object at 0x794a0f134400>, <__main__.Case object at 0x794a0f134d00>, <__main__.Case object at 0x794a0f1350f0>, <__main__.Case object at 0x794a0f136c50>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f135a50>, <__main__.Case object at 0x794a0f135bd0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a18052500>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f134940>, <__main__.Case object at 0x794a0f135570>, <__main__.Case object at 0x794a0f1368c0>, <__main__.Case object at 0x794a0f136dd0>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f1365c0>, <__main__.Case object at 0x794a0f136ef0>, <__main__.Case object at 0x794a0f134fd0>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.2999999999999997, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.6000000000000001, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.8, time steps: 8\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (5, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 5), 4, 1.0)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 1)\n",
      "Integrated case process. comm case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 1)\n",
      "Integrated case process. comm case (7, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6000000000000001, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.8, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a18069b40>, <__main__.Case object at 0x794a0f1353c0>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f136ec0>, <__main__.Case object at 0x794a0f1355d0>, <__main__.Case object at 0x794a0f134ee0>, <__main__.Case object at 0x794a0f1367d0>, <__main__.Case object at 0x794a0f137fa0>, <__main__.Case object at 0x794a0f136080>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f134d90>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a1807a980>, <__main__.Case object at 0x794a0f134610>, <__main__.Case object at 0x794a0f135f00>, <__main__.Case object at 0x794a0f1341c0>, <__main__.Case object at 0x794a0f1372b0>, <__main__.Case object at 0x794a0f134c10>, <__main__.Case object at 0x794a0f135720>, <__main__.Case object at 0x794a0f136aa0>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f137940>, <__main__.Case object at 0x794a0f134a60>, <__main__.Case object at 0x794a0f135180>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.4000000000000001, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.4000000000000001, time steps: 12\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 1, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "Episode: 57, Total Steps: 12, Total Rewards: [89, 92], Status Episode: True\n",
      "------------------------------------------End of episode 57 loop--------------------\n",
      "----- starting point of Episode 58 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[0, 0], False, [[None, None, None], [None, 'agent', 'empty'], [None, 'empty', 'empty']], []]\n",
      "next state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[9, 0], False, [[None, None, None], ['empty', 'agent', None], ['empty', 'empty', None]], []]\n",
      "next state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[1, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((9, 0), 3, 1, 6)]\n",
      "next state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "comm next state for agent 0: ((8, 0), 2, 1, 8)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 0], False, [[None, None, None], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((0, 0), 4, 1, 9)]\n",
      "next state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "state for agent 0: [[2, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 0), 2, 1, 8)]\n",
      "next state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "comm next state for agent 0: ((8, 1), 3, 1, 11)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[8, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((1, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 0], False, [[None, None, None], ['agent', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((8, 1), 3, 1, 11)]\n",
      "next state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "comm next state for agent 0: ((7, 1), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 1], False, [['empty', 'empty', 'empty'], ['obstacle', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((2, 0), 4, 1, 12)]\n",
      "next state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "state for agent 0: [[3, 1], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((7, 1), 2, 1, 12)]\n",
      "next state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[7, 2], False, [['obstacle', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['empty', 'empty', 'empty']], ((3, 0), 2, 1, 12)]\n",
      "next state for agent 1: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 58 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[3, 2], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['empty', 'obstacle', 'empty']], 0]\n",
      "next state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['empty', 'empty', 'agent'], ['empty', 'empty', 'empty']], ((3, 1), 2, 1, 12)]\n",
      "next state for agent 1: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[4, 2], False, [['empty', 'empty', 'empty'], ['agent', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((3, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((4, 2), 4, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 58 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "state for agent 0: [[5, 2], False, [['empty', 'empty', 'obstacle'], ['agent', 'empty', 'empty'], ['empty', 'empty', 'agent']], 0]\n",
      "next state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'empty', 'empty'], ['empty', 'agent', 'empty']], ((4, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "comm next state for agent 1: ((5, 2), 4, 1, 12)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "state for agent 0: [[6, 2], False, [['empty', 'obstacle', 'empty'], ['agent', 'agent', 'empty'], ['empty', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'empty', 'empty']], ((5, 2), 4, 1, 12)]\n",
      "next state for agent 1: [[6, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 58 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "state for agent 0: [[6, 3], False, [['empty', 'agent', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 3], False, [['empty', 'empty', 'empty'], ['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty']], ((6, 2), 2, 1, 12)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty']], ((6, 3), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 58 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "state for agent 0: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['target', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty']], ((6, 3), 2, 1, 12)]\n",
      "next state for agent 1: [[6, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((6, 4), 2, 1, 12)]\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 12)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 58 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "Agent 0 reached the target! Next state: [262.5, 262.5, 287.5, 287.5], Target: [262.5, 262.5, 287.5, 287.5]\n",
      "state for agent 0: [[6, 5], False, [['obstacle', 'agent', 'empty'], ['target', 'empty', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "state for agent 1: [[6, 3], False, [['empty', 'empty', 'empty'], ['empty', 'empty', 'empty'], ['obstacle', 'agent', 'empty']], ((6, 4), 2, 1, 12)]\n",
      "next state for agent 1: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "comm next state for agent 1: ((6, 5), 3, 1, 8)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "----- starting point of Episode 58 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "Agent 0 is locked. Done status: True, win status: True\n",
      "Agent 1 hit an obstacle! Next state: [262.5, 212.5, 287.5, 237.5]\n",
      "state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'empty'], ['empty', 'target', 'agent'], ['empty', 'empty', 'empty']], 0]\n",
      "next state for agent 0: [[5, 5], True, [['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty'], ['empty', 'empty', 'empty']], 0]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "state for agent 1: [[6, 4], False, [['empty', 'agent', 'empty'], ['obstacle', 'empty', 'empty'], ['agent', 'empty', 'empty']], ((6, 5), 3, 1, 8)]\n",
      "next state for agent 1: [[5, 4], False, [['empty', 'empty', 'empty'], ['obstacle', 'obstacle', 'agent'], ['empty', 'agent', 'empty']], 0]\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x794a0f10e080>, <__main__.Case object at 0x794a0f134790>, <__main__.Case object at 0x794a0f135840>, <__main__.Case object at 0x794a0f135480>, <__main__.Case object at 0x794a0f137460>, <__main__.Case object at 0x794a0f137be0>, <__main__.Case object at 0x794a0f135150>, <__main__.Case object at 0x794a0f137130>, <__main__.Case object at 0x794a0f137850>, <__main__.Case object at 0x794a0f137ee0>, <__main__.Case object at 0x794a0f134be0>, <__main__.Case object at 0x794a0f136560>, <__main__.Case object at 0x794a0f135090>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x794a1807be20>, <__main__.Case object at 0x794a29440100>, <__main__.Case object at 0x794a0f1361d0>, <__main__.Case object at 0x794a0f134c40>, <__main__.Case object at 0x794a0f134d00>, <__main__.Case object at 0x794a0f136830>]\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 2, tv: 0.4000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 3, tv: 0.4000000000000001, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.4000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.8, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 3), solution: 2, tv: 0.8, time steps: 12\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 2, 1)\n",
      "Integrated case process. comm case (8, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6000000000000001, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6000000000000001, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.8, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.5, time steps: 12\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x794a0f134d90>, <__main__.Case object at 0x794a0f1343d0>, <__main__.Case object at 0x794a0f134fd0>, <__main__.Case object at 0x794a0f1354e0>, <__main__.Case object at 0x794a0f134070>, <__main__.Case object at 0x794a0f136710>, <__main__.Case object at 0x794a0f136f50>, <__main__.Case object at 0x794a0f134700>, <__main__.Case object at 0x794a0f137fa0>, <__main__.Case object at 0x794a0f137040>, <__main__.Case object at 0x794a0f137ac0>, <__main__.Case object at 0x794a0f1357b0>, <__main__.Case object at 0x794a0f134430>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x794a0f1252d0>, <__main__.Case object at 0x794a0f1362c0>, <__main__.Case object at 0x794a0f136860>, <__main__.Case object at 0x794a0f136d10>, <__main__.Case object at 0x794a0f135300>, <__main__.Case object at 0x794a0f135960>, <__main__.Case object at 0x794a0f1342e0>, <__main__.Case object at 0x794a0f136ec0>, <__main__.Case object at 0x794a0f134160>, <__main__.Case object at 0x794a0f1367d0>, <__main__.Case object at 0x794a0f1379a0>, <__main__.Case object at 0x794a0f135ed0>]\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 1)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (4, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 1), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 5), solution: 3, tv: 1, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 12\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 12\n",
      "Episode: 58, Total Steps: 13, Total Rewards: [89, -112], Status Episode: False\n",
      "------------------------------------------End of episode 58 loop--------------------\n",
      "Success rate: 49.152542372881356%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import psutil\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment_ma_sensory_vector_dynamic import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env, alpha, gamma, epsilon):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def choose_action(self, agent_idx, state):\n",
    "        state = tuple(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_action = self.q_tables[agent_idx][state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    def learn(self, agent_idx, state, action, reward, next_state, case_base=None):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "        current_q = self.q_tables[agent_idx][state][action]\n",
    "        max_next_q = max(self.q_tables[agent_idx][next_state])\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_tables[agent_idx][state][action] = new_q\n",
    "\n",
    "\n",
    "class Case:\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=0.5, total_time_steps=0):\n",
    "        self.problem = ast.literal_eval(problem) if isinstance(problem, str) else problem\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "        self.total_time_steps = total_time_steps  # New attribute for total time steps\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6\n",
    "        v = state1.size\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.1):\n",
    "        state = ast.literal_eval(state) if isinstance(state, str) else state\n",
    "        for case in case_base:\n",
    "            if state == case.problem: \n",
    "                return case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(agent_idx, c, own_temp_case_base, comm_temp_case_base, source='own'):\n",
    "        \"\"\"Reuse step for adding cases to temporary case bases.\"\"\"\n",
    "        if source == 'own':\n",
    "            own_temp_case_base.append(c)\n",
    "        elif source == 'comm':\n",
    "            comm_temp_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(agent_idx, case_base, temporary_case_base, successful_episodes, total_steps):\n",
    "        for case in case_base:\n",
    "            if any((case.problem, case.solution) == (temp_case.problem, temp_case.solution) for temp_case in temporary_case_base):\n",
    "                if successful_episodes:\n",
    "                    case.trust_value += 0.1\n",
    "                else:\n",
    "                    case.trust_value -= 0.4\n",
    "            else:\n",
    "                if successful_episodes:\n",
    "                    case.trust_value -= 0.2\n",
    "            \n",
    "            case.trust_value = max(0, min(case.trust_value, 1))\n",
    "            print(f\"case content after REVISE for agent {agent_idx}, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "    @staticmethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def retain(agent_idx, case_base, own_temp_case_base, comm_temp_case_base, successful_episodes, total_steps, threshold=0.49):\n",
    "        if successful_episodes:\n",
    "            for temp_case in reversed(own_temp_case_base):\n",
    "                state = tuple(np.atleast_1d(temp_case.problem))\n",
    "\n",
    "                existing_case = next((case for case in case_base if tuple(np.atleast_1d(case.problem)) == state), None)\n",
    "                \n",
    "                if existing_case is None:\n",
    "                    case_base.append(temp_case)\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} is empty. Temporary case base stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")\n",
    "                else:\n",
    "                    if total_steps < existing_case.total_time_steps:\n",
    "                        # Update the case in the case base if the new case has fewer total steps\n",
    "                        existing_case.solution = temp_case.solution\n",
    "                        existing_case.trust_value = max(0, temp_case.trust_value)\n",
    "                        existing_case.total_time_steps = total_steps\n",
    "                        print(f\"Episode succeeded, updated case base with fewer steps: {temp_case.problem, temp_case.solution, temp_case.trust_value, total_steps}\")\n",
    "                    else:\n",
    "                        print(f\"Episode succeeded, case {temp_case.problem} for agent {agent_idx} is not updated as it has more or equal steps.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Episode not succeeded, temporary case base from own experience is not stored to the case base\")\n",
    "\n",
    "        case_base_dict = {tuple(np.atleast_1d(case.problem)): case for case in case_base}\n",
    "\n",
    "        for temp_comm_case in reversed(comm_temp_case_base):\n",
    "            state_comm = tuple(np.atleast_1d(temp_comm_case.problem))\n",
    "            existing_case = case_base_dict.get(state_comm)\n",
    "\n",
    "            if existing_case is None:\n",
    "                case_base.append(temp_comm_case)\n",
    "                case_base_dict[state_comm] = temp_comm_case\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} is empty. Temporary case base stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "            else:\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} for agent {agent_idx} is not empty. Temporary case base that not stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "\n",
    "        # Remove cases with trust values below the threshold\n",
    "        case_base[:] = [case for case in case_base if case.trust_value >= threshold]\n",
    "\n",
    "        for case in case_base:\n",
    "            print(f\"cases content after RETAIN, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "        return case_base\n",
    "\n",
    "    \n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.render = render\n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.epsilon_min = epsilon_min  \n",
    "\n",
    "        self.problem_solvers = [ProblemSolver(num_actions, self.env, alpha, gamma, epsilon) for _ in range(self.env.num_agents)]\n",
    "        self.case_bases = [[] for _ in range(self.env.num_agents)]  # Individual case bases for each agent\n",
    "        self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for own experiences\n",
    "        self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for communication experiences\n",
    "        self.successful_episodes = [0] * self.env.num_agents\n",
    "        self.rewards_per_episode = [[] for _ in range(self.env.num_agents)]  \n",
    "        self.total_successful_episodes = 0 \n",
    "        self.action_type = [0] * self.env.num_agents\n",
    "\n",
    "    def run(self):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "        success_steps = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            states = self.env.reset()\n",
    "            episode_reward = [0] * self.env.num_agents\n",
    "            total_steps = 0 \n",
    "            self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            success_count = [0] * self.env.num_agents\n",
    "            dones = [False] * self.env.num_agents\n",
    "            win_states = [False] * self.env.num_agents\n",
    "            successful_episodes = False\n",
    "\n",
    "            while not(all(dones)):\n",
    "                print(f\"----- starting point of Episode {episode} in steps {total_steps} loop -----\")\n",
    "                \n",
    "                actions = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    # print(f\"state before take action: {state}\")\n",
    "                    action = self.take_action(agent_idx, state)\n",
    "                    actions.append(action)\n",
    "\n",
    "                # print(f\"actions pass to the environment\")\n",
    "                next_states, rewards, dones = self.env.step(actions)\n",
    "\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = actions[agent_idx]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "\n",
    "                    physical_state = tuple(state[0])\n",
    "                    win_state = state[1]\n",
    "                    comm_state = state[2]  # Communication state containing messages from other agents\n",
    "\n",
    "                    physical_next_state = tuple(next_state[0])\n",
    "                    win_next_state = next_state[1]\n",
    "                    sensory_next_state = next_state[2]\n",
    "                    comm_next_state = tuple(next_state[3]) if next_state[3] != 0 else next_state[3]\n",
    "\n",
    "                    physical_action = action[0]\n",
    "                    comm_action = action[1]\n",
    "\n",
    "                    # Process messages received from other agents\n",
    "                    print(f\"state for agent {agent_idx}: {state}\")\n",
    "                    print(f\"next state for agent {agent_idx}: {next_state}\")\n",
    "                    print(f\"comm next state for agent {agent_idx}: {comm_next_state}\")\n",
    "                    # print(f\"comm next state content: {comm_next_state[0]}\")\n",
    "                    \n",
    "                    # if all(element is None for element in comm_next_state):\n",
    "                    # if (comm_next_state == [None]) or (comm_next_state is None):\n",
    "                    if (comm_next_state == 0):\n",
    "                        pass\n",
    "                    else:\n",
    "                        comm_case = Case(problem=comm_next_state[0], solution=comm_next_state[1], trust_value=comm_next_state[2], total_time_steps=comm_next_state[3])\n",
    "                        Case.reuse(agent_idx, comm_case, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='comm')\n",
    "\n",
    "                    # print(f\"state agent {agent_idx} before update: {physical_state}\")\n",
    "                    # print(f\"win state agent {agent_idx} before update: {win_next_state}\")\n",
    "                    # print(f\"action agent {agent_idx} before update: {physical_action}\")\n",
    "                    # print(f\"reward agent {agent_idx} before update: {reward}\")\n",
    "                    # print(f\"next state agent {agent_idx} before update: {physical_next_state}\")\n",
    "\n",
    "                    c = Case(physical_state, physical_action, total_time_steps=total_steps)\n",
    "                    Case.reuse(agent_idx, c, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='own')\n",
    "\n",
    "                    # if self.action_type[agent_idx] == 0:\n",
    "                    #     if not env.locked[agent_idx]:\n",
    "                    #         print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                    #         self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                    #     else:\n",
    "                    #         print(f\"action type of agent: {agent_idx}: using problem solver but locked, no learning\")\n",
    "                    if self.action_type[agent_idx] == 0:\n",
    "                        print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                        self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                    else:\n",
    "                        print(f\"action type of agent: {agent_idx}: using solution from case base, no learning\")\n",
    "\n",
    "                    if (win_next_state): \n",
    "                        success_count[agent_idx] += 1\n",
    "                        # print(f\"agent{agent_idx} hit !!!!!\")\n",
    "                    # else:\n",
    "                    #     print(f\"agent{agent_idx} not hit !!!!!\")\n",
    "\n",
    "                    episode_reward[agent_idx] += reward\n",
    "                    win_states.append(win_next_state)  \n",
    "\n",
    "                states = next_states\n",
    "                total_steps += 1\n",
    "\n",
    "                self.env.render()\n",
    "                \n",
    "            if all(win_states):\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(total_steps)\n",
    "                successful_episodes = True\n",
    "                \n",
    "\n",
    "            \n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                print(f\"win status of agent {agent_idx}  before update the case base: {win_states[agent_idx]}\")\n",
    "                self.rewards_per_episode[agent_idx].append(episode_reward[agent_idx])\n",
    "\n",
    "                print(f\"agent{agent_idx} own temp case base: {self.own_temp_case_bases[agent_idx]}\")\n",
    "                print(f\"agent{agent_idx} comm temp case base: {self.comm_temp_case_bases[agent_idx]}\")\n",
    "                \n",
    "                \n",
    "                Case.revise(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], win_states[agent_idx], total_steps = total_steps)\n",
    "                self.case_bases[agent_idx] = Case.retain(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], win_states[agent_idx], total_steps = total_steps)\n",
    "               \n",
    "                \n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "            print(f\"Episode: {episode}, Total Steps: {total_steps}, Total Rewards: {episode_reward}, Status Episode: {successful_episodes}\")\n",
    "            print(f\"------------------------------------------End of episode {episode} loop--------------------\")\n",
    "\n",
    "        # self.save_case_base_temporary()  # Save temporary case base after training\n",
    "        # self.save_case_base()  # Save case base after training\n",
    "\n",
    "        success_rate = self.total_successful_episodes / episodes * 100\n",
    "\n",
    "        return self.rewards_per_episode, success_rate, memory_usage, gpu_memory_usage, success_steps\n",
    "\n",
    "    def take_action(self, agent_idx, state):\n",
    "        # print(f\"state detected in take action function: {state}\")\n",
    "        physical_state = tuple(state[0])\n",
    "        win_state = state[1]\n",
    "        comm_state = state[2]\n",
    "        # similar_solution = None\n",
    "\n",
    "        # if np.random.rand() < 0.01:\n",
    "        if np.random.rand() < self.epsilon:\n",
    "        # if np.random.rand() < 0:\n",
    "            # physical_action = np.random.choice(self.num_actions)\n",
    "            # comm_action = 0\n",
    "\n",
    "            physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "            comm_action = 0  # No communication action if using problem solver action\n",
    "            self.action_type[agent_idx] = 0\n",
    "            print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        else:\n",
    "            similar_solution = Case.retrieve(physical_state, self.case_bases[agent_idx])\n",
    "            if similar_solution is not None:\n",
    "                physical_action = similar_solution.solution\n",
    "                comm_action = (similar_solution.problem, similar_solution.solution, similar_solution.trust_value, similar_solution.total_time_steps)\n",
    "                self.action_type[agent_idx] = 1\n",
    "                # print(f\"Problem detected as a similiar soulution in case base: {similar_solution.problem}\")\n",
    "                print(f\"Physical Action for Agent {agent_idx} from case base: {physical_action}\")\n",
    "                # print(f\"Communication Action for Agent {agent_idx} from case base: {comm_action}\")\n",
    "                # print(f\"Trust value detected as a similiar solution in case base: {similar_solution.trust_value}\")\n",
    "            else:\n",
    "                physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "                comm_action = 0  # No communication action if using problem solver action\n",
    "                self.action_type[agent_idx] = 0\n",
    "                print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        # print(f\"physical action returned from the take action: {physical_action}\")\n",
    "        # print(f\"comm action returned from the take action: {comm_action}\")\n",
    "\n",
    "        return (physical_action, comm_action)\n",
    "\n",
    "    def case_exists_in_case_base(self, case, case_base):\n",
    "        \"\"\"Check if a case exists in the given case base.\"\"\"\n",
    "        return any(existing_case.problem == case.problem and existing_case.solution == case.solution for existing_case in case_base)\n",
    "        \n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_temporary_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.own_temp_case_bases[agent_idx] + self.comm_temp_case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Temporary case base for Agent {agent_idx} saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Case base for Agent {agent_idx} saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            try:\n",
    "                with open(filename, 'r') as file:\n",
    "                    case_base_data = json.load(file)\n",
    "                    self.case_bases[agent_idx] = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"], case[\"total_time_steps\"]) for case in case_base_data]\n",
    "                    print(f\"Case base for Agent {agent_idx} loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Case base file for Agent {agent_idx} not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    # def plot_rewards(self, rewards):\n",
    "    #     for agent_idx in range(self.env.num_agents):\n",
    "    #         plt.plot([reward for reward in rewards[agent_idx]], label=f'Agent {agent_idx}')\n",
    "    #     plt.xlabel('Episode')\n",
    "    #     plt.ylabel('Total Reward')\n",
    "    #     plt.title('Rewards over Episodes')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True)\n",
    "    #     plt.show()\n",
    "    \n",
    "    def plot_rewards(self, rewards, window=5):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            # Calculate the moving average of rewards over the specified window size\n",
    "            moving_avg_rewards = [np.mean(rewards[agent_idx][i:i + window]) for i in range(0, len(rewards[agent_idx]), window)]\n",
    "            \n",
    "            plt.plot(moving_avg_rewards, label=f'Agent {agent_idx}')\n",
    "        \n",
    "        plt.xlabel(f'Episode (Averaged over every {window} episodes)')\n",
    "        plt.ylabel('Average Total Reward')\n",
    "        plt.title('Average Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.ylim(-400, 400)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.ylim(top=500)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 2\n",
    "    num_obstacles = 10\n",
    "    obstacles_random_steps = 20\n",
    "    is_agent_silent = False\n",
    "    episodes=59\n",
    "    max_steps=400\n",
    "    alpha=0.1\n",
    "    gamma=0.9\n",
    "    epsilon=0.1\n",
    "    epsilon_decay = 0.995  \n",
    "    epsilon_min = 0.01  \n",
    "    render = True\n",
    "    sensory_size = 3 #it must be odd, if event will be converted to one level odd number above\n",
    "    gpixels=50\n",
    "    gheight=10\n",
    "    gwidth=10\n",
    "    is_sensor_active = True\n",
    "\n",
    "    env = Env(num_agents=num_agents, num_obstacles=num_obstacles, obstacles_random_steps = obstacles_random_steps, is_agent_silent=is_agent_silent, sensory_size=sensory_size, gpixels=gpixels, gheight=gheight, gwidth=gwidth, is_sensor_active=is_sensor_active)\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    agent = QCBRL(num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run()\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
