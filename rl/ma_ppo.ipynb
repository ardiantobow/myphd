{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon_clip = 0.2\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "max_steps = 200\n",
    "max_episodes = 1000\n",
    "\n",
    "# Define the environment\n",
    "class GridEnvironment:\n",
    "    def __init__(self, size=10, target=(9, 9)):\n",
    "        self.size = size\n",
    "        self.target = target\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_positions = [(0, 0), (self.size-1, self.size-1)]\n",
    "        self.steps = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = []\n",
    "        next_state = []\n",
    "        for i, action in enumerate(actions):\n",
    "            x, y = self.agent_positions[i]\n",
    "            if action == 0:  # move up\n",
    "                x = max(0, x-1)\n",
    "            elif action == 1:  # move down\n",
    "                x = min(self.size-1, x+1)\n",
    "            elif action == 2:  # move left\n",
    "                y = max(0, y-1)\n",
    "            elif action == 3:  # move right\n",
    "                y = min(self.size-1, y+1)\n",
    "\n",
    "            self.agent_positions[i] = (x, y)\n",
    "            if (x, y) == self.target:\n",
    "                rewards.append(100)\n",
    "            else:\n",
    "                rewards.append(-1)\n",
    "\n",
    "            next_state.append(self.get_state()[i])\n",
    "\n",
    "        self.steps += 1\n",
    "        done = self.steps >= max_steps or all(pos == self.target for pos in self.agent_positions)\n",
    "        return next_state, rewards, done\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for pos in self.agent_positions:\n",
    "            state = np.zeros((self.size, self.size))\n",
    "            state[pos] = 1\n",
    "            state[self.target] = 2\n",
    "            states.append(state.flatten())\n",
    "        return states\n",
    "\n",
    "# Define the PPO network\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.policy_head = nn.Linear(256, action_dim)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy, value\n",
    "\n",
    "def select_action(agent, state):\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    policy, _ = agent(state)\n",
    "    action_prob = torch.softmax(policy, dim=-1)\n",
    "    action = np.random.choice(len(action_prob[0]), p=action_prob.detach().numpy()[0])\n",
    "    return action\n",
    "\n",
    "def compute_returns(rewards, dones, gamma):\n",
    "    R = 0\n",
    "    returns = deque()\n",
    "    for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "        R = r + gamma * R * (1 - d)\n",
    "        returns.appendleft(R)\n",
    "    return list(returns)\n",
    "\n",
    "def update_policy(agent, optimizer, states, actions, old_log_probs, returns, advantages):\n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, len(states), batch_size):\n",
    "            state_batch = torch.FloatTensor(states[i:i+batch_size])\n",
    "            action_batch = torch.LongTensor(actions[i:i+batch_size])\n",
    "            old_log_prob_batch = torch.FloatTensor(old_log_probs[i:i+batch_size])\n",
    "            return_batch = torch.FloatTensor(returns[i:i+batch_size])\n",
    "            advantage_batch = torch.FloatTensor(advantages[i:i+batch_size])\n",
    "\n",
    "            policy, value = agent(state_batch)\n",
    "            value = value.squeeze()\n",
    "            action_prob = torch.softmax(policy, dim=-1)\n",
    "            dist = torch.distributions.Categorical(action_prob)\n",
    "            log_prob = dist.log_prob(action_batch)\n",
    "\n",
    "            ratio = torch.exp(log_prob - old_log_prob_batch)\n",
    "            surr1 = ratio * advantage_batch\n",
    "            surr2 = torch.clamp(ratio, 1 - epsilon_clip, 1 + epsilon_clip) * advantage_batch\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = nn.functional.mse_loss(value, return_batch)\n",
    "\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "env = GridEnvironment()\n",
    "state_dim = env.size * env.size\n",
    "action_dim = 4\n",
    "\n",
    "agent_a = PPOAgent(state_dim, action_dim)\n",
    "agent_b = PPOAgent(state_dim, action_dim)\n",
    "optimizer_a = optim.Adam(agent_a.parameters(), lr=lr)\n",
    "optimizer_b = optim.Adam(agent_b.parameters(), lr=lr)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    states = env.reset()\n",
    "    states_a = []\n",
    "    states_b = []\n",
    "    actions_a = []\n",
    "    actions_b = []\n",
    "    rewards_a = []\n",
    "    rewards_b = []\n",
    "    log_probs_a = []\n",
    "    log_probs_b = []\n",
    "    dones = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action_a = select_action(agent_a, states[0])\n",
    "        action_b = select_action(agent_b, states[1])\n",
    "\n",
    "        actions = [action_a, action_b]\n",
    "        next_states, rewards, done = env.step(actions)\n",
    "\n",
    "        states_a.append(states[0])\n",
    "        states_b.append(states[1])\n",
    "        actions_a.append(action_a)\n",
    "        actions_b.append(action_b)\n",
    "        rewards_a.append(rewards[0])\n",
    "        rewards_b.append(rewards[1])\n",
    "        dones.append(done)\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    returns_a = compute_returns(rewards_a, dones, gamma)\n",
    "    returns_b = compute_returns(rewards_b, dones, gamma)\n",
    "\n",
    "    states_tensor_a = torch.FloatTensor(states_a)\n",
    "    states_tensor_b = torch.FloatTensor(states_b)\n",
    "    actions_tensor_a = torch.LongTensor(actions_a)\n",
    "    actions_tensor_b = torch.LongTensor(actions_b)\n",
    "\n",
    "    policy_a, values_a = agent_a(states_tensor_a)\n",
    "    policy_b, values_b = agent_b(states_tensor_b)\n",
    "\n",
    "    values_a = values_a.squeeze().detach().numpy()\n",
    "    values_b = values_b.squeeze().detach().numpy()\n",
    "\n",
    "    action_probs_a = torch.softmax(policy_a, dim=-1).detach().numpy()\n",
    "    action_probs_b = torch.softmax(policy_b, dim=-1).detach().numpy()\n",
    "\n",
    "    old_log_probs_a = np.log([action_probs_a[i, actions_a[i]] for i in range(len(actions_a))])\n",
    "    old_log_probs_b = np.log([action_probs_b[i, actions_b[i]] for i in range(len(actions_b))])\n",
    "\n",
    "    advantages_a = np.array(returns_a) - values_a\n",
    "    advantages_b = np.array(returns_b) - values_b\n",
    "\n",
    "    # Normalize advantages\n",
    "    advantages_a = (advantages_a - advantages_a.mean()) / (advantages_a.std() + 1e-8)\n",
    "    advantages_b = (advantages_b - advantages_b.mean()) / (advantages_b.std() + 1e-8)\n",
    "\n",
    "    # Update policy for Agent A\n",
    "    update_policy(agent_a, optimizer_a, states_a, actions_a, old_log_probs_a, returns_a, advantages_a)\n",
    "\n",
    "    # Update policy for Agent B\n",
    "    update_policy(agent_b, optimizer_b, states_b, actions_b, old_log_probs_b, returns_b, advantages_b)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode} complete\")\n",
    "\n",
    "# Testing the trained agents\n",
    "def test_agents(env, agent_a, agent_b, max_steps=200):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action_a = select_action(agent_a, state[0])\n",
    "        action_b = select_action(agent_b, state[1])\n",
    "        actions = [action_a, action_b]\n",
    "        next_state, rewards, done = env.step(actions)\n",
    "        total_reward += sum(rewards)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Evaluate trained agents\n",
    "total_rewards = []\n",
    "for _ in range(10):\n",
    "    total_reward = test_agents(env, agent_a, agent_b)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print(f\"Average total reward over 10 episodes: {np.mean(total_rewards)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
