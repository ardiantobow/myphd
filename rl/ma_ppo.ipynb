{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6594/1014945320.py:168: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  states_tensor_a = torch.FloatTensor(states_a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 complete\n",
      "Episode 10 complete\n",
      "Episode 20 complete\n",
      "Episode 30 complete\n",
      "Episode 40 complete\n",
      "Episode 50 complete\n",
      "Episode 60 complete\n",
      "Episode 70 complete\n",
      "Episode 80 complete\n",
      "Episode 90 complete\n",
      "Episode 100 complete\n",
      "Episode 110 complete\n",
      "Episode 120 complete\n",
      "Episode 130 complete\n",
      "Episode 140 complete\n",
      "Episode 150 complete\n",
      "Episode 160 complete\n",
      "Episode 170 complete\n",
      "Episode 180 complete\n",
      "Episode 190 complete\n",
      "Episode 200 complete\n",
      "Episode 210 complete\n",
      "Episode 220 complete\n",
      "Episode 230 complete\n",
      "Episode 240 complete\n",
      "Episode 250 complete\n",
      "Episode 260 complete\n",
      "Episode 270 complete\n",
      "Episode 280 complete\n",
      "Episode 290 complete\n",
      "Episode 300 complete\n",
      "Episode 310 complete\n",
      "Episode 320 complete\n",
      "Episode 330 complete\n",
      "Episode 340 complete\n",
      "Episode 350 complete\n",
      "Episode 360 complete\n",
      "Episode 370 complete\n",
      "Episode 380 complete\n",
      "Episode 390 complete\n",
      "Episode 400 complete\n",
      "Episode 410 complete\n",
      "Episode 420 complete\n",
      "Episode 430 complete\n",
      "Episode 440 complete\n",
      "Episode 450 complete\n",
      "Episode 460 complete\n",
      "Episode 470 complete\n",
      "Episode 480 complete\n",
      "Episode 490 complete\n",
      "Episode 500 complete\n",
      "Episode 510 complete\n",
      "Episode 520 complete\n",
      "Episode 530 complete\n",
      "Episode 540 complete\n",
      "Episode 550 complete\n",
      "Episode 560 complete\n",
      "Episode 570 complete\n",
      "Episode 580 complete\n",
      "Episode 590 complete\n",
      "Episode 600 complete\n",
      "Episode 610 complete\n",
      "Episode 620 complete\n",
      "Episode 630 complete\n",
      "Episode 640 complete\n",
      "Episode 650 complete\n",
      "Episode 660 complete\n",
      "Episode 670 complete\n",
      "Episode 680 complete\n",
      "Episode 690 complete\n",
      "Episode 700 complete\n",
      "Episode 710 complete\n",
      "Episode 720 complete\n",
      "Episode 730 complete\n",
      "Episode 740 complete\n",
      "Episode 750 complete\n",
      "Episode 760 complete\n",
      "Episode 770 complete\n",
      "Episode 780 complete\n",
      "Episode 790 complete\n",
      "Episode 800 complete\n",
      "Episode 810 complete\n",
      "Episode 820 complete\n",
      "Episode 830 complete\n",
      "Episode 840 complete\n",
      "Episode 850 complete\n",
      "Episode 860 complete\n",
      "Episode 870 complete\n",
      "Episode 880 complete\n",
      "Episode 890 complete\n",
      "Episode 900 complete\n",
      "Episode 910 complete\n",
      "Episode 920 complete\n",
      "Episode 930 complete\n",
      "Episode 940 complete\n",
      "Episode 950 complete\n",
      "Episode 960 complete\n",
      "Episode 970 complete\n",
      "Episode 980 complete\n",
      "Episode 990 complete\n",
      "Average total reward over 10 episodes: -400.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon_clip = 0.2\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "max_steps = 200\n",
    "max_episodes = 1000\n",
    "\n",
    "# Define the environment\n",
    "class GridEnvironment:\n",
    "    def __init__(self, size=10, target=(9, 9)):\n",
    "        self.size = size\n",
    "        self.target = target\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_positions = [(0, 0), (self.size-1, self.size-1)]\n",
    "        self.steps = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = []\n",
    "        next_state = []\n",
    "        for i, action in enumerate(actions):\n",
    "            x, y = self.agent_positions[i]\n",
    "            if action == 0:  # move up\n",
    "                x = max(0, x-1)\n",
    "            elif action == 1:  # move down\n",
    "                x = min(self.size-1, x+1)\n",
    "            elif action == 2:  # move left\n",
    "                y = max(0, y-1)\n",
    "            elif action == 3:  # move right\n",
    "                y = min(self.size-1, y+1)\n",
    "\n",
    "            self.agent_positions[i] = (x, y)\n",
    "            if (x, y) == self.target:\n",
    "                rewards.append(100)\n",
    "            else:\n",
    "                rewards.append(-1)\n",
    "\n",
    "            next_state.append(self.get_state()[i])\n",
    "\n",
    "        self.steps += 1\n",
    "        done = self.steps >= max_steps or all(pos == self.target for pos in self.agent_positions)\n",
    "        return next_state, rewards, done\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for pos in self.agent_positions:\n",
    "            state = np.zeros((self.size, self.size))\n",
    "            state[pos] = 1\n",
    "            state[self.target] = 2\n",
    "            states.append(state.flatten())\n",
    "        return states\n",
    "\n",
    "# Define the PPO network\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.policy_head = nn.Linear(256, action_dim)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy, value\n",
    "\n",
    "def select_action(agent, state):\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    policy, _ = agent(state)\n",
    "    action_prob = torch.softmax(policy, dim=-1)\n",
    "    action = np.random.choice(len(action_prob[0]), p=action_prob.detach().numpy()[0])\n",
    "    return action\n",
    "\n",
    "def compute_returns(rewards, dones, gamma):\n",
    "    R = 0\n",
    "    returns = deque()\n",
    "    for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "        R = r + gamma * R * (1 - d)\n",
    "        returns.appendleft(R)\n",
    "    return list(returns)\n",
    "\n",
    "def update_policy(agent, optimizer, states, actions, old_log_probs, returns, advantages):\n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, len(states), batch_size):\n",
    "            state_batch = torch.FloatTensor(states[i:i+batch_size])\n",
    "            action_batch = torch.LongTensor(actions[i:i+batch_size])\n",
    "            old_log_prob_batch = torch.FloatTensor(old_log_probs[i:i+batch_size])\n",
    "            return_batch = torch.FloatTensor(returns[i:i+batch_size])\n",
    "            advantage_batch = torch.FloatTensor(advantages[i:i+batch_size])\n",
    "\n",
    "            policy, value = agent(state_batch)\n",
    "            value = value.squeeze()\n",
    "            action_prob = torch.softmax(policy, dim=-1)\n",
    "            dist = torch.distributions.Categorical(action_prob)\n",
    "            log_prob = dist.log_prob(action_batch)\n",
    "\n",
    "            ratio = torch.exp(log_prob - old_log_prob_batch)\n",
    "            surr1 = ratio * advantage_batch\n",
    "            surr2 = torch.clamp(ratio, 1 - epsilon_clip, 1 + epsilon_clip) * advantage_batch\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = nn.functional.mse_loss(value, return_batch)\n",
    "\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "env = GridEnvironment()\n",
    "state_dim = env.size * env.size\n",
    "action_dim = 4\n",
    "\n",
    "agent_a = PPOAgent(state_dim, action_dim)\n",
    "agent_b = PPOAgent(state_dim, action_dim)\n",
    "optimizer_a = optim.Adam(agent_a.parameters(), lr=lr)\n",
    "optimizer_b = optim.Adam(agent_b.parameters(), lr=lr)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    states = env.reset()\n",
    "    states_a = []\n",
    "    states_b = []\n",
    "    actions_a = []\n",
    "    actions_b = []\n",
    "    rewards_a = []\n",
    "    rewards_b = []\n",
    "    log_probs_a = []\n",
    "    log_probs_b = []\n",
    "    dones = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action_a = select_action(agent_a, states[0])\n",
    "        action_b = select_action(agent_b, states[1])\n",
    "\n",
    "        actions = [action_a, action_b]\n",
    "        next_states, rewards, done = env.step(actions)\n",
    "\n",
    "        states_a.append(states[0])\n",
    "        states_b.append(states[1])\n",
    "        actions_a.append(action_a)\n",
    "        actions_b.append(action_b)\n",
    "        rewards_a.append(rewards[0])\n",
    "        rewards_b.append(rewards[1])\n",
    "        dones.append(done)\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    returns_a = compute_returns(rewards_a, dones, gamma)\n",
    "    returns_b = compute_returns(rewards_b, dones, gamma)\n",
    "\n",
    "    states_tensor_a = torch.FloatTensor(states_a)\n",
    "    states_tensor_b = torch.FloatTensor(states_b)\n",
    "    actions_tensor_a = torch.LongTensor(actions_a)\n",
    "    actions_tensor_b = torch.LongTensor(actions_b)\n",
    "\n",
    "    policy_a, values_a = agent_a(states_tensor_a)\n",
    "    policy_b, values_b = agent_b(states_tensor_b)\n",
    "\n",
    "    values_a = values_a.squeeze().detach().numpy()\n",
    "    values_b = values_b.squeeze().detach().numpy()\n",
    "\n",
    "    action_probs_a = torch.softmax(policy_a, dim=-1).detach().numpy()\n",
    "    action_probs_b = torch.softmax(policy_b, dim=-1).detach().numpy()\n",
    "\n",
    "    old_log_probs_a = np.log([action_probs_a[i, actions_a[i]] for i in range(len(actions_a))])\n",
    "    old_log_probs_b = np.log([action_probs_b[i, actions_b[i]] for i in range(len(actions_b))])\n",
    "\n",
    "    advantages_a = np.array(returns_a) - values_a\n",
    "    advantages_b = np.array(returns_b) - values_b\n",
    "\n",
    "    # Normalize advantages\n",
    "    advantages_a = (advantages_a - advantages_a.mean()) / (advantages_a.std() + 1e-8)\n",
    "    advantages_b = (advantages_b - advantages_b.mean()) / (advantages_b.std() + 1e-8)\n",
    "\n",
    "    # Update policy for Agent A\n",
    "    update_policy(agent_a, optimizer_a, states_a, actions_a, old_log_probs_a, returns_a, advantages_a)\n",
    "\n",
    "    # Update policy for Agent B\n",
    "    update_policy(agent_b, optimizer_b, states_b, actions_b, old_log_probs_b, returns_b, advantages_b)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode} complete\")\n",
    "\n",
    "# Testing the trained agents\n",
    "def test_agents(env, agent_a, agent_b, max_steps=200):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action_a = select_action(agent_a, state[0])\n",
    "        action_b = select_action(agent_b, state[1])\n",
    "        actions = [action_a, action_b]\n",
    "        next_state, rewards, done = env.step(actions)\n",
    "        total_reward += sum(rewards)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Evaluate trained agents\n",
    "total_rewards = []\n",
    "for _ in range(10):\n",
    "    total_reward = test_agents(env, agent_a, agent_b)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print(f\"Average total reward over 10 episodes: {np.mean(total_rewards)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
